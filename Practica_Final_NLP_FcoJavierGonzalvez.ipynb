{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Practica_Final_NLP_FcoJavierGonzalvez.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Te-bDSN1c6IP"
      },
      "source": [
        "\n",
        ">#     **PRACTICA DE LA ASIGNATURA *NATURAL LANGUAGE PROCESSING - NLP -***\n",
        "                                                                          Profesor  David Torrejón Moya\n",
        "                                                                            Alumno  Francisco Javier Gonzálvez Chico"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRJ7fiaDYMqo",
        "colab_type": "text"
      },
      "source": [
        "# Introducción \n",
        "\n",
        "La idea de la práctica es visitar aquellos temas que en cierta manera nos permitan ver más contenido del curso.\n",
        "\n",
        "La práctica esta dividida en 4 o 5 subapartados, que ya tenéis en este mismo Notebook. Estos subapartados estan aquí para que rellenéis el código que hace falta para la realización de la práctica. Obviamente podéis usar tantas celdas como os hagan falta, es más es de agradecer si el código final esta algo \"limpio\". Usar funciones, algo de comentario, etc, etc...\n",
        "\n",
        "Usaremos 2 datasets, uno para el primer ejercicio, y otro para el resto de ejercicios.\n",
        "\n",
        "Ejercicios:\n",
        "\n",
        "\n",
        "1.   Machine Learning vs Deep Learning (Acordaros que hay que implementar el pipeline visto en clase entero)\n",
        "\n",
        "    1.1. Implementación de un modelo de Sentiment Analysis con algún algoritmo de Machine Learning Clásico.\n",
        "    \n",
        "    1.2. Implementación de un modelo de Sentiment Analysis con alguna arquitectura de Deep Learning.\n",
        "    \n",
        "    1.3. Breve Comparación de resultados. Confusion Matrix.\n",
        "    \n",
        "2. Hacer Analysis de los tweets del segundo dataset. Que temas aparecen? Como se representan estos temas? De que hablan unos y otros?\n",
        "\n",
        "3. Escoged a uno de los dos presidentes, y escribid tweets como ellos, usando un Modelo Generativo.\n",
        "\n",
        "En cada ejercicio, espero explicaciones y razonamientos del porque una arquitectura y no otra, por ejemplo en Deep Learning, porque usar Convolutionals en lugar de recurrentes, o en Machine Learning, Bayes en lugar de SVM. Hay que explicar el pipeline, sobretodo el preproceso de datos, con lo que habrá que hacer un pequeño estudio de que datos tenemos, y si hay cosas que se pueden ignorar, si hacéis stemming, o no, etc, etc...\n",
        "\n",
        "Acordaros de que objetivo final no es que obtengáis una accuracy brutal, es que comprendais que pasa cuando usais un algoritmo u otro, y que problemas o beneficios nos dan.\n",
        "\n",
        "![IMAGEN](https://i.pinimg.com/736x/19/63/8c/19638c0b33e2f7822d6806ce31d89d84--funny-cartoons-funny-jokes.jpg)\n",
        "\n",
        "Mucha suerte y ánimo!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VKxviRgjlSKW"
      },
      "source": [
        "># **IMPORTACIONES Y DESCARGAS NECESARIAS**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0PEHsVXlbaT",
        "colab_type": "code",
        "outputId": "6530ed92-9a2c-44c1-a673-f44e3da28a05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# EJECUTAR SIEMPRE, LAS PRIMERAS, TODAS LAS CELDAS HASTA -EJERCICIO 1-\n",
        "\n",
        "from google.colab import files, drive\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import io\n",
        "import re\n",
        "import spacy\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from keras.layers import Embedding, Input, Dense, Dropout, Concatenate, Conv2D, GlobalMaxPooling2D, Reshape, Average, average, Lambda, CuDNNLSTM, LSTM, Bidirectional\n",
        "from keras.models import Model\n",
        "from keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from keras import backend as K\n",
        "\n",
        "from gensim import corpora, models, similarities\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "from random import sample, shuffle, choice\n",
        "from collections import Counter\n",
        "from math import exp\n",
        "from copy import copy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "%matplotlib inline\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uU6X4lncUi-",
        "colab_type": "code",
        "outputId": "f8ead439-d099-4c0f-fcc6-00eda70f3e71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "# instalamos e importamos la librería de contraciones del inglés\n",
        "\n",
        "!pip install contractions\n",
        "\n",
        "import contractions"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading https://files.pythonhosted.org/packages/85/41/c3dfd5feb91a8d587ed1a59f553f07c05f95ad4e5d00ab78702fbf8fe48a/contractions-0.0.24-py2.py3-none-any.whl\n",
            "Collecting textsearch\n",
            "  Downloading https://files.pythonhosted.org/packages/42/a8/03407021f9555043de5492a2bd7a35c56cc03c2510092b5ec018cae1bbf1/textsearch-0.0.17-py2.py3-none-any.whl\n",
            "Collecting pyahocorasick\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/9f/f0d8e8850e12829eea2e778f1c90e3c53a9a799b7f412082a5d21cd19ae1/pyahocorasick-1.4.0.tar.gz (312kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 13.9MB/s \n",
            "\u001b[?25hCollecting Unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 57.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.0-cp36-cp36m-linux_x86_64.whl size=81702 sha256=8e9bd74899d28d8011bd7767fee3485ac9d5d727d0fffd3212e0c5a7d69fedfd\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/90/61/87a55f5b459792fbb2b7ba6b31721b06ff5cf6bde541b40994\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, Unidecode, textsearch, contractions\n",
            "Successfully installed Unidecode-1.1.1 contractions-0.0.24 pyahocorasick-1.4.0 textsearch-0.0.17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57Kn4w_bsm9R",
        "colab_type": "code",
        "outputId": "4ab154b9-cb05-4014-9883-40b8f67a3ba8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "# instalamos e importamos la librería de stop words\n",
        "\n",
        "!pip install stop_words\n",
        "\n",
        "from stop_words import get_stop_words\n",
        "\n",
        "stop_words = get_stop_words('en')\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting stop_words\n",
            "  Downloading https://files.pythonhosted.org/packages/1c/cb/d58290804b7a4c5daa42abbbe2a93c477ae53e45541b1825e86f0dfaaf63/stop-words-2018.7.23.tar.gz\n",
            "Building wheels for collected packages: stop-words\n",
            "  Building wheel for stop-words (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stop-words: filename=stop_words-2018.7.23-cp36-none-any.whl size=32917 sha256=3c42fda0439324d36a8e56497f88477ccfde77b3f24363fb8e32fdcecece2ee4\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/37/6a/2b295e03bd07290f0da95c3adb9a74ba95fbc333aa8b0c7c78\n",
            "Successfully built stop-words\n",
            "Installing collected packages: stop-words\n",
            "Successfully installed stop-words-2018.7.23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qmL7nEzz3PZ",
        "colab_type": "code",
        "outputId": "5d68e9c0-a118-4c56-af33-8d85939637cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "# instalamos e importamos la librería de visualización pyLDAvis\n",
        "\n",
        "!pip install pyLDAvis\n",
        "\n",
        "import pyLDAvis.gensim"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyLDAvis\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/3a/af82e070a8a96e13217c8f362f9a73e82d61ac8fff3a2561946a97f96266/pyLDAvis-2.1.2.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 6.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.34.2)\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.17.5)\n",
            "Requirement already satisfied: scipy>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.25.3)\n",
            "Requirement already satisfied: joblib>=0.8.4 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.14.1)\n",
            "Requirement already satisfied: jinja2>=2.7.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (2.11.1)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (2.7.1)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (3.6.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.16.0)\n",
            "Collecting funcy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/4b/6ffa76544e46614123de31574ad95758c421aae391a1764921b8a81e1eae/funcy-1.14.tar.gz (548kB)\n",
            "\u001b[K     |████████████████████████████████| 552kB 46.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDAvis) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDAvis) (2.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.7.2->pyLDAvis) (1.1.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.8.1)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (0.7.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (19.3.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (45.1.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (8.2.0)\n",
            "Building wheels for collected packages: pyLDAvis, funcy\n",
            "  Building wheel for pyLDAvis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyLDAvis: filename=pyLDAvis-2.1.2-py2.py3-none-any.whl size=97711 sha256=6da64d6d3376dada13fcbbfae859ca77e10c119de3b7c4cb4b9905490d3db185\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/71/24/513a99e58bb6b8465bae4d2d5e9dba8f0bef8179e3051ac414\n",
            "  Building wheel for funcy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for funcy: filename=funcy-1.14-py2.py3-none-any.whl size=32042 sha256=e84782d0149ff9a22f6f15ed9d7c65743c689dcffbae7171be18a29544740f98\n",
            "  Stored in directory: /root/.cache/pip/wheels/20/5a/d8/1d875df03deae6f178dfdf70238cca33f948ef8a6f5209f2eb\n",
            "Successfully built pyLDAvis funcy\n",
            "Installing collected packages: funcy, pyLDAvis\n",
            "Successfully installed funcy-1.14 pyLDAvis-2.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbYn-edBV3F1",
        "colab_type": "code",
        "outputId": "c97b1c7f-577c-4ac5-aba4-65f3b0807d6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# descargamos el fichero de Jerga de Dimitrios Effrosynidis\n",
        "\n",
        "!wget https://raw.githubusercontent.com/Deffro/text-preprocessing-techniques/master/slang.txt\n",
        "\n",
        "# creamos un diccionario de jerga y su conversión correcta... lo usaremos en los preprocesamientos de los datasets\n",
        "jerga = pd.read_csv(\"slang.txt\", header = None, encoding = 'latin1', delimiter = \"\\t\")\n",
        "jerga.columns = ['Jerga', 'Texto']\n",
        "\n",
        "jerga_dict = dict([(Jerga, Texto) for Jerga, Texto in zip(jerga.Jerga, jerga.Texto)])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-21 19:27:40--  https://raw.githubusercontent.com/Deffro/text-preprocessing-techniques/master/slang.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5181 (5.1K) [text/plain]\n",
            "Saving to: ‘slang.txt’\n",
            "\n",
            "\rslang.txt             0%[                    ]       0  --.-KB/s               \rslang.txt           100%[===================>]   5.06K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-02-21 19:27:40 (57.1 MB/s) - ‘slang.txt’ saved [5181/5181]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMESTLF1YMyO",
        "colab_type": "text"
      },
      "source": [
        "># **EJERCICIO 1**\n",
        "###<pre>      ***Machine Learning vs Deep Learning***\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AwJz6eQab6iy"
      },
      "source": [
        ">># **Dataset**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZzoiapQQapi",
        "colab_type": "text"
      },
      "source": [
        "#####<pre>      Cargamos el Dataset \"train_sentiment.csv\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jc6NHUT7O1ey",
        "colab_type": "code",
        "outputId": "b91314d3-eb05-4cc2-cbe8-7f64e8b7e9db",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-55563efc-90a2-4de6-9451-b013cf58729b\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-55563efc-90a2-4de6-9451-b013cf58729b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving train_sentiment.csv to train_sentiment.csv\n",
            "User uploaded file \"train_sentiment.csv\" with length 8664015 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tqn69N5dePxH",
        "colab_type": "text"
      },
      "source": [
        "#####<pre>      Obtenemos una breve descripción, su estructura y le damos un primer vistazo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqtzHn4DQTRG",
        "colab_type": "code",
        "outputId": "c0be4c55-1a4e-416b-b866-1b0f56eff150",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        }
      },
      "source": [
        "df1 = pd.read_csv(io.StringIO(uploaded['train_sentiment.csv'].decode('ISO-8859-1')))\n",
        "\n",
        "print(\"\\t\\t NULOS\\n\", \"\\t\\t-------\\n\", df1.isnull().any(), \"\\n\\n\", sep = '')\n",
        "\n",
        "df1.describe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t\t NULOS\n",
            "\t\t-------\n",
            "ItemID           False\n",
            "Sentiment        False\n",
            "SentimentText    False\n",
            "dtype: bool\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ItemID</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>99989.000000</td>\n",
              "      <td>99989.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>50005.110042</td>\n",
              "      <td>0.564632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>28865.894393</td>\n",
              "      <td>0.495808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>25009.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>50006.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>75003.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>100000.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              ItemID     Sentiment\n",
              "count   99989.000000  99989.000000\n",
              "mean    50005.110042      0.564632\n",
              "std     28865.894393      0.495808\n",
              "min         1.000000      0.000000\n",
              "25%     25009.000000      0.000000\n",
              "50%     50006.000000      1.000000\n",
              "75%     75003.000000      1.000000\n",
              "max    100000.000000      1.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnP_WHiMaAfb",
        "colab_type": "code",
        "outputId": "c2563106-9881-4452-efbc-55e11206909e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "df1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ItemID</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>SentimentText</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>is so sad for my APL frie...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>I missed the New Moon trail...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>omg its already 7:30 :O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99984</th>\n",
              "      <td>99996</td>\n",
              "      <td>0</td>\n",
              "      <td>@Cupcake  seems like a repeating problem   hop...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99985</th>\n",
              "      <td>99997</td>\n",
              "      <td>1</td>\n",
              "      <td>@cupcake__ arrrr we both replied to each other...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99986</th>\n",
              "      <td>99998</td>\n",
              "      <td>0</td>\n",
              "      <td>@CuPcAkE_2120 ya i thought so</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99987</th>\n",
              "      <td>99999</td>\n",
              "      <td>1</td>\n",
              "      <td>@Cupcake_Dollie Yes. Yes. I'm glad you had mor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99988</th>\n",
              "      <td>100000</td>\n",
              "      <td>1</td>\n",
              "      <td>@cupcake_kayla haha yes you do</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>99989 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       ItemID  Sentiment                                      SentimentText\n",
              "0           1          0                       is so sad for my APL frie...\n",
              "1           2          0                     I missed the New Moon trail...\n",
              "2           3          1                            omg its already 7:30 :O\n",
              "3           4          0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
              "4           5          0           i think mi bf is cheating on me!!!   ...\n",
              "...       ...        ...                                                ...\n",
              "99984   99996          0  @Cupcake  seems like a repeating problem   hop...\n",
              "99985   99997          1  @cupcake__ arrrr we both replied to each other...\n",
              "99986   99998          0                     @CuPcAkE_2120 ya i thought so \n",
              "99987   99999          1  @Cupcake_Dollie Yes. Yes. I'm glad you had mor...\n",
              "99988  100000          1                    @cupcake_kayla haha yes you do \n",
              "\n",
              "[99989 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E33vMCvPRRgV"
      },
      "source": [
        "#####<pre>      Observamos que el Dataset tiene 99.689 filas y 3 columnas sin valores nulos\n",
        "~~~\n",
        "          > La primera columna -ItemID- es discreta [min = 1 ; max = 100.000], un identificador numérico único\n",
        "            Suponemos que este dataset tuvo originalmente 100K filas y que en algún tratamiento anterior (probablemente una eliminación \n",
        "            de filas vacías) se descartaron 11 de sus registros\n",
        "          > La segunda columna -Sentiment- es binaria [0 / 1] y se trata de la etiqueta indicadora del problema a resolver\n",
        "          > La tercera columna -SentimentText- [string], contiene el texto a analizar en el problema\n",
        "~~~"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NapT4P1ciBM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# eliminamos la columna ItemID ya que no nos va a aportar información útil\n",
        "\n",
        "df1.drop(['ItemID'], axis = 1, inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPJ3R4pifp0L",
        "colab_type": "code",
        "outputId": "84295efb-bc38-4a1d-d47b-f2475556f873",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "print(\"     COUNT\\n\", \"    -------\\n\", df1['Sentiment'].value_counts(), \"\\n\\n\", sep = '')\n",
        "\n",
        "for i in range(0, 10):\n",
        "  print(df1['Sentiment'][i], \"\\t\", df1['SentimentText'][i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     COUNT\n",
            "    -------\n",
            "1    56457\n",
            "0    43532\n",
            "Name: Sentiment, dtype: int64\n",
            "\n",
            "\n",
            "0 \t                      is so sad for my APL friend.............\n",
            "0 \t                    I missed the New Moon trailer...\n",
            "1 \t               omg its already 7:30 :O\n",
            "0 \t           .. Omgaga. Im sooo  im gunna CRy. I've been at this dentist since 11.. I was suposed 2 just get a crown put on (30mins)...\n",
            "0 \t          i think mi bf is cheating on me!!!       T_T\n",
            "0 \t          or i just worry too much?        \n",
            "1 \t        Juuuuuuuuuuuuuuuuussssst Chillin!!\n",
            "0 \t        Sunny Again        Work Tomorrow  :-|       TV Tonight\n",
            "1 \t       handed in my uniform today . i miss you already\n",
            "1 \t       hmmmm.... i wonder how she my number @-)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NQ97dwuZjo8H"
      },
      "source": [
        "#####<pre>      El Dataset está bastante balanceado con respecto a la etiqueta [1 - 56.46%  / 0 - 43.54%]\n",
        "~~~\n",
        "        A la vista de la etiqueta y del texto completo de los 10 primeros registros:\n",
        "          > El valor 0 de la etiqueta apunta a un sentimiento negativo y el 1 a un sentimiento positivo\n",
        "~~~"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wyL2gRRemyD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# transformamos la etiqueta para que sea más significativa\n",
        "\n",
        "for fila in range(0, len(df1)):\n",
        "  etiqueta = df1.loc[fila, 'Sentiment']\n",
        "\n",
        "  etiqueta = 'POS' if etiqueta == 1 else 'NEG'\n",
        "\n",
        "  df1.loc[fila, 'Sentiment'] = etiqueta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "237ofC5Wsmj2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# función para extraer las palabras de la columna -SentimentText-\n",
        "\n",
        "def extraer_vocab(df):\n",
        "  vocab = list()\n",
        "\n",
        "  for index, row in df.iterrows():\n",
        "    for word in row['SentimentText'].strip().split(' '):\n",
        "      if word not in vocab:\n",
        "        vocab.append(word)\n",
        "\n",
        "  print(f\"Número de Palabras: {len(vocab)}\")\n",
        "\n",
        "  return vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJXqtsYUrs4J",
        "colab_type": "code",
        "outputId": "d4a4ab60-ae85-45d4-ec99-6fdf55056cbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vocab = extraer_vocab(df1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Número de Palabras: 183645\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "we9cveFexdJs",
        "colab_type": "text"
      },
      "source": [
        "#####<pre>      La columna -SentimentText- contiene 183.645 tokens diferentes, un vocabulario (creemos) demasiado amplio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8CRlZj9Qet0f"
      },
      "source": [
        ">># **Pipeline**\n",
        "\n",
        "#####<pre>        Nos enfrentamos a un problema de clasificación 'Supervised Learning'\n",
        "\n",
        "![](https://dzone.com/storage/temp/3307020-ml-pipeline.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1S-ugld0mZAh"
      },
      "source": [
        ">># **Preprocesamiento**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDv37XZkdyre",
        "colab_type": "text"
      },
      "source": [
        "#####<pre>      Bajamos a local el vocabulario obtenido para poder analizarlo con detalle (colab es limitado para esto)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQ75OZMEnGtG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "v_df = pd.DataFrame({\"words\": vocab})\n",
        "v_df.to_csv('/content/gdrive/My Drive/NLP/Practica Final/vocab.csv', sep = ';', decimal = '.', index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2QPmrJKhRcr",
        "colab_type": "text"
      },
      "source": [
        "#####<pre>      Tras observar el vocabulario, vemos que contiene muchas palabras 'complicadas' para nuestro trabajo:\n",
        "~~~\n",
        "          > Espacios sobrantes a izquierda y derecha de las palabras\n",
        "          > Palabras mal escritas, mayúsculas incorrectas y letras muy repetidas para inferir énfasis (gunna, YouuuuU)\n",
        "          > Números, Fechas y Horas\n",
        "          > Caracteres Especiales\n",
        "          > Emojis\n",
        "          > Menciones de usuarios (@martin)\n",
        "          > Hashtags (#lovesongs)\n",
        "          > URLs\n",
        "          > Jerga y Contracciones del lenguaje\n",
        "          > Signos de puntuación\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQt-70bMnIlJ",
        "colab_type": "text"
      },
      "source": [
        "#####<pre>      Trataremos el vocabulario combinando técnicas de [**Dimitrios Effrosynidis**](https://github.com/Deffro/text-preprocessing-techniques) y [**Adriano Carmezim**](https://github.com/Carmezim/crypto-twitter-sentiment-analysis) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdMuIxQJwLB9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# limpiamos en un orden lógico\n",
        "\n",
        "for fila in range(0, len(df1)):\n",
        "  texto = df1.loc[fila, 'SentimentText']\n",
        "\n",
        "  # Ponemos todo en minúsculas\n",
        "  texto = texto.lower()\n",
        "\n",
        "  # Eliminamos el texto unicode (\"\\u002c\", \"x96\", \"\"\")\n",
        "  texto = re.sub(r'(\\\\u[0-9A-Fa-f]+)', '', texto)       \n",
        "  texto = re.sub(r'[^\\x00-\\x7f]', '', texto)\n",
        "\n",
        "  # Convertimos los Emojis Positivos\n",
        "  texto = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\)|:\\s?D|:-D|x-?D|X-?D|@-\\)|<3|:\\*|;-?\\)|;-?D|\\(-?;)', 'EMO_POS', texto)\n",
        "  \n",
        "  # Convertimos los Emojis Negativos\n",
        "  texto = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:|:,\\(|:\\'\\(|:\"\\()|:-\\||t_t', 'EMO_NEG', texto)\n",
        "\n",
        "  # Eliminamos las URLs ya que entendemos que no aportarán información significativa al modelo\n",
        "  texto = re.sub(r'((www\\.[\\S]+)|(https?://[\\S]+))', '', texto)\n",
        "  texto = re.sub(r'((www\\.[^\\s]+)|(https?://[^\\s]+))', '', texto)\n",
        "\n",
        "  # Eliminamos las menciones de usuarios porque creemos que los nombres o alias no tendrán significación en nuestro análisis\n",
        "  texto = re.sub(r'@[\\S]+', '', texto)\n",
        "  texto = re.sub(r'@[^\\s]+', '', texto)\n",
        "\n",
        "  # Eliminamos los hashtags porque aunque quiza alguno pudiera apuntar un sentimiento, normalmente suelen ser palabras unidas #contraelcancer que no podrán aportar a nuestro análisis \n",
        "  texto = re.sub(r'#([^\\s]+)', '', texto)\n",
        "  texto = re.sub(r'#(\\S+)', '', texto)\n",
        "\n",
        "  # Eliminamos las siglas de los retweets \n",
        "  texto = re.sub(r'\\brt\\b', '', texto)\n",
        "\n",
        "  # Eliminamos los signos de puntuación\n",
        "  texto = re.sub(r'[\\'\"?!,.():;]', '', texto)\n",
        "\n",
        "  # Eliminamos los caracteres especiales\n",
        "  texto = re.sub(r'[^a-zA-z0-9\\s]', '', texto)\n",
        "  \n",
        "  for t in texto.split(' '):    \n",
        "    # Convertimos las repeticiones masivas de letras\n",
        "    texto = texto.replace(t, re.sub(r'(.)\\1+', r'\\1\\1', t))\n",
        "\n",
        "    # Convertimos la jerga\n",
        "    if t in jerga_dict:\n",
        "      texto = texto.replace(t, jerga_dict[t])\n",
        "\n",
        "  # Traducimos las contracciones\n",
        "  texto = contractions.fix(texto)\n",
        "\n",
        "  # Eliminamos los números\n",
        "  texto = re.sub(r'[0123456789]', '', texto)\n",
        "\n",
        "  # Eliminamos los espacios en blanco sobrantes\n",
        "  texto = texto.strip()\n",
        "  texto = re.sub(r'\\s+', ' ', texto)\n",
        "\n",
        "  df1.loc[fila, 'SentimentText'] = texto"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fT8ZlH7r4JR",
        "colab_type": "text"
      },
      "source": [
        "#####<pre>      Veamos como ha quedado el vocabulario"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGPOVFlWnVwi",
        "colab_type": "code",
        "outputId": "256002e8-0b8a-4d85-c3c1-8b720709365c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vocab = extraer_vocab(df1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Número de Palabras: 62981\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7xmjz65HjvI",
        "colab_type": "text"
      },
      "source": [
        "#####<pre>      La columna -SentimentText- se ha reducido notablemente, de 183.645 a 62.981 tokens diferentes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l3xFs3Gr64_0"
      },
      "source": [
        "#####<pre>      Lemmatizaremos, apoyándonos en la librería **spacy**, con objeto de normalizar el texto en lo posible"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4Hmnjdv93WC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for fila in range(0, len(df1)):\n",
        "  texto = nlp(df1.loc[fila, 'SentimentText'])\n",
        "  texto = ' '.join([t.lemma_ if t.lemma_ != '-PRON-' else t.text for t in texto])\n",
        "\n",
        "  df1.loc[fila, 'SentimentText'] = texto"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6ZEKsvHYpJW",
        "colab_type": "text"
      },
      "source": [
        "#####<pre>      Decidimos no utilizar la eliminación de stop-words\n",
        "~~~\n",
        "          Consideramos que muchas de estas palabras pueden ser significativas para nuestro sentiment analysis\n",
        "~~~"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJKsChKVAOKq",
        "colab_type": "code",
        "outputId": "8eb3bb94-1626-44f3-cb9f-c244a6d97502",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "# Veamos como ha quedado nuestro dataset\n",
        "print(\"\\t\\t NULOS\\n\", \"\\t\\t-------\\n\", df1.isnull().any(), \"\\n\\n\", sep = '')\n",
        "\n",
        "df1.describe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t\t NULOS\n",
            "\t\t-------\n",
            "Sentiment        False\n",
            "SentimentText     True\n",
            "dtype: bool\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>SentimentText</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>99989</td>\n",
              "      <td>99444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>2</td>\n",
              "      <td>95802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>POS</td>\n",
              "      <td>thank</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>56457</td>\n",
              "      <td>256</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Sentiment SentimentText\n",
              "count      99989         99444\n",
              "unique         2         95802\n",
              "top          POS         thank\n",
              "freq       56457           256"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K10-kk7RAicu",
        "colab_type": "text"
      },
      "source": [
        "#####<pre>      Como nos temíamos, al eliminar tantos tokens suplerfluos se han vaciado totalmente algunos -SentimentText-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nn8LyKKJBQIf",
        "colab_type": "code",
        "outputId": "274b4b5a-235d-4c20-87c4-f10ce15e9fa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "print(f\"SentimentText está a 'null' en {df1['SentimentText'].isnull().sum()} registros\\nProcedemos a eliminar estos registros que han quedado inservibles\\n\\n\")\n",
        "\n",
        "# Procedemos a eliminar los nulos\n",
        "df1 = df1.drop(df1[(df1['SentimentText'].isnull())].index)\n",
        "\n",
        "df1.describe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SentimentText esta a 'null' en 545 registros\n",
            "Procedemos a eliminar estos registros que han quedado inservibles\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>SentimentText</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>99444</td>\n",
              "      <td>99444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>2</td>\n",
              "      <td>95802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>POS</td>\n",
              "      <td>thank</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>56124</td>\n",
              "      <td>256</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Sentiment SentimentText\n",
              "count      99444         99444\n",
              "unique         2         95802\n",
              "top          POS         thank\n",
              "freq       56124           256"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Nh3cY7OTc8M6"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ju8g-BmQiQeT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# guardamos el trabajo para evitar reprocesos posteriores\n",
        "\n",
        "df1.to_csv('/content/gdrive/My Drive/NLP/Practica Final/df1.csv', sep = ';', decimal = '.', index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hQX79EYVc8R_"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RLIVrZXm5Hn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# desde aqui cargamos el dataset ya preprocesado (limpio)\n",
        "\n",
        "df1 = pd.read_csv('/content/gdrive/My Drive/NLP/Practica Final/df1.csv', sep = ';', decimal = '.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r2fmLTm3m6V4"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "N1olNKMQkpHT"
      },
      "source": [
        ">># **Partición del Dataset**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vasDEu8AptRU",
        "colab_type": "text"
      },
      "source": [
        "#####<pre>      Reducimos el dataset a su quinta parte, ya que casi 100K filas serán demasiadas para entrenar los modelos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hc2bsAXOqGsV",
        "colab_type": "code",
        "outputId": "51689670-7b94-4cd4-cb64-59f067347dad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# aprovechamos la aleatoriedad de train_test_split para elegir la quinta parte de los registros del dataset\n",
        "\n",
        "resto, df2 = train_test_split(df1, test_size = 0.2, shuffle = True, random_state = 0)\n",
        "\n",
        "print(f\"El dataset reducido a su quinta parte tiene {df2.shape[0]} filas\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "El dataset reducido a su quinta parte tiene 19889 filas\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RTwJlgBEcct4"
      },
      "source": [
        "#####<pre>      Realizamos las tres particiones:\n",
        "~~~\n",
        "          > un 70% del dataset servirá para el entrenamiento\n",
        "          > un 10% del dataset servirá para la validación\n",
        "          > un 20% del dataset servirá para el testeo\n",
        "~~~"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCspYZUKmjPg",
        "colab_type": "code",
        "outputId": "a320a81b-699e-4d13-bd4c-442697c14733",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "trainT, test = train_test_split(df2, test_size = 0.2, shuffle = True, random_state = 0)\n",
        "train, validate = train_test_split(trainT, test_size = 0.1, shuffle = True, random_state = 0)\n",
        "\n",
        "print(f\"El conjunto Train    ha quedado con {train.shape[0]} filas y {train.shape[1]} columnas\")\n",
        "print(f\"El conjunto Validate ha quedado con  {validate.shape[0]} filas y {validate.shape[1]} columnas\")\n",
        "print(f\"El conjunto Test     ha quedado con  {test.shape[0]} filas y {test.shape[1]} columnas\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "El conjunto Train    ha quedado con 14319 filas y 2 columnas\n",
            "El conjunto Validate ha quedado con  1592 filas y 2 columnas\n",
            "El conjunto Test     ha quedado con  3978 filas y 2 columnas\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQbJHvKwno92",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# preparamos los datos\n",
        "\n",
        "# para los modelos de Machine Learning utilizaremos para entrenar la partición trainT (train + validate)\n",
        "values_trnT = trainT.values\n",
        "\n",
        "X_trnT = values_trnT[:, 1]\n",
        "y_trnT = values_trnT[:, 0]\n",
        "\n",
        "# para los modelos de Deep Learning utilizaremos en el entrenamiento train y validate por separado\n",
        "values_trn = train.values\n",
        "values_vld = validate.values\n",
        "values_tst = test.values\n",
        "\n",
        "X_trn = values_trn[:, 1]\n",
        "y_trn = values_trn[:, 0]\n",
        "\n",
        "X_vld = values_vld[:, 1]\n",
        "y_vld = values_vld[:, 0]\n",
        "\n",
        "X_tst = values_tst[:, 1]\n",
        "y_tst = values_tst[:, 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPw8jkZDYvqx",
        "colab_type": "text"
      },
      "source": [
        ">># **Modelo Machine Learning *Naive Bayes***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8zPvAFVNzqOT"
      },
      "source": [
        "#####<pre>      El primer modelo predictor ML que implementaremos será un clasificador *Naive Bayes*:\n",
        "~~~\n",
        "          > Generaremos un Pipeline con un extractor de características y el modelo a entrenar\n",
        "~~~"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gU1bSBL70Gml",
        "colab_type": "code",
        "outputId": "f257b969-d4dc-422b-d6e6-2c39278a3ed5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "source": [
        "pipeline_nb = Pipeline([('extractor', CountVectorizer()),\n",
        "                        ('modelo', MultinomialNB())])\n",
        "\n",
        "# veamos que parámetros podemos configurar\n",
        "pipeline_nb.get_params()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'extractor': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
              "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
              "                 ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
              "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                 tokenizer=None, vocabulary=None),\n",
              " 'extractor__analyzer': 'word',\n",
              " 'extractor__binary': False,\n",
              " 'extractor__decode_error': 'strict',\n",
              " 'extractor__dtype': numpy.int64,\n",
              " 'extractor__encoding': 'utf-8',\n",
              " 'extractor__input': 'content',\n",
              " 'extractor__lowercase': True,\n",
              " 'extractor__max_df': 1.0,\n",
              " 'extractor__max_features': None,\n",
              " 'extractor__min_df': 1,\n",
              " 'extractor__ngram_range': (1, 1),\n",
              " 'extractor__preprocessor': None,\n",
              " 'extractor__stop_words': None,\n",
              " 'extractor__strip_accents': None,\n",
              " 'extractor__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              " 'extractor__tokenizer': None,\n",
              " 'extractor__vocabulary': None,\n",
              " 'memory': None,\n",
              " 'modelo': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
              " 'modelo__alpha': 1.0,\n",
              " 'modelo__class_prior': None,\n",
              " 'modelo__fit_prior': True,\n",
              " 'steps': [('extractor',\n",
              "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
              "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
              "                   ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
              "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                   tokenizer=None, vocabulary=None)),\n",
              "  ('modelo', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
              " 'verbose': False}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Mz2gI2Cp6VfT"
      },
      "source": [
        "#####<pre>      Para el ***extractor*** dejaremos la mayoría de valores a parametrizar por defecto, excepto:\n",
        "~~~\n",
        "          > max_df:      para asegurarnos que se ignoren palabras altamente repetidas (compensando así no haber filtrado las stop-words)\n",
        "          > ngram_range: para probar con diferentes tamaños de n-grams\n",
        "~~~\n",
        "#####<pre>      Para el ***modelo*** dejaremos todos los valores a parametrizar por defecto, excepto:\n",
        "~~~\n",
        "          > alpha: para probar con diferentes regularizaciones\n",
        "~~~"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VK-YpSG50akN",
        "colab_type": "code",
        "outputId": "3c1d4a2e-a7ca-4666-a64d-518e5a7ba8ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        }
      },
      "source": [
        "parameters_grid_nb = {'extractor__max_df': (0.5, 0.75, 1.0),\n",
        "                      'extractor__ngram_range': ((1, 1), (1, 2), (1, 3), (2, 3), (1, 4)),\n",
        "                      'modelo__alpha': (1, 1e-1, 1e-2, 1e-3)}\n",
        "\n",
        "# definimos nuestra grid de entreno\n",
        "gs_nb = GridSearchCV(pipeline_nb, parameters_grid_nb, n_jobs = -1, verbose = 1)\n",
        "\n",
        "# entrenamos la grid para obtener la mejor parametrización\n",
        "# denotar que para los modelos de Machine Learning utilizaremos para el entrenamiento los conjuntos -train- y -validate- juntos\n",
        "gs_nb.fit(X_trnT, y_trnT)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   24.2s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:  2.8min\n",
            "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  4.4min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=None, error_score=nan,\n",
              "             estimator=Pipeline(memory=None,\n",
              "                                steps=[('extractor',\n",
              "                                        CountVectorizer(analyzer='word',\n",
              "                                                        binary=False,\n",
              "                                                        decode_error='strict',\n",
              "                                                        dtype=<class 'numpy.int64'>,\n",
              "                                                        encoding='utf-8',\n",
              "                                                        input='content',\n",
              "                                                        lowercase=True,\n",
              "                                                        max_df=1.0,\n",
              "                                                        max_features=None,\n",
              "                                                        min_df=1,\n",
              "                                                        ngram_range=(1, 1),\n",
              "                                                        preprocessor=None,\n",
              "                                                        stop_words=None,\n",
              "                                                        strip_accents=None,\n",
              "                                                        token_patte...\n",
              "                                                        vocabulary=None)),\n",
              "                                       ('modelo',\n",
              "                                        MultinomialNB(alpha=1.0,\n",
              "                                                      class_prior=None,\n",
              "                                                      fit_prior=True))],\n",
              "                                verbose=False),\n",
              "             iid='deprecated', n_jobs=-1,\n",
              "             param_grid={'extractor__max_df': (0.5, 0.75, 1.0),\n",
              "                         'extractor__ngram_range': ((1, 1), (1, 2), (1, 3),\n",
              "                                                    (2, 3), (1, 4)),\n",
              "                         'modelo__alpha': (1, 0.1, 0.01, 0.001)},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8-W8Bzq-p9z",
        "colab_type": "code",
        "outputId": "bd4eb3e7-da75-4241-fe6b-7d3d9131380a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# presentamos la mejor parametrización obtenida para el modelo NB\n",
        "best_params_nb = gs_nb.best_estimator_.get_params()\n",
        "\n",
        "print(\"La mejor parametrización obtenida por la Grid para el modelo NB es:\")\n",
        "for param_name in sorted(parameters_grid_nb.keys()):\n",
        "    print(\"\\t%s: %r\" % (param_name, best_params_nb[param_name]))\n",
        "print(\"\\nMejor score: %0.3f\" % gs_nb.best_score_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "La mejor parametrización obtenida por la Grid para el modelo NB es:\n",
            "\textractor__max_df: 0.5\n",
            "\textractor__ngram_range: (1, 2)\n",
            "\tmodelo__alpha: 1\n",
            "\n",
            "Mejor score: 0.741\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enBQXvvDzmyg",
        "colab_type": "text"
      },
      "source": [
        "#####<pre>     Nos parece que el mejor ***score*** obtenido en train por el modelo *Naive Bayes* es decente pero no demasiado bueno"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yAbBrkiQTZxh"
      },
      "source": [
        "#####<pre>     Parametrizaremos el ***pipeline*** con estos *mejores parámetros* obtenidos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NG9ZT7aLTlHS",
        "colab_type": "code",
        "outputId": "893f003d-0fa1-471b-98a2-5ed3ca09f18b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "# para sucesivas sesiones, evitamos entrenar de nuevo la grid imponiendo manualmente los mejores parámetros obtenidos\n",
        "# pipeline_nb.set_params(**best_params_nb)\n",
        "\n",
        "pipeline_nb = Pipeline([('extractor', CountVectorizer(max_df = 0.5, ngram_range = (1, 2))),\n",
        "                        ('modelo', MultinomialNB(alpha = 1))])\n",
        "\n",
        "pipeline_nb.fit(X_trnT, y_trnT)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('extractor',\n",
              "                 CountVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=0.5,\n",
              "                                 max_features=None, min_df=1,\n",
              "                                 ngram_range=(1, 2), preprocessor=None,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, vocabulary=None)),\n",
              "                ('modelo',\n",
              "                 MultinomialNB(alpha=1, class_prior=None, fit_prior=True))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ESQ9RDTcTgVL"
      },
      "source": [
        "#####<pre>     Evaluaremos el modelo contra el conjunto de datos de test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bONmDztHcOxF",
        "colab_type": "code",
        "outputId": "1720aaed-7b66-4be0-c1b8-a70f95646564",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "scoring_nb = pipeline_nb.score(X_tst, y_tst)\n",
        "print(\"El scoring de accuracy obtenido para el modelo NB en test es %0.3f\" % scoring_nb)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "El scoring de accuracy obtenido para el modelo NB en test es 0.743\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QfijLqyxY2m5"
      },
      "source": [
        "#####<pre>      Veamos su matriz de confusión"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5yIX1P8QGAB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# función para pintar la matriz de confusión \n",
        "def plot_confusion_matrix(confmat, modelo, fsc):\n",
        "    fig, ax = plt.subplots(figsize = (6, 6))\n",
        "    ax.matshow(confmat, cmap = plt.cm.Purples, alpha = 0.5)\n",
        "\n",
        "    for i in range(confmat.shape[0]):\n",
        "        for j in range(confmat.shape[1]):\n",
        "            ax.text(x = j, y = i, s = confmat[i, j], va = 'center', ha = 'center', size = 16)\n",
        "    \n",
        "    ax.xaxis.set_ticks_position('bottom')\n",
        "    title = modelo + f'\\nF1-SCORE: %0.3f' % fsc\n",
        "\n",
        "    plt.title(title, size = 20)   \n",
        "    plt.xlabel('predicción', size = 16)\n",
        "    plt.ylabel('etiqueta', size = 16)\n",
        "\n",
        "    target_names = ['NEG', \"POS\"]\n",
        "    tick_marks = np.arange(len(target_names))\n",
        "    plt.xticks(tick_marks, target_names)\n",
        "    plt.yticks(tick_marks, target_names)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# función para obtener el F1-SCORE\n",
        "def calculo_fsc(tp, fp, fn):\n",
        "  sen = tp / (tp + fn)\n",
        "  ppv = tp / (tp + fp)\n",
        "  fsc = 2 * (sen * ppv / (sen + ppv))\n",
        "\n",
        "  return fsc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0RdewcdQ9f3",
        "colab_type": "code",
        "outputId": "ab986174-1d93-40f0-b5b8-d105999e6fe6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        }
      },
      "source": [
        "y_pred_nb = pipeline_nb.predict(X_tst)\n",
        "confmat_nb = confusion_matrix(y_tst, y_pred_nb)\n",
        "tn, fp, fn, tp = confusion_matrix(y_tst, y_pred_nb).ravel()\n",
        "fsc_nb = calculo_fsc(tp, fp, fn)\n",
        "plot_confusion_matrix(confmat_nb, 'NAIVE BAYES', fsc_nb)\n",
        "\n",
        "print(f\"\\nEl conjunto de test tiene {tn + fp + fn + tp} registros:\")\n",
        "print(f\"\\t · {tn} + {fp} = {tn + fp} mensajes negativos\\n\\t · {fn} + {tp} = {tp + fn} mensajes positivos\")\n",
        "print(f\"\\nLa matriz de confusión nos dice que de los {tn + fp} mensajes con etiqueta NEG:\")\n",
        "print(f\"\\t · {tn} se están prediciendo correctamente [predicción = NEG]\")\n",
        "print(f\"\\t · {fp} se están prediciendo como falsos positivos [predicción = POS]\")\n",
        "print(f\"\\nY que de los {tp + fn} mensajes con etiqueta POS:\")\n",
        "print(f\"\\t · {tp} se están prediciendo correctamente [predicción = POS]\")\n",
        "print(f\"\\t · {fn} se están prediciendo como falsos negativos [predicción = NEG]\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAGoCAYAAAC9qKUEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nACncRUQCpHAXEQmQwl1EJED/H5SZ8nmNA4i3AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "El conjunto de test tiene 3978 registros:\n",
            "\t · 1193 + 564 = 1757 mensajes negativos\n",
            "\t · 459 + 1762 = 2221 mensajes positivos\n",
            "\n",
            "La matriz de confusión nos dice que de los 1757 mensajes con etiqueta NEG:\n",
            "\t · 1193 se están prediciendo correctamente [predicción = NEG]\n",
            "\t · 564 se están prediciendo como falsos positivos [predicción = POS]\n",
            "\n",
            "Y que de los 2221 mensajes con etiqueta POS:\n",
            "\t · 1762 se están prediciendo correctamente [predicción = POS]\n",
            "\t · 459 se están prediciendo como falsos negativos [predicción = NEG]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sYHR3wb8c3rV"
      },
      "source": [
        ">># **Modelo Machine Learning *Support Vector Machine (SVM)***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Mr-S6ioidOz3"
      },
      "source": [
        "#####<pre>      El segundo modelo predictor ML que implementaremos será un clasificador *SVM*:\n",
        "~~~\n",
        "          > Generaremos un Pipeline con un extractor de características y el modelo a entrenar (las SVM ya integran un parámetro regularizador)\n",
        "~~~"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "3e8f316d-acf7-4c9b-dd1d-3d1a9b85f70c",
        "id": "3TBx9xumdXrN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 969
        }
      },
      "source": [
        "pipeline_svm = Pipeline([('extractor', CountVectorizer()),\n",
        "                         ('modelo', SVC(class_weight = 'balanced', probability = True))])\n",
        "\n",
        "# veamos que parámetros podemos configurar\n",
        "pipeline_svm.get_params()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'extractor': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
              "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
              "                 ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
              "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                 tokenizer=None, vocabulary=None),\n",
              " 'extractor__analyzer': 'word',\n",
              " 'extractor__binary': False,\n",
              " 'extractor__decode_error': 'strict',\n",
              " 'extractor__dtype': numpy.int64,\n",
              " 'extractor__encoding': 'utf-8',\n",
              " 'extractor__input': 'content',\n",
              " 'extractor__lowercase': True,\n",
              " 'extractor__max_df': 1.0,\n",
              " 'extractor__max_features': None,\n",
              " 'extractor__min_df': 1,\n",
              " 'extractor__ngram_range': (1, 1),\n",
              " 'extractor__preprocessor': None,\n",
              " 'extractor__stop_words': None,\n",
              " 'extractor__strip_accents': None,\n",
              " 'extractor__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              " 'extractor__tokenizer': None,\n",
              " 'extractor__vocabulary': None,\n",
              " 'memory': None,\n",
              " 'modelo': SVC(C=1.0, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
              "     decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
              "     max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
              "     verbose=False),\n",
              " 'modelo__C': 1.0,\n",
              " 'modelo__break_ties': False,\n",
              " 'modelo__cache_size': 200,\n",
              " 'modelo__class_weight': 'balanced',\n",
              " 'modelo__coef0': 0.0,\n",
              " 'modelo__decision_function_shape': 'ovr',\n",
              " 'modelo__degree': 3,\n",
              " 'modelo__gamma': 'scale',\n",
              " 'modelo__kernel': 'rbf',\n",
              " 'modelo__max_iter': -1,\n",
              " 'modelo__probability': True,\n",
              " 'modelo__random_state': None,\n",
              " 'modelo__shrinking': True,\n",
              " 'modelo__tol': 0.001,\n",
              " 'modelo__verbose': False,\n",
              " 'steps': [('extractor',\n",
              "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
              "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
              "                   ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
              "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                   tokenizer=None, vocabulary=None)),\n",
              "  ('modelo',\n",
              "   SVC(C=1.0, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
              "       decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
              "       max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
              "       verbose=False))],\n",
              " 'verbose': False}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zVS6s1w_dy18"
      },
      "source": [
        "#####<pre>      Para el ***extractor*** dejaremos la mayoría de valores a parametrizar por defecto, excepto:\n",
        "~~~\n",
        "          > analyzer:    probaremos con palabras y caracteres\n",
        "          > max_df:      para asegurarnos que se ignoren palabras altamente repetidas (compensando así no haber filtrado las stop-words)\n",
        "          > ngram_range: para probar con diferentes tamaños de n-grams\n",
        "~~~\n",
        "#####<pre>      Para el ***modelo*** dejaremos la mayoría de valores a parametrizar por defecto, excepto:\n",
        "~~~\n",
        "          > kernel: para probar lineal y exponencial\n",
        "          > C:      para probar diferentes grados de regularización\n",
        "~~~"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fw7T-S9enCKu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# esta es la grid de parámetros con la que deseabamos entrenar la SVM\n",
        "# no entendemos porqué (probablemente por falta de recursos) no nos ha sido posible finalizar ningún entrenamiento con estos juegos de valores y lo hemos intentado repetidas veces\n",
        "# de hecho (como se puede observar en la siguiente celda) para poder ver una finalización del entrenamiento hemos tenido que reducirlos al mínimo e incluso disminuir el K-fold a 3 \n",
        "\n",
        "parameters_grid_svm = {'extractor__analyzer': ('word', 'char'),\n",
        "                       'extractor__max_df': (0.5, 0.75, 1.0),\n",
        "                       'extractor__ngram_range': ((1, 3), (2, 3), (1, 4)),\n",
        "                       'modelo__kernel': ('rbf', 'linear'),\n",
        "                       'modelo__C': (1, 1e-1)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "fdb8fc01-bc32-4eaa-eb80-50d5fdd8b725",
        "id": "HLEOHwH1dy19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "source": [
        "# grid reducida a su mínima expresión \n",
        "parameters_grid_svm = {'modelo__C': (1, 1e-1)}\n",
        "\n",
        "# definimos nuestra grid de entreno con disminución del K-fold a 3\n",
        "gs_svm = GridSearchCV(pipeline_svm, parameters_grid_svm, n_jobs = -1, verbose = 1, cv = 3)\n",
        "\n",
        "# entrenamos la grid para obtener la mejor parametrización\n",
        "# denotar que para los modelos de Machine Learning utilizaremos para el entrenamiento los conjuntos -train- y -validate- juntos.\n",
        "gs_svm.fit(X_trnT, y_trnT)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:  7.2min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=3, error_score=nan,\n",
              "             estimator=Pipeline(memory=None,\n",
              "                                steps=[('extractor',\n",
              "                                        CountVectorizer(analyzer='word',\n",
              "                                                        binary=False,\n",
              "                                                        decode_error='strict',\n",
              "                                                        dtype=<class 'numpy.int64'>,\n",
              "                                                        encoding='utf-8',\n",
              "                                                        input='content',\n",
              "                                                        lowercase=True,\n",
              "                                                        max_df=1.0,\n",
              "                                                        max_features=None,\n",
              "                                                        min_df=1,\n",
              "                                                        ngram_range=(1, 1),\n",
              "                                                        preprocessor=None,\n",
              "                                                        stop_words=None,\n",
              "                                                        strip_accents=None,\n",
              "                                                        token_pattern=...\n",
              "                                            cache_size=200,\n",
              "                                            class_weight='balanced', coef0=0.0,\n",
              "                                            decision_function_shape='ovr',\n",
              "                                            degree=3, gamma='scale',\n",
              "                                            kernel='rbf', max_iter=-1,\n",
              "                                            probability=True, random_state=None,\n",
              "                                            shrinking=True, tol=0.001,\n",
              "                                            verbose=False))],\n",
              "                                verbose=False),\n",
              "             iid='deprecated', n_jobs=-1, param_grid={'modelo__C': (1, 0.1)},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "6664a853-71bb-4ef8-f2e3-c26e6185da5e",
        "id": "5itE39zhgGKl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# presentamos la mejor parametrización obtenida para el modelo SVM\n",
        "best_params_svm = gs_svm.best_estimator_.get_params()\n",
        "\n",
        "print(\"La mejor parametrización obtenida por la Grid para el modelo SVM es:\")\n",
        "for param_name in sorted(parameters_grid_svm.keys()):\n",
        "    print(\"\\t%s: %r\" % (param_name, best_params_svm[param_name]))\n",
        "print(\"\\nMejor score: %0.3f\" % gs_svm.best_score_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "La mejor parametrización obtenida por la Grid para el modelo SVM es:\n",
            "\tmodelo__C: 1\n",
            "\n",
            "Mejor score: 0.734\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fuF1VbOUgGKp"
      },
      "source": [
        "#####<pre>     Parece que el mejor ***score*** obtenido en train por el modelo *SVM* [0.734] es peor que el del *Naive Bayes*\n",
        "~~~\n",
        "          Debemos indicar (tal y como se puede ver en las celdas de entrenamiento de este modelo SVM) que nos ha sido imposible entrenarlo\n",
        "          con la parametrización deseada\n",
        "          Para poder ver finalizar el entrenamientos tuvimos que reducir la parametrización al mínimo e incluso disminuir el K-fold a 3\n",
        "          Y el resultado fue la parametrización por defecto del Pipeline\n",
        "          Muy probablemente, conociendo la potencia de las Support Vector Machines, de haber podido ejecutar un mejor entrenamiento, como\n",
        "          pretendiamos, con varios parámetros y diferentes valores, su score final hubiera mejorado bastante al del modelo Naive Bayes\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "09J5jcJ7VKhv"
      },
      "source": [
        "#####<pre>     Parametrizaremos el ***pipeline*** con estos *mejores parámetros* obtenidos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mdIqaGnVRNl",
        "colab_type": "code",
        "outputId": "bf56e453-b02c-42b6-94df-388cf46950ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "# para sucesivas sesiones, evitamos entrenar de nuevo la grid imponiendo manualmente los mejores parámetros obtenidos\n",
        "# pipeline_svm.set_params(**best_params_svm)\n",
        "\n",
        "pipeline_svm = Pipeline([('extractor', CountVectorizer()),\n",
        "                         ('modelo', SVC(C = 1, class_weight = 'balanced', probability = True))])\n",
        "\n",
        "pipeline_svm.fit(X_trnT, y_trnT)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('extractor',\n",
              "                 CountVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=1.0,\n",
              "                                 max_features=None, min_df=1,\n",
              "                                 ngram_range=(1, 1), preprocessor=None,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, vocabulary=None)),\n",
              "                ('modelo',\n",
              "                 SVC(C=1, break_ties=False, cache_size=200,\n",
              "                     class_weight='balanced', coef0=0.0,\n",
              "                     decision_function_shape='ovr', degree=3, gamma='scale',\n",
              "                     kernel='rbf', max_iter=-1, probability=True,\n",
              "                     random_state=None, shrinking=True, tol=0.001,\n",
              "                     verbose=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4gry8II5VKl2"
      },
      "source": [
        "#####<pre>     Evaluaremos el modelo contra el conjunto de datos de test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a95b38b3-88f6-40be-dfc5-c3cc520e105e",
        "id": "a8ks0yukgGKq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "scoring_svm = pipeline_svm.score(X_tst, y_tst)\n",
        "print(\"El scoring de accuracy obtenido para el modelo SVM en test es %0.3f\" % scoring_svm)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "El scoring de accuracy obtenido para el modelo SVM en test es 0.761\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "874yO7FE75Gc",
        "colab_type": "text"
      },
      "source": [
        "#####<pre>     Sorprendentemente, la *SVM* con su parametrización por defecto, supera el score del *Naive Bayes*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjiVrjzJP-gM",
        "colab_type": "text"
      },
      "source": [
        "#####<pre>      Veamos su matriz de confusión"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "9479ade6-570a-457b-eab3-13c33cea0395",
        "id": "lTZwGXS4VhRg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        }
      },
      "source": [
        "y_pred_svm = pipeline_svm.predict(X_tst)\n",
        "confmat_svm = confusion_matrix(y_tst, y_pred_svm)\n",
        "tn, fp, fn, tp = confusion_matrix(y_tst, y_pred_svm).ravel()\n",
        "fsc_svm = calculo_fsc(tp, fp, fn)\n",
        "plot_confusion_matrix(confmat_svm, 'SVM', fsc_svm)\n",
        "\n",
        "print(f\"\\nEl conjunto de test tiene {tn + fp + fn + tp} registros:\")\n",
        "print(f\"\\t · {tn} + {fp} = {tn + fp} mensajes negativos\\n\\t · {fn} + {tp} = {tp + fn} mensajes positivos\")\n",
        "print(f\"\\nLa matriz de confusión nos dice que de los {tn + fp} mensajes con etiqueta NEG:\")\n",
        "print(f\"\\t · {tn} se están prediciendo correctamente [predicción = NEG]\")\n",
        "print(f\"\\t · {fp} se están prediciendo como falsos positivos [predicción = POS]\")\n",
        "print(f\"\\nY que de los {tp + fn} mensajes con etiqueta POS:\")\n",
        "print(f\"\\t · {tp} se están prediciendo correctamente [predicción = POS]\")\n",
        "print(f\"\\t · {fn} se están prediciendo como falsos negativos [predicción = NEG]\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAGoCAYAAAC9qKUEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "El conjunto de test tiene 3978 registros:\n",
            "\t · 1285 + 472 = 1757 mensajes negativos\n",
            "\t · 477 + 1744 = 2221 mensajes positivos\n",
            "\n",
            "La matriz de confusión nos dice que de los 1757 mensajes con etiqueta NEG:\n",
            "\t · 1285 se están prediciendo correctamente [predicción = NEG]\n",
            "\t · 472 se están prediciendo como falsos positivos [predicción = POS]\n",
            "\n",
            "Y que de los 2221 mensajes con etiqueta POS:\n",
            "\t · 1744 se están prediciendo correctamente [predicción = POS]\n",
            "\t · 477 se están prediciendo como falsos negativos [predicción = NEG]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7IOWSFzW-gP1"
      },
      "source": [
        ">># **Modelo Deep Learning *Convolutional Neural Network (CNN)***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wqYxX0BdOYqi"
      },
      "source": [
        "#####<pre>      El primer modelo predictor DL que implementaremos será una Red Convolucional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTCS_x9Edk-F",
        "colab_type": "text"
      },
      "source": [
        "#####<pre>      Pero lo primero será preparar los datos de entrenamiento para el Deep Learning\n",
        "~~~\n",
        "          Este preprocesamiento en el que tokenizaremos los registros para extraer el vocabulario y marcar el umbral de descarte de\n",
        "          registros outliers, lo realizaremos únicamente sobre los 14.319 registros del conjunto de entrenamiento\n",
        "          Los conjuntos de validación y testeo serán sometidos después a la preparación basada en las referencias tomadas de train,\n",
        "          para así medir objetivamente la capacidad de predicción de los modelos\n",
        "~~~"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZAlRPP6kpFb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# función para el tokenizado de las particiones de datos\n",
        "def tokenizado(textos, clases):\n",
        "  tokens = []\n",
        "  docs_tokenizados = []\n",
        "\n",
        "  for x, y in zip(textos, clases):\n",
        "      token = [t.text for t in nlp(x)]\n",
        "      tokens += token\n",
        "      docs_tokenizados.append((token, y))\n",
        "  \n",
        "  return tokens, docs_tokenizados"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsnYApHfFesh",
        "colab_type": "code",
        "outputId": "7559ac33-d054-4d05-f24f-7e62588e4471",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# tokenizamos los datos del conjunto de train\n",
        "\n",
        "tokens, docs_trn = tokenizado(X_trn, y_trn)\n",
        "\n",
        "print(f\"Tenemos {len(docs_trn)} registros y {len(tokens)} tokens/palabras en el conjunto de train\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tenemos 14319 registros y 182855 tokens/palabras en el conjunto de train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7XQf7ZlxHmk",
        "colab_type": "code",
        "outputId": "be7878fc-3b78-46eb-bb35-d35d068f5488",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# vocabulario de los datos de train\n",
        "\n",
        "vocab_counter = Counter(tokens)\n",
        "vocab = list(vocab_counter.keys())\n",
        "\n",
        "print(f\"Tenemos {len(vocab)} tokens/palabras diferentes en el conjunto de train\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tenemos 15416 tokens/palabras diferentes en el conjunto de train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lpwd1K02Vgv",
        "colab_type": "code",
        "outputId": "d7eebfb3-6b47-4aff-8747-bec08279ff43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# tratamiento de outliers\n",
        "\n",
        "# longitud máxima de los documentos\n",
        "maxlen = max([len(x) for x, _ in docs_trn])\n",
        "\n",
        "# longitud de cada documento\n",
        "lens = [len(x) for x, _ in docs_trn]\n",
        "\n",
        "# mediana de la lista de longitudes de documento\n",
        "mediana = np.median(np.array(lens))\n",
        "media = np.mean(np.array(lens))\n",
        "\n",
        "# calculamos el límite outlier como el doble de la mediana\n",
        "maxlen = int(mediana)*2\n",
        "\n",
        "print(f\"Mediana de las longitudes de documento en train: {int(mediana)} palabras\")\n",
        "print(f\"Máxima Longitud, límite outlier: {maxlen} palabras\")\n",
        "\n",
        "# filtramos los documentos tokenizados para desechar los que tienen mas de 24 palabras\n",
        "docs_trn_ok = [(x, y) for x, y in docs_trn if len(x) <= maxlen]\n",
        "\n",
        "print(f\"\\nEl conjunto de train sin outliers queda con {len(docs_trn_ok)} documentos\")\n",
        "print(f\"Se desechan {len(docs_trn) - len(docs_trn_ok)} documentos demasiado largos.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mediana de las longitudes de documento en train: 12 palabras\n",
            "Máxima Longitud, límite outlier: 24 palabras\n",
            "\n",
            "El conjunto de train sin outliers queda con 13402 documentos\n",
            "Se desechan 917 documentos demasiado largos.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgOY9zz65hFR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# indexamos el vocabulario y las etiquetas\n",
        "w2id = {k:i for i, k in enumerate(vocab)}\n",
        "\n",
        "labels = ['NEG', 'POS']\n",
        "l2id = {label:i for i, label in enumerate(labels)}\n",
        "\n",
        "# añadimos el PAD <UNK> al vocabulario indexado para poder clasificar palabras desconocidas o que no aparezcan un mínimo de veces en el corpus\n",
        "w2id['<UNK>'] = len(w2id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_uyyuM76rk1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# función para realizar el encoder de los conjuntos de datos particionados\n",
        "def encoder_corpus(data, maxlen, vocab_counter, w2id, min_apar):\n",
        "  corpus = list()\n",
        "\n",
        "  for x, y in data:\n",
        "    texto = np.zeros((maxlen))\n",
        "    label = np.zeros(2)\n",
        "\n",
        "    if y == 'NEG':\n",
        "      label[0] = 1\n",
        "    else:\n",
        "      label[1] = 1\n",
        "\n",
        "    for i, t in enumerate(x):\n",
        "        texto[i] = w2id[t] if t in vocab_counter and vocab_counter[t] >= min_apar else w2id['<UNK>']\n",
        "\n",
        "    corpus.append((texto, label))\n",
        "  \n",
        "  x, y = zip(*corpus)\n",
        "  textos = np.array(list(x))\n",
        "  labels = np.array(list(y))\n",
        "\n",
        "  return textos, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZCnPdEkCdTm",
        "colab_type": "code",
        "outputId": "dc9dc180-5ac4-477d-deb7-5d6b6d69c17d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# establecemos que para considerar un token/palabra debe haber aparecido en el vocabulario total al menos 5 veces\n",
        "MIN_APARICIONES = 5\n",
        "\n",
        "# codificamos el conjunto de datos de entrenamiento\n",
        "textos_trn, labels_trn = encoder_corpus(docs_trn_ok, maxlen, vocab_counter, w2id, MIN_APARICIONES)\n",
        "\n",
        "print(f\"El formato de las features de train ha quedado con {textos_trn.shape[1]} columnas\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "El formato de las features de train ha quedado con 24 columnas\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hWKmlRk_SO0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# realizamos el mismo tokenizado, tratamiento de outliers y encoder para los datos de los conjuntos de validación y test, tomando como referencia el vocabulario de train\n",
        "\n",
        "_, docs_vld = tokenizado(X_vld, y_vld)\n",
        "docs_vld_ok = [(x, y) for x, y in docs_vld if len(x) <= maxlen]\n",
        "textos_vld, labels_vld  = encoder_corpus(docs_vld_ok, maxlen, vocab_counter, w2id, MIN_APARICIONES)\n",
        "\n",
        "_, docs_tst = tokenizado(X_tst, y_tst)\n",
        "docs_tst_ok = [(x, y) for x, y in docs_tst if len(x) <= maxlen]\n",
        "textos_tst, labels_tst  = encoder_corpus(docs_tst_ok, maxlen, vocab_counter, w2id, MIN_APARICIONES)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "do1WlraWKTFC"
      },
      "source": [
        "#####<pre>      Montamos nuestra arquitectura CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wt1FOS9hKcVH",
        "colab_type": "code",
        "outputId": "f2558c99-4c87-4ab7-cdd4-8f73234246e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        }
      },
      "source": [
        "# capa de entrada con 24 neuronas\n",
        "input_layer = Input(shape = (maxlen,))\n",
        "\n",
        "# capa embedding con 100 vectores clasificadores de salida\n",
        "embedding = Embedding(output_dim = 100, input_dim = len(w2id), input_length = maxlen)(input_layer)\n",
        "\n",
        "# capa de reestructuración\n",
        "reshape = Reshape((maxlen, 100, 1))(embedding)\n",
        "\n",
        "# tres capas convolucionales 2D (activadas por 'relu'), con MaxPooling (para reducir overfitting) que filtrarán en paralelo con diferentes kernel\n",
        "conv_1 = Conv2D(filters = 50, kernel_size = (1, 100), activation = 'relu', padding = 'valid')(reshape)\n",
        "mp_1 = GlobalMaxPooling2D()(conv_1)\n",
        "conv_2 = Conv2D(filters = 50, kernel_size = (2, 100), activation = 'relu', padding = 'valid')(reshape)\n",
        "mp_2 = GlobalMaxPooling2D()(conv_2)\n",
        "conv_5 = Conv2D(filters = 50, kernel_size = (5, 100), activation = 'relu', padding = 'valid')(reshape)\n",
        "mp_5 = GlobalMaxPooling2D()(conv_5)\n",
        "\n",
        "# concatenamos las salidas de las capas convolucionales para que entren en la etapa final\n",
        "concatenacion = Concatenate()([mp_1, mp_2, mp_5])\n",
        "\n",
        "# 2 capas densas (activadas por 'relu') con dropout (para reducir overfitting)\n",
        "dense_1 = Dense(100, activation = 'relu')(concatenacion)\n",
        "drop_1 = Dropout(0.5)(dense_1)\n",
        "dense_2 = Dense(100, activation = 'relu')(drop_1)\n",
        "drop_2 = Dropout(0.5)(dense_2)\n",
        "\n",
        "# capa densa de salida (softmax) con las clases a predecir\n",
        "out_layer = Dense(len(l2id), activation = 'softmax')(drop_2)\n",
        "\n",
        "# definimos el modelo CNN\n",
        "model_cnn = Model(inputs = input_layer, outputs = out_layer)\n",
        "\n",
        "# compilamos \n",
        "model_cnn.compile(loss = 'categorical_crossentropy', optimizer = 'rmsprop', metrics = ['accuracy'])\n",
        "\n",
        "model_cnn.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 24)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 24, 100)      1541700     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "reshape_1 (Reshape)             (None, 24, 100, 1)   0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 24, 1, 50)    5050        reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 23, 1, 50)    10050       reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 20, 1, 50)    25050       reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d_1 (GlobalM (None, 50)           0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d_2 (GlobalM (None, 50)           0           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d_3 (GlobalM (None, 50)           0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 150)          0           global_max_pooling2d_1[0][0]     \n",
            "                                                                 global_max_pooling2d_2[0][0]     \n",
            "                                                                 global_max_pooling2d_3[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 100)          15100       concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 100)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 100)          10100       dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 100)          0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 2)            202         dropout_2[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 1,607,252\n",
            "Trainable params: 1,607,252\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4M4MJuuQPpZ",
        "colab_type": "code",
        "outputId": "234c88c1-3008-425c-e166-db191cad483b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "# entrenamos nuestro modelo CNN 5 épocas con batch_size 256, evaluándolo contra los datos de validación \n",
        "model_cnn.fit(textos_trn, labels_trn, epochs = 5, batch_size = 256,\n",
        "              validation_data = (textos_vld, labels_vld))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 13402 samples, validate on 1473 samples\n",
            "Epoch 1/5\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "13402/13402 [==============================] - 7s 504us/step - loss: 0.6348 - acc: 0.6386 - val_loss: 0.5348 - val_acc: 0.7373\n",
            "Epoch 2/5\n",
            "13402/13402 [==============================] - 6s 425us/step - loss: 0.5214 - acc: 0.7481 - val_loss: 0.5037 - val_acc: 0.7631\n",
            "Epoch 3/5\n",
            "13402/13402 [==============================] - 6s 432us/step - loss: 0.4719 - acc: 0.7814 - val_loss: 0.5008 - val_acc: 0.7542\n",
            "Epoch 4/5\n",
            "13402/13402 [==============================] - 6s 424us/step - loss: 0.4390 - acc: 0.8008 - val_loss: 0.4981 - val_acc: 0.7631\n",
            "Epoch 5/5\n",
            "13402/13402 [==============================] - 6s 422us/step - loss: 0.4097 - acc: 0.8191 - val_loss: 0.5332 - val_acc: 0.7583\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fcbff4daf28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThX6UIztavLc",
        "colab_type": "code",
        "outputId": "58903046-b2d9-48a5-b908-d06aff76734d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "scoring_cnn = model_cnn.evaluate(textos_tst, labels_tst)\n",
        "\n",
        "print(\"El scoring de accuracy obtenido para el modelo CNN en test es %0.3f\" % scoring_cnn[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3741/3741 [==============================] - 0s 105us/step\n",
            "El scoring de accuracy obtenido para el modelo CNN en test es 0.748\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Mx3eygA9hYu6"
      },
      "source": [
        "#####<pre>     La *CNN* obtiene un scoring intermedio entre los modelos de ML *Naive Bayes* y *SVM*\n",
        "~~~\n",
        "          A la vista de la sucesion de las épocas de entrenamiento, la CNN tiende al overfitting\n",
        "          Hemos decidido no entrenar mas épocas, ni bajar el batch_size por este motivo... parece que memoriza los datos\n",
        "          de train sin mejorar nada contra el conjunto de validación\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QuKtPZSXizBq"
      },
      "source": [
        "#####<pre>     Veamos su matriz de confusión"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8-_dT8NjSHn",
        "colab_type": "code",
        "outputId": "d5be88ba-a07f-4c4b-865c-0a7bdffa9422",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        }
      },
      "source": [
        "predictions_cnn = model_cnn.predict(textos_tst)\n",
        "confmat_cnn = confusion_matrix(labels_tst.argmax(axis = 1), predictions_cnn.argmax(axis = 1))\n",
        "tn, fp, fn, tp = confusion_matrix(labels_tst.argmax(axis = 1), predictions_cnn.argmax(axis = 1)).ravel()\n",
        "fsc_cnn = calculo_fsc(tp, fp, fn)\n",
        "plot_confusion_matrix(confmat_cnn, 'CNN', fsc_cnn)\n",
        "\n",
        "print(f\"\\nEl conjunto de test tiene {tn + fp + fn + tp} registros:\")\n",
        "print(f\"\\t · {tn} + {fp} = {tn + fp} mensajes negativos\\n\\t · {fn} + {tp} = {tp + fn} mensajes positivos\")\n",
        "print(f\"\\nLa matriz de confusión nos dice que de los {tn + fp} mensajes con etiqueta NEG:\")\n",
        "print(f\"\\t · {tn} se están prediciendo correctamente [predicción = NEG]\")\n",
        "print(f\"\\t · {fp} se están prediciendo como falsos positivos [predicción = POS]\")\n",
        "print(f\"\\nY que de los {tp + fn} mensajes con etiqueta POS:\")\n",
        "print(f\"\\t · {tp} se están prediciendo correctamente [predicción = POS]\")\n",
        "print(f\"\\t · {fn} se están prediciendo como falsos negativos [predicción = NEG]\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAGoCAYAAAC9qKUEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "El conjunto de test tiene 3741 registros:\n",
            "\t · 1098 + 531 = 1629 mensajes negativos\n",
            "\t · 409 + 1703 = 2112 mensajes positivos\n",
            "\n",
            "La matriz de confusión nos dice que de los 1629 mensajes con etiqueta NEG:\n",
            "\t · 1098 se están prediciendo correctamente [predicción = NEG]\n",
            "\t · 531 se están prediciendo como falsos positivos [predicción = POS]\n",
            "\n",
            "Y que de los 2112 mensajes con etiqueta POS:\n",
            "\t · 1703 se están prediciendo correctamente [predicción = POS]\n",
            "\t · 409 se están prediciendo como falsos negativos [predicción = NEG]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2t-A_bc0ghVM"
      },
      "source": [
        ">># **Modelo Deep Learning *Arquitectura DAN***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KjVpM65IghVP"
      },
      "source": [
        "#####<pre>      El segundo modelo predictor DL que implementaremos será una Arquitectura DAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3Qc8D2-ipB5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# funciones auxiliares\n",
        "\n",
        "def mask_aware_mean(x):\n",
        "    mask = K.not_equal(K.sum(K.abs(x), axis = 2, keepdims = True), 0)\n",
        "    n = K.sum(K.cast(mask, 'float32'), axis = 1, keepdims = False)\n",
        "    x_mean = K.sum(x, axis=1, keepdims = False) / n\n",
        "    return x_mean\n",
        "\n",
        "def mask_aware_mean_output_shape(input_shape):\n",
        "    shape = list(input_shape)\n",
        "    assert len(shape) == 3\n",
        "    return (shape[0], shape[2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "7cbc6348-58aa-4c24-bab4-dad51fe54865",
        "id": "8AngIKwUicH4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "# capa de entrada con 24 neuronas\n",
        "input_layer_2 = Input(shape = (maxlen,))\n",
        "\n",
        "# capa embedding con 100 vectores clasificadores de salida\n",
        "embedding_2 = Embedding(output_dim = 100, input_dim = len(w2id), input_length = maxlen)(input_layer_2)\n",
        "\n",
        "# capa de promediado de features embeddings\n",
        "promediado = Lambda(mask_aware_mean, mask_aware_mean_output_shape, name='embedding_average')(embedding_2)\n",
        "\n",
        "# 2 capas densas (activadas por 'relu') con dropout (para reducir overfitting)\n",
        "dense_1_2 = Dense(100, activation = 'relu')(promediado)\n",
        "drop_1_2 = Dropout(0.75)(dense_1_2)\n",
        "dense_2_2 = Dense(100, activation = 'relu')(drop_1_2)\n",
        "drop_2_2 = Dropout(0.75)(dense_2_2)\n",
        "\n",
        "# capa densa de salida (softmax) con las clases a predecir\n",
        "out_layer_2 = Dense(len(l2id), activation = 'softmax')(drop_2_2)\n",
        "\n",
        "# definimos el modelo CNN\n",
        "model_dan = Model(inputs = input_layer_2, outputs = out_layer_2)\n",
        "\n",
        "# compilamos \n",
        "model_dan.compile(loss = 'categorical_crossentropy', optimizer = 'rmsprop', metrics = ['accuracy'])\n",
        "\n",
        "model_dan.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Large dropout rate: 0.75 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:Large dropout rate: 0.75 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 24)                0         \n",
            "_________________________________________________________________\n",
            "embedding_2 (Embedding)      (None, 24, 100)           1541700   \n",
            "_________________________________________________________________\n",
            "embedding_average (Lambda)   (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 2)                 202       \n",
            "=================================================================\n",
            "Total params: 1,562,102\n",
            "Trainable params: 1,562,102\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "57b134f4-a36a-4c0f-e401-e7f9b81a1ef5",
        "id": "RVeMQVA_kMdo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# entrenamos nuestro modelo DAN 5 épocas con batch_size 256, evaluándolo contra los datos de validación \n",
        "model_dan.fit(textos_trn, labels_trn, epochs = 5, batch_size = 256,\n",
        "              validation_data = (textos_vld, labels_vld))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 13402 samples, validate on 1473 samples\n",
            "Epoch 1/5\n",
            "13402/13402 [==============================] - 2s 147us/step - loss: 0.6818 - acc: 0.5622 - val_loss: 0.6690 - val_acc: 0.5743\n",
            "Epoch 2/5\n",
            "13402/13402 [==============================] - 2s 113us/step - loss: 0.6546 - acc: 0.6088 - val_loss: 0.6000 - val_acc: 0.6864\n",
            "Epoch 3/5\n",
            "13402/13402 [==============================] - 2s 116us/step - loss: 0.5850 - acc: 0.7030 - val_loss: 0.5474 - val_acc: 0.7400\n",
            "Epoch 4/5\n",
            "13402/13402 [==============================] - 2s 116us/step - loss: 0.5352 - acc: 0.7460 - val_loss: 0.5186 - val_acc: 0.7542\n",
            "Epoch 5/5\n",
            "13402/13402 [==============================] - 2s 116us/step - loss: 0.5039 - acc: 0.7686 - val_loss: 0.5142 - val_acc: 0.7542\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fcbf8f4ff98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "6d208a12-a766-47e0-c6e8-d6b3dfabd029",
        "id": "dHS1khRDkMds",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "scoring_dan = model_dan.evaluate(textos_tst, labels_tst)\n",
        "\n",
        "print(\"El scoring de accuracy obtenido para el modelo DAN en test es %0.3f\" % scoring_dan[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3741/3741 [==============================] - 0s 39us/step\n",
            "El scoring de accuracy obtenido para el modelo DAN en test es 0.758\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QsnNCoWKkMdv"
      },
      "source": [
        "#####<pre>     La *DAN* obtiene el mejor scoring de todos los modelos vistos\n",
        "~~~\n",
        "          Esta arquitectura parece que también tiende al overfitting\n",
        "          Al igual que con la CNN, hemos decidido no entrenar mas épocas, ni bajar el batch_size por este motivo\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yxGHfAfikMdw"
      },
      "source": [
        "#####<pre>     Veamos su matriz de confusión"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "9f3f4b9a-fb56-439e-8f06-b42b427294da",
        "id": "dSZTygX2kMdw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        }
      },
      "source": [
        "predictions_dan = model_dan.predict(textos_tst)\n",
        "confmat_dan = confusion_matrix(labels_tst.argmax(axis = 1), predictions_dan.argmax(axis = 1))\n",
        "tn, fp, fn, tp = confusion_matrix(labels_tst.argmax(axis = 1), predictions_dan.argmax(axis = 1)).ravel()\n",
        "fsc_dan = calculo_fsc(tp, fp, fn)\n",
        "plot_confusion_matrix(confmat_dan, 'DAN', fsc_dan)\n",
        "\n",
        "print(f\"\\nEl conjunto de test tiene {tn + fp + fn + tp} registros:\")\n",
        "print(f\"\\t · {tn} + {fp} = {tn + fp} mensajes negativos\\n\\t · {fn} + {tp} = {tp + fn} mensajes positivos\")\n",
        "print(f\"\\nLa matriz de confusión nos dice que de los {tn + fp} mensajes con etiqueta NEG:\")\n",
        "print(f\"\\t · {tn} se están prediciendo correctamente [predicción = NEG]\")\n",
        "print(f\"\\t · {fp} se están prediciendo como falsos positivos [predicción = POS]\")\n",
        "print(f\"\\nY que de los {tp + fn} mensajes con etiqueta POS:\")\n",
        "print(f\"\\t · {tp} se están prediciendo correctamente [predicción = POS]\")\n",
        "print(f\"\\t · {fn} se están prediciendo como falsos negativos [predicción = NEG]\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAGoCAYAAAC9qKUEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "El conjunto de test tiene 3741 registros:\n",
            "\t · 1171 + 458 = 1629 mensajes negativos\n",
            "\t · 448 + 1664 = 2112 mensajes positivos\n",
            "\n",
            "La matriz de confusión nos dice que de los 1629 mensajes con etiqueta NEG:\n",
            "\t · 1171 se están prediciendo correctamente [predicción = NEG]\n",
            "\t · 458 se están prediciendo como falsos positivos [predicción = POS]\n",
            "\n",
            "Y que de los 2112 mensajes con etiqueta POS:\n",
            "\t · 1664 se están prediciendo correctamente [predicción = POS]\n",
            "\t · 448 se están prediciendo como falsos negativos [predicción = NEG]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ATZGfPS6zfMJ"
      },
      "source": [
        ">># **Todos los Modelos Prediciendo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCoQiolR-l0z",
        "colab_type": "text"
      },
      "source": [
        "#####<pre>     Vamos a elegir 20 registros POS y 20 NEG, al azar, de los datos que desechamos para reducir el dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4uzyKTF-zCE",
        "colab_type": "code",
        "outputId": "ad8c0a1f-2697-4930-ee12-201b4b669070",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "values_resto = resto.values\n",
        "n_regs = 20\n",
        "\n",
        "# elegimos 20 registros 'POS' y 20 'NEG' (tenemos en cuenta que no superen el maxlen de palabras para que los modelos de depp learning puedan predecir)\n",
        "ej_POS = sample([(label, text) for label, text in values_resto if (label == 'POS' and len(text.split(' ')) <= maxlen)], n_regs)\n",
        "ej_NEG = sample([(label, text) for label, text in values_resto if (label == 'NEG' and len(text.split(' ')) <= maxlen)], n_regs)\n",
        "\n",
        "# preparacion para ML\n",
        "corpus = ej_POS + ej_NEG\n",
        "y, x = zip(*corpus)\n",
        "textos_ml = np.array(list(x), dtype = object)\n",
        "labels_ml = np.array(list(y), dtype = object)\n",
        "\n",
        "# preparacion para DL\n",
        "_, textos_tk = tokenizado(textos_ml, labels_ml)\n",
        "textos_tk_ok = [(x, y) for x, y in textos_tk if len(x) <= maxlen]\n",
        "textos_dl, _  = encoder_corpus(textos_tk_ok, maxlen, vocab_counter, w2id, MIN_APARICIONES)\n",
        "\n",
        "for i in range(0, len(ej_POS)):\n",
        "  print(ej_POS[i], \"\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\", ej_NEG[i]) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('POS', 'follow me so i can dm you my new like when i call and i get a personal greeting') \n",
            "\t\t\t\t\t\t\t\t\t\t\t ('NEG', 'i can haz umbrella but i be already damp from go to asda I will come and sing at you another time')\n",
            "('POS', 'goodnight I be from vancouver bc') \n",
            "\t\t\t\t\t\t\t\t\t\t\t ('NEG', 'no more food for you')\n",
            "('POS', 'people talk abt other people kidsall child be precious') \n",
            "\t\t\t\t\t\t\t\t\t\t\t ('NEG', 'aww poor grant look cool tho')\n",
            "('POS', 'we end up do very little shopping and more wandering than anything my hubby wear me outi can not keep up with him') \n",
            "\t\t\t\t\t\t\t\t\t\t\t ('NEG', 'but its less than hour from you')\n",
            "('POS', 'I be glad to hear your theory be at least well found so then who would apldeap be') \n",
            "\t\t\t\t\t\t\t\t\t\t\t ('NEG', 'I have be okay but a little sad because i be graduate i do not want to leave hsh')\n",
            "('POS', 'basically and josh and mcnasty too') \n",
            "\t\t\t\t\t\t\t\t\t\t\t ('NEG', 'aghh i can not find the remote')\n",
            "('POS', 'i think it be an awesome thing') \n",
            "\t\t\t\t\t\t\t\t\t\t\t ('NEG', 'hmm okayy I be really sorry to hear about it why do you thikn she would have do that s')\n",
            "('POS', 'do you tell your mom bout our birdmaniac twin thing') \n",
            "\t\t\t\t\t\t\t\t\t\t\t ('NEG', 'i can not either i would cry but i will not haha')\n",
            "('POS', 'oyess glambert will kill that drake guy') \n",
            "\t\t\t\t\t\t\t\t\t\t\t ('NEG', 'the batt to my internet connection die guess its back to the mobile')\n",
            "('POS', 'those weird link continue down the page for a loong time very strange but convenient make my order') \n",
            "\t\t\t\t\t\t\t\t\t\t\t ('NEG', 'i be do not worry miss your booty face')\n",
            "('POS', 'chillin at homewatche a movie on tv amp make some quothealthifiedquot streusel coffee cake') \n",
            "\t\t\t\t\t\t\t\t\t\t\t ('NEG', 'i want to take piece of the past and bring them to the presentthing would changea life without youthat would not be the same')\n",
            "('POS', 'well if it mean i get to see you') \n",
            "\t\t\t\t\t\t\t\t\t\t\t ('NEG', 'a')\n",
            "('POS', 'so can you help me') \n",
            "\t\t\t\t\t\t\t\t\t\t\t ('NEG', 'my baby but i may have to change')\n",
            "('POS', 'quotdream of a well world because its comingquot someone could live off of those word dude') \n",
            "\t\t\t\t\t\t\t\t\t\t\t ('NEG', 'your link be break')\n",
            "('POS', 'awwe as lease you have fun last night babygirl') \n",
            "\t\t\t\t\t\t\t\t\t\t\t ('NEG', 'ouchh')\n",
            "('POS', 'oh yes i be sleep in too not because I have work alot which i have not but just because') \n",
            "\t\t\t\t\t\t\t\t\t\t\t ('NEG', 'can not hahaha shit damnit')\n",
            "('POS', 'thank for the am now follow you') \n",
            "\t\t\t\t\t\t\t\t\t\t\t ('NEG', 'can not wait for next set of refreshment be starvin lunch have mushroom in it yuk')\n",
            "('POS', 'oh my he s be naughty') \n",
            "\t\t\t\t\t\t\t\t\t\t\t ('NEG', 'i hate thunder i wake up really scared just now cus of it')\n",
            "('POS', 'thank for the post amp the warm wish enjoy you be day') \n",
            "\t\t\t\t\t\t\t\t\t\t\t ('NEG', 'oh i miss the pic')\n",
            "('POS', 'your tweet be just include in the long poem in the world') \n",
            "\t\t\t\t\t\t\t\t\t\t\t ('NEG', 'quotim give up on you i do not care how you mess up your life nowquot emo_neg')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-ZpI2mDI0xH",
        "colab_type": "text"
      },
      "source": [
        "#####<pre>     Veamos que predicciones nos dan nuestro cuatro modelos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSFEUqsoC-pB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds_nb = pipeline_nb.predict(textos_ml)\n",
        "preds_svm = pipeline_svm.predict(textos_ml)\n",
        "preds_cnn = model_cnn.predict(textos_dl)\n",
        "preds_dan = model_dan.predict(textos_dl)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d7ea2f43-de9b-4367-cc5e-772442bed074",
        "id": "ILY8pewoH8OM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        }
      },
      "source": [
        "# presentamos las predicciones tabuladas\n",
        "\n",
        "n = 0\n",
        "cont_pos = {'1_NB': 0, '2_SVM': 0, '3_CNN': 0, '4_DAN': 0}\n",
        "cont_neg = {'1_NB': 0, '2_SVM': 0, '3_CNN': 0, '4_DAN': 0}\n",
        "\n",
        "for i, j, k, l, m in zip(labels_ml, preds_nb, preds_svm, preds_cnn, preds_dan):\n",
        "  if n == 0:\n",
        "    print(\"+-------------------------------------------------------------------------+\")\n",
        "    print(\"|  ETIQUETAS  |                        PREDICCIONES                       |\")\n",
        "    print(\"|    REALES   |       NB            SVM            CNN            DAN     |\")\n",
        "    print(\"|-------------+--------------+--------------+--------------+--------------|\")\n",
        "\n",
        "  if l[0] > l[1]:\n",
        "    l = 'NEG'\n",
        "  else:\n",
        "    l = 'POS'\n",
        "\n",
        "  if m[0] > m[1]:\n",
        "    m = 'NEG'\n",
        "  else:\n",
        "    m = 'POS'\n",
        "  \n",
        "  if i == 'POS':\n",
        "    if j == i: cont_pos['1_NB'] += 1\n",
        "    if k == i: cont_pos['2_SVM'] += 1\n",
        "    if l == i: cont_pos['3_CNN'] += 1\n",
        "    if m == i: cont_pos['4_DAN'] += 1\n",
        "  else:\n",
        "    if j == i: cont_neg['1_NB'] += 1\n",
        "    if k == i: cont_neg['2_SVM'] += 1\n",
        "    if l == i: cont_neg['3_CNN'] += 1\n",
        "    if m == i: cont_neg['4_DAN'] += 1\n",
        "\n",
        "  print(f\"|     {i}     |  {j.lower()} ··· {'OK' if j == i else 'KO'}  |  {k.lower()} ··· {'OK' if k == i else 'KO'}  |  {l.lower()} ··· {'OK' if l == i else 'KO'}  |  {m.lower()} ··· {'OK' if m == i else 'KO'}  |\")\n",
        "\n",
        "  n += 1\n",
        "  if (n == n_regs or n == 2*n_regs):\n",
        "    print(\"+-------------+--------------+--------------+--------------+--------------+\")\n",
        "\n",
        "print(\"\\n\\nRecuento de aciertos POS: \", cont_pos, \"\\nRecuento de aciertos NEG: \", cont_neg)\n",
        "print(\"\\n\\n          Recuento total: \", {k: cont_pos[k] + cont_neg[k] for k in cont_pos})"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------------------------------------------------------------------+\n",
            "|  ETIQUETAS  |                        PREDICCIONES                       |\n",
            "|    REALES   |       NB            SVM            CNN            DAN     |\n",
            "|-------------+--------------+--------------+--------------+--------------|\n",
            "|     POS     |  pos ··· OK  |  neg ··· KO  |  pos ··· OK  |  pos ··· OK  |\n",
            "|     POS     |  pos ··· OK  |  pos ··· OK  |  pos ··· OK  |  pos ··· OK  |\n",
            "|     POS     |  pos ··· OK  |  pos ··· OK  |  pos ··· OK  |  pos ··· OK  |\n",
            "|     POS     |  pos ··· OK  |  neg ··· KO  |  neg ··· KO  |  pos ··· OK  |\n",
            "|     POS     |  pos ··· OK  |  pos ··· OK  |  pos ··· OK  |  pos ··· OK  |\n",
            "|     POS     |  pos ··· OK  |  pos ··· OK  |  pos ··· OK  |  pos ··· OK  |\n",
            "|     POS     |  pos ··· OK  |  pos ··· OK  |  pos ··· OK  |  pos ··· OK  |\n",
            "|     POS     |  pos ··· OK  |  pos ··· OK  |  pos ··· OK  |  pos ··· OK  |\n",
            "|     POS     |  pos ··· OK  |  pos ··· OK  |  neg ··· KO  |  pos ··· OK  |\n",
            "|     POS     |  neg ··· KO  |  neg ··· KO  |  pos ··· OK  |  neg ··· KO  |\n",
            "|     POS     |  neg ··· KO  |  pos ··· OK  |  pos ··· OK  |  pos ··· OK  |\n",
            "|     POS     |  neg ··· KO  |  pos ··· OK  |  pos ··· OK  |  pos ··· OK  |\n",
            "|     POS     |  neg ··· KO  |  neg ··· KO  |  pos ··· OK  |  pos ··· OK  |\n",
            "|     POS     |  pos ··· OK  |  pos ··· OK  |  pos ··· OK  |  pos ··· OK  |\n",
            "|     POS     |  pos ··· OK  |  pos ··· OK  |  pos ··· OK  |  pos ··· OK  |\n",
            "|     POS     |  neg ··· KO  |  neg ··· KO  |  neg ··· KO  |  neg ··· KO  |\n",
            "|     POS     |  pos ··· OK  |  pos ··· OK  |  pos ··· OK  |  pos ··· OK  |\n",
            "|     POS     |  neg ··· KO  |  neg ··· KO  |  neg ··· KO  |  neg ··· KO  |\n",
            "|     POS     |  pos ··· OK  |  pos ··· OK  |  pos ··· OK  |  pos ··· OK  |\n",
            "|     POS     |  pos ··· OK  |  pos ··· OK  |  pos ··· OK  |  pos ··· OK  |\n",
            "+-------------+--------------+--------------+--------------+--------------+\n",
            "|     NEG     |  pos ··· KO  |  pos ··· KO  |  pos ··· KO  |  neg ··· OK  |\n",
            "|     NEG     |  neg ··· OK  |  neg ··· OK  |  pos ··· KO  |  neg ··· OK  |\n",
            "|     NEG     |  neg ··· OK  |  neg ··· OK  |  neg ··· OK  |  neg ··· OK  |\n",
            "|     NEG     |  pos ··· KO  |  pos ··· KO  |  pos ··· KO  |  pos ··· KO  |\n",
            "|     NEG     |  neg ··· OK  |  neg ··· OK  |  neg ··· OK  |  neg ··· OK  |\n",
            "|     NEG     |  neg ··· OK  |  neg ··· OK  |  neg ··· OK  |  neg ··· OK  |\n",
            "|     NEG     |  neg ··· OK  |  neg ··· OK  |  neg ··· OK  |  neg ··· OK  |\n",
            "|     NEG     |  neg ··· OK  |  neg ··· OK  |  neg ··· OK  |  neg ··· OK  |\n",
            "|     NEG     |  neg ··· OK  |  neg ··· OK  |  neg ··· OK  |  neg ··· OK  |\n",
            "|     NEG     |  neg ··· OK  |  neg ··· OK  |  neg ··· OK  |  neg ··· OK  |\n",
            "|     NEG     |  pos ··· KO  |  neg ··· OK  |  neg ··· OK  |  pos ··· KO  |\n",
            "|     NEG     |  pos ··· KO  |  pos ··· KO  |  pos ··· KO  |  pos ··· KO  |\n",
            "|     NEG     |  neg ··· OK  |  neg ··· OK  |  pos ··· KO  |  neg ··· OK  |\n",
            "|     NEG     |  neg ··· OK  |  neg ··· OK  |  neg ··· OK  |  neg ··· OK  |\n",
            "|     NEG     |  pos ··· KO  |  pos ··· KO  |  pos ··· KO  |  pos ··· KO  |\n",
            "|     NEG     |  neg ··· OK  |  neg ··· OK  |  neg ··· OK  |  neg ··· OK  |\n",
            "|     NEG     |  pos ··· KO  |  neg ··· OK  |  pos ··· KO  |  neg ··· OK  |\n",
            "|     NEG     |  neg ··· OK  |  neg ··· OK  |  neg ··· OK  |  neg ··· OK  |\n",
            "|     NEG     |  neg ··· OK  |  neg ··· OK  |  neg ··· OK  |  neg ··· OK  |\n",
            "|     NEG     |  pos ··· KO  |  pos ··· KO  |  neg ··· OK  |  pos ··· KO  |\n",
            "+-------------+--------------+--------------+--------------+--------------+\n",
            "\n",
            "\n",
            "Recuento de aciertos POS:  {'1_NB': 14, '2_SVM': 14, '3_CNN': 16, '4_DAN': 17} \n",
            "Recuento de aciertos NEG:  {'1_NB': 13, '2_SVM': 15, '3_CNN': 13, '4_DAN': 15}\n",
            "\n",
            "\n",
            "          Recuento total:  {'1_NB': 27, '2_SVM': 29, '3_CNN': 29, '4_DAN': 32}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPbjehrlIvSV",
        "colab_type": "text"
      },
      "source": [
        "#####<pre>     Los resultados parecen responder a los ***scorings*** obtenidos por cada modelo\n",
        "~~~\n",
        "         La DAN es la que mejor predice, seguida de la CNN y la SVM (prácticamente igualadas), quedando el Naive Bayes en último lugar\n",
        "         Como es lógico, los modelos de Deep Learning lo hacen mejor que los de Machine Learning (incluso habiendo entrenado con menos\n",
        "         datos, ya que los modelos de Deep Learning no contaron con el conjunto de validate incluido en train)\n",
        "         Aunque también la SVM, recordemos que con la parametrización por defecto, está prácticamente a su altura\n",
        "                \n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eWE9zdoon0OH"
      },
      "source": [
        "#####<pre>     **CONCLUSIONES:** Los resultados prediciendo responden perfectamente a los F1-Score de los modelos.\n",
        "#####<pre>                   Observamos que predicen mejor la etiqueta -POS-, la -NEG- les cuesta bastante más."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BR0WMzwUQ7rb"
      },
      "source": [
        "># **EJERCICIO 2**\n",
        "###<pre>      ***Tweet Analysis***\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EVRw3u0XRmnV"
      },
      "source": [
        ">># **Dataset**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Hc7_WUvhRmnW"
      },
      "source": [
        "#####<pre>      Cargamos el Dataset \"dataset_2.json\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b994d5cc-4374-492a-89df-413ce3e34f86",
        "id": "tx6JLyfpRbNT",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b6e3b3eb-bc45-472e-a294-aa35f1d96db2\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-b6e3b3eb-bc45-472e-a294-aa35f1d96db2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving dataset_2.json to dataset_2.json\n",
            "User uploaded file \"dataset_2.json\" with length 1070528 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1n0-CIuLR1aO"
      },
      "source": [
        "#####<pre>      Obtenemos una breve descripción, su estructura y le damos un primer vistazo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "87387658-d7aa-4247-e997-9035697c367b",
        "id": "oBxy9SpjR1aU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "df2 = pd.read_json(io.StringIO(uploaded['dataset_2.json'].decode('utf-8')))\n",
        "\n",
        "df2"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>5849</th>\n",
              "      <th>5850</th>\n",
              "      <th>5851</th>\n",
              "      <th>5852</th>\n",
              "      <th>5853</th>\n",
              "      <th>5854</th>\n",
              "      <th>5855</th>\n",
              "      <th>5856</th>\n",
              "      <th>5857</th>\n",
              "      <th>5858</th>\n",
              "      <th>5859</th>\n",
              "      <th>5860</th>\n",
              "      <th>5861</th>\n",
              "      <th>5862</th>\n",
              "      <th>5863</th>\n",
              "      <th>5864</th>\n",
              "      <th>5865</th>\n",
              "      <th>5866</th>\n",
              "      <th>5867</th>\n",
              "      <th>5868</th>\n",
              "      <th>5869</th>\n",
              "      <th>5870</th>\n",
              "      <th>5871</th>\n",
              "      <th>5872</th>\n",
              "      <th>5873</th>\n",
              "      <th>5874</th>\n",
              "      <th>5875</th>\n",
              "      <th>5876</th>\n",
              "      <th>5877</th>\n",
              "      <th>5878</th>\n",
              "      <th>5879</th>\n",
              "      <th>5880</th>\n",
              "      <th>5881</th>\n",
              "      <th>5882</th>\n",
              "      <th>5883</th>\n",
              "      <th>5884</th>\n",
              "      <th>5885</th>\n",
              "      <th>5886</th>\n",
              "      <th>5887</th>\n",
              "      <th>5888</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>tweet</th>\n",
              "      <td>“Low plastic stool, cheap but delicious noodle...</td>\n",
              "      <td>“Low plastic stool, cheap but delicious noodle...</td>\n",
              "      <td>This National Gun Violence Awareness Day, show...</td>\n",
              "      <td>We can never truly repay the debt we owe our f...</td>\n",
              "      <td>This Center is for the leaders of tomorrow who...</td>\n",
              "      <td>Happy Mother’s Day to every mom out there, esp...</td>\n",
              "      <td>There are few issues more important to the sec...</td>\n",
              "      <td>These talented young people will create a bett...</td>\n",
              "      <td>Our statement on the passing of Former First L...</td>\n",
              "      <td>Incredible to have a Chicago team in the Final...</td>\n",
              "      <td>Michelle and I are so inspired by all the youn...</td>\n",
              "      <td>Our most important task as a nation is to make...</td>\n",
              "      <td>In Singapore with young people who are advocat...</td>\n",
              "      <td>41: I like the competition. And the loyalty to...</td>\n",
              "      <td>Congrats to @LoyolaChicago and Sister Jean for...</td>\n",
              "      <td>Just because I have more time to watch games d...</td>\n",
              "      <td>Have fun out there among the stars. https://t....</td>\n",
              "      <td>Four years ago, @MichelleObama and I had the p...</td>\n",
              "      <td>I got my start holding community meetings in C...</td>\n",
              "      <td>Young people have helped lead all our great mo...</td>\n",
              "      <td>Billy Graham was a humble servant who prayed f...</td>\n",
              "      <td>We are grieving with Parkland. But we are not ...</td>\n",
              "      <td>Happy Valentine’s Day, @MichelleObama. You mak...</td>\n",
              "      <td>Dr. King was 26 when the Montgomery bus boycot...</td>\n",
              "      <td>All across America people chose to get involve...</td>\n",
              "      <td>Ten-year-old Jahkil Jackson is on a mission to...</td>\n",
              "      <td>Chris Long gave his paychecks from the first s...</td>\n",
              "      <td>Kat Creech, a wedding planner in Houston, turn...</td>\n",
              "      <td>As we count down to the new year, we get to re...</td>\n",
              "      <td>On behalf of the Obama family, Merry Christmas...</td>\n",
              "      <td>There's no better time than the holiday season...</td>\n",
              "      <td>Happy Hanukkah, everybody, from the Obama fami...</td>\n",
              "      <td>Just got off a call to thank folks who are wor...</td>\n",
              "      <td>Michelle and I are delighted to congratulate P...</td>\n",
              "      <td>From the Obama family to yours, we wish you a ...</td>\n",
              "      <td>ME:  Joe, about halfway through the speech, I’...</td>\n",
              "      <td>This is what happens when the people vote. Con...</td>\n",
              "      <td>Every election matters - those who show up det...</td>\n",
              "      <td>May God also grant all of us the wisdom to ask...</td>\n",
              "      <td>We grieve with all the families in Sutherland ...</td>\n",
              "      <td>...</td>\n",
              "      <td>Spoke to U.K. Prime Minister Theresa May today...</td>\n",
              "      <td>Spoke to U.K. Prime Minister Theresa May today...</td>\n",
              "      <td>Spoke to U.K. Prime Minister Theresa May today...</td>\n",
              "      <td>Spoke to U.K. Prime Minister Theresa May today...</td>\n",
              "      <td>Spoke to U.K. Prime Minister Theresa May today...</td>\n",
              "      <td>Spoke to U.K. Prime Minister Theresa May today...</td>\n",
              "      <td>Spoke to U.K. Prime Minister Theresa May today...</td>\n",
              "      <td>Spoke to U.K. Prime Minister Theresa May today...</td>\n",
              "      <td>Spoke to U.K. Prime Minister Theresa May today...</td>\n",
              "      <td>Spoke to U.K. Prime Minister Theresa May today...</td>\n",
              "      <td>Spoke to U.K. Prime Minister Theresa May today...</td>\n",
              "      <td>Spoke to U.K. Prime Minister Theresa May today...</td>\n",
              "      <td>Spoke to U.K. Prime Minister Theresa May today...</td>\n",
              "      <td>Spoke to U.K. Prime Minister Theresa May today...</td>\n",
              "      <td>Spoke to U.K. Prime Minister Theresa May today...</td>\n",
              "      <td>Spoke to U.K. Prime Minister Theresa May today...</td>\n",
              "      <td>Spoke to U.K. Prime Minister Theresa May today...</td>\n",
              "      <td>Spoke to U.K. Prime Minister Theresa May today...</td>\n",
              "      <td>Spoke to U.K. Prime Minister Theresa May today...</td>\n",
              "      <td>Spoke to U.K. Prime Minister Theresa May today...</td>\n",
              "      <td>Spoke to U.K. Prime Minister Theresa May today...</td>\n",
              "      <td>Spoke to U.K. Prime Minister Theresa May today...</td>\n",
              "      <td>Big day for healthcare. Working hard!</td>\n",
              "      <td>Today on #NationalAgDay, we honor our great Am...</td>\n",
              "      <td>Honored to sign S.442 today. With this legisla...</td>\n",
              "      <td>Joined the @HouseGOP Conference this morning a...</td>\n",
              "      <td>Thank you Louisville, Kentucky. Together, we w...</td>\n",
              "      <td>Thank you Louisville, Kentucky- on my way! #MA...</td>\n",
              "      <td>Congratulations Eric &amp;amp; Lara. Very proud an...</td>\n",
              "      <td>What about all of the contact with the Clinton...</td>\n",
              "      <td>Just heard Fake News CNN is doing polls again ...</td>\n",
              "      <td>The real story that Congress, the FBI and all ...</td>\n",
              "      <td>The Democrats made up and pushed the Russian s...</td>\n",
              "      <td>James Clapper and others stated that there is ...</td>\n",
              "      <td>#ICYMI: Weekly Address \\n➡️https://t.co/ckVx2z...</td>\n",
              "      <td>...vast sums of money to NATO &amp;amp; the United...</td>\n",
              "      <td>Despite what you have heard from the FAKE NEWS...</td>\n",
              "      <td>Great meeting with the @RepublicanStudy Commit...</td>\n",
              "      <td>\"The President Changed. So Has Small Businesse...</td>\n",
              "      <td>North Korea is behaving very badly. They have ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>label</th>\n",
              "      <td>OBAMA</td>\n",
              "      <td>OBAMA</td>\n",
              "      <td>OBAMA</td>\n",
              "      <td>OBAMA</td>\n",
              "      <td>OBAMA</td>\n",
              "      <td>OBAMA</td>\n",
              "      <td>OBAMA</td>\n",
              "      <td>OBAMA</td>\n",
              "      <td>OBAMA</td>\n",
              "      <td>OBAMA</td>\n",
              "      <td>OBAMA</td>\n",
              "      <td>OBAMA</td>\n",
              "      <td>OBAMA</td>\n",
              "      <td>OBAMA</td>\n",
              "      <td>OBAMA</td>\n",
              "      <td>OBAMA</td>\n",
              "      <td>OBAMA</td>\n",
              "      <td>OBAMA</td>\n",
              "      <td>OBAMA</td>\n",
              "      <td>OBAMA</td>\n",
              "      <td>OBAMA</td>\n",
              "      <td>OBAMA</td>\n",
              "      <td>OBAMA</td>\n",
              "      <td>OBAMA</td>\n",
              "      <td>OBAMA</td>\n",
              "      <td>OBAMA</td>\n",
              "      <td>OBAMA</td>\n",
              "      <td>OBAMA</td>\n",
              "      <td>OBAMA</td>\n",
              "      <td>OBAMA</td>\n",
              "      <td>OBAMA</td>\n",
              "      <td>OBAMA</td>\n",
              "      <td>OBAMA</td>\n",
              "      <td>OBAMA</td>\n",
              "      <td>OBAMA</td>\n",
              "      <td>OBAMA</td>\n",
              "      <td>OBAMA</td>\n",
              "      <td>OBAMA</td>\n",
              "      <td>OBAMA</td>\n",
              "      <td>OBAMA</td>\n",
              "      <td>...</td>\n",
              "      <td>TRUMP</td>\n",
              "      <td>TRUMP</td>\n",
              "      <td>TRUMP</td>\n",
              "      <td>TRUMP</td>\n",
              "      <td>TRUMP</td>\n",
              "      <td>TRUMP</td>\n",
              "      <td>TRUMP</td>\n",
              "      <td>TRUMP</td>\n",
              "      <td>TRUMP</td>\n",
              "      <td>TRUMP</td>\n",
              "      <td>TRUMP</td>\n",
              "      <td>TRUMP</td>\n",
              "      <td>TRUMP</td>\n",
              "      <td>TRUMP</td>\n",
              "      <td>TRUMP</td>\n",
              "      <td>TRUMP</td>\n",
              "      <td>TRUMP</td>\n",
              "      <td>TRUMP</td>\n",
              "      <td>TRUMP</td>\n",
              "      <td>TRUMP</td>\n",
              "      <td>TRUMP</td>\n",
              "      <td>TRUMP</td>\n",
              "      <td>TRUMP</td>\n",
              "      <td>TRUMP</td>\n",
              "      <td>TRUMP</td>\n",
              "      <td>TRUMP</td>\n",
              "      <td>TRUMP</td>\n",
              "      <td>TRUMP</td>\n",
              "      <td>TRUMP</td>\n",
              "      <td>TRUMP</td>\n",
              "      <td>TRUMP</td>\n",
              "      <td>TRUMP</td>\n",
              "      <td>TRUMP</td>\n",
              "      <td>TRUMP</td>\n",
              "      <td>TRUMP</td>\n",
              "      <td>TRUMP</td>\n",
              "      <td>TRUMP</td>\n",
              "      <td>TRUMP</td>\n",
              "      <td>TRUMP</td>\n",
              "      <td>TRUMP</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 5889 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    0     ...                                               5888\n",
              "tweet  “Low plastic stool, cheap but delicious noodle...  ...  North Korea is behaving very badly. They have ...\n",
              "label                                              OBAMA  ...                                              TRUMP\n",
              "\n",
              "[2 rows x 5889 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "cdc56f1f-4787-47bc-d1e7-9e90f1c0b6dc",
        "id": "bl4CoSgER1aY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "# mejor, transponemos el dataset\n",
        "df2t = df2.T\n",
        "\n",
        "# observamos duplicados en los tweets 0 y 1... es posible que haya más, los eliminamos\n",
        "df2t = df2t.drop_duplicates()\n",
        "\n",
        "print(\"\\t NULOS\\n\", \"\\t-------\\n\", df2t.isnull().any(), \"\\n\\n\", sep = '')\n",
        "\n",
        "df2t.describe()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t NULOS\n",
            "\t-------\n",
            "tweet    False\n",
            "label    False\n",
            "dtype: bool\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>5723</td>\n",
              "      <td>5723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>5723</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>Celebrate President Obama's birthday by wishin...</td>\n",
              "      <td>TRUMP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>1</td>\n",
              "      <td>2862</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    tweet  label\n",
              "count                                                5723   5723\n",
              "unique                                               5723      2\n",
              "top     Celebrate President Obama's birthday by wishin...  TRUMP\n",
              "freq                                                    1   2862"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Td7bN20HTb59"
      },
      "source": [
        "#####<pre>      Observamos que el Dataset tenía 5.889 tweets y tras la eliminación de duplicaciones queda con 5.723\n",
        "~~~\n",
        "          > La primera columna -tweet- [string], contiene el texto a analizar en el problema\n",
        "          > La segunda columna -label- es binaria [OBAMA / TRUMP], indica el presidente autor del tweet\n",
        "~~~"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b7f59a72-a80b-4f16-b355-83298fd6bcf7",
        "id": "aHA5biweTb6C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        }
      },
      "source": [
        "# reindexamos el dataset para que no nos falten claves (por la eliminación de duplicaciones)\n",
        "df2t = df2t.reset_index()\n",
        "\n",
        "# eliminamos la columna index antigua (que se queda residente tras reindexar)\n",
        "df2t.drop(['index'], axis = 1, inplace = True)\n",
        "\n",
        "print(\"        COUNT\\n\", \"       -------\\n\", df2t['label'].value_counts(), \"\\n\\n\", sep = '')\n",
        "\n",
        "tw_tr = ''\n",
        "for i in range(0, 5):\n",
        "  print(df2t['label'][i], \"\\t\", df2t['tweet'][i])\n",
        "  tw_tr = tw_tr + f\"{df2t['label'][5722 - i]}\\t{df2t['tweet'][5722 - i]}\\n\"\n",
        "\n",
        "print(tw_tr)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        COUNT\n",
            "       -------\n",
            "TRUMP    2862\n",
            "OBAMA    2861\n",
            "Name: label, dtype: int64\n",
            "\n",
            "\n",
            "OBAMA \t “Low plastic stool, cheap but delicious noodles, cold Hanoi beer.” This is how I’ll remember Tony. He taught us about food — but more importantly, about its ability to bring us together. To make us a little less afraid of the unknown. We’ll miss him. https://t.co/orEXIaEMZM\n",
            "OBAMA \t This National Gun Violence Awareness Day, show your commitment to keeping our kids safe from gun violence. Then, for everyone we’ve lost, take action to change our leadership and our laws until they reflect that commitment - no matter how long it takes. https://t.co/lraIwlQAgl\n",
            "OBAMA \t We can never truly repay the debt we owe our fallen heroes. But we can remember them, honor their sacrifice, and affirm in our own lives those enduring ideals of justice, equality, and opportunity for which generations of Americans have given that last full measure of devotion. https://t.co/pRxwmaMClL\n",
            "OBAMA \t This Center is for the leaders of tomorrow who are ready to step up and build the world as it should be. Michelle and I are grateful to Chicagoans and the Chicago City Council for making it happen. https://t.co/86uiJ55azV\n",
            "OBAMA \t Happy Mother’s Day to every mom out there, especially the remarkable moms in my life, @MichelleObama and my mother-in-law, Marian Robinson. https://t.co/n65fyWg7O8\n",
            "TRUMP\tNorth Korea is behaving very badly. They have been \"playing\" the United States for years. China has done little to help!\n",
            "TRUMP\t\"The President Changed. So Has Small Businesses' Confidence\"\n",
            "https://t.co/daTGjPmYeJ\n",
            "TRUMP\tGreat meeting with the @RepublicanStudy Committee this morning at the @WhiteHouse! https://t.co/8Y2UoHoYaY\n",
            "TRUMP\tDespite what you have heard from the FAKE NEWS, I had a GREAT meeting with German Chancellor Angela Merkel. Nevertheless, Germany owes.....\n",
            "TRUMP\t...vast sums of money to NATO &amp; the United States must be paid more for the powerful, and very expensive, defense it provides to Germany!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WBIEx0nSTb6E"
      },
      "source": [
        "#####<pre>      El Dataset está perfectamente balanceado con respecto a la etiqueta [TRUMP - 50%  / OBAMA - 50%]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5s034z7dhztD"
      },
      "source": [
        ">># **Pipeline**\n",
        "\n",
        "#####<pre>        Nos enfrentamos a un problema de Clasificación No Supervisada 'Topic Modeling'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "z8FNCd-fjbEG",
        "outputId": "85784c23-e85d-48ed-f05f-7bf6067b2fb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vocab = list()\n",
        "\n",
        "for index, row in df2t.iterrows():\n",
        "  for word in row['tweet'].strip().split(' '):\n",
        "    if word not in vocab:\n",
        "      vocab.append(word)\n",
        "\n",
        "print(f\"Número de Palabras: {len(vocab)}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Número de Palabras: 21634\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9jmhJqpQjMCY"
      },
      "source": [
        "#####<pre>      La columna -tweet- contiene 21.634 tokens diferentes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "21_ZRRFkoZ-t"
      },
      "source": [
        ">># **Preprocesamiento**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vO5ATqvtoZ-4"
      },
      "source": [
        "#####<pre>      Preprocesaremos de forma análoga a como lo hicimos en el Ejercicio 1 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GX3nYmcqoZ-7",
        "colab": {}
      },
      "source": [
        "# limpiamos en un orden lógico\n",
        "\n",
        "for fila in range(0, len(df2t)):\n",
        "  texto = df2t.loc[fila, 'tweet']\n",
        "\n",
        "  # Ponemos todo en minúsculas\n",
        "  texto = texto.lower()\n",
        "\n",
        "  # Eliminamos el texto unicode (\"\\u002c\", \"x96\", \"\"\")\n",
        "  texto = re.sub(r'(\\\\u[0-9A-Fa-f]+)', '', texto)       \n",
        "  texto = re.sub(r'[^\\x00-\\x7f]', '', texto)\n",
        "\n",
        "  # Eliminamos los Emojis Positivos\n",
        "  texto = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\)|:\\s?D|:-D|x-?D|X-?D|@-\\)|<3|:\\*|;-?\\)|;-?D|\\(-?;)', '', texto)\n",
        "  \n",
        "  # Eliminamos los Emojis Negativos\n",
        "  texto = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:|:,\\(|:\\'\\(|:\"\\()|:-\\||t_t', '', texto)\n",
        "\n",
        "  # Eliminamos las URLs ya que entendemos que no aportarán información significativa al modelo\n",
        "  texto = re.sub(r'((www\\.[\\S]+)|(https?://[\\S]+))', '', texto)\n",
        "  texto = re.sub(r'((www\\.[^\\s]+)|(https?://[^\\s]+))', '', texto)\n",
        "\n",
        "  # Eliminamos las menciones de usuarios porque creemos que los nombres o alias no tendrán significación en nuestro análisis\n",
        "  texto = re.sub(r'@[\\S]+', '', texto)\n",
        "  texto = re.sub(r'@[^\\s]+', '', texto)\n",
        "\n",
        "  # Eliminamos los hashtags porque creemos que no aportarán a nuestro análisis \n",
        "  texto = re.sub(r'#([^\\s]+)', '', texto)\n",
        "  texto = re.sub(r'#(\\S+)', '', texto)\n",
        "\n",
        "  # Eliminamos las siglas de los retweets \n",
        "  texto = re.sub(r'\\brt\\b', '', texto)\n",
        "\n",
        "  # Eliminamos los signos de puntuación\n",
        "  texto = re.sub(r'[\\'\"?!,.():;]', '', texto)\n",
        "\n",
        "  # Eliminamos los caracteres especiales\n",
        "  texto = re.sub(r'[^a-zA-z0-9\\s]', '', texto)\n",
        "  \n",
        "  for t in texto.split(' '):    \n",
        "    # Convertimos las repeticiones masivas de letras\n",
        "    texto = texto.replace(t, re.sub(r'(.)\\1+', r'\\1\\1', t))\n",
        "\n",
        "    # Convertimos la jerga\n",
        "    if t in jerga_dict:\n",
        "      texto = texto.replace(t, jerga_dict[t])\n",
        "\n",
        "  # Traducimos las contracciones\n",
        "  texto = contractions.fix(texto)\n",
        "\n",
        "  # Eliminamos los números\n",
        "  texto = re.sub(r'[0123456789]', '', texto)\n",
        "\n",
        "  # Eliminamos los espacios en blanco sobrantes\n",
        "  texto = texto.strip()\n",
        "  texto = re.sub(r'\\s+', ' ', texto)\n",
        "\n",
        "  df2t.loc[fila, 'tweet'] = texto"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l4CNcqKhwLjI"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PNnMmesJwLjS",
        "colab": {}
      },
      "source": [
        "# guardamos el trabajo para evitar reprocesos posteriores y para utilizarlo en el Ejercicio 3\n",
        "\n",
        "df2t.to_csv('/content/gdrive/My Drive/NLP/Practica Final/df2t.csv', sep = ';', decimal = '.', index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Wqo2wgK3wLje"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nDajLvNtQ0-v",
        "colab": {}
      },
      "source": [
        "# Cargamos el dataset ya preprocesado (limpio)\n",
        "\n",
        "df2t = pd.read_csv('/content/gdrive/My Drive/NLP/Practica Final/df2t.csv', sep = ';', decimal = '.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0OKRgb8aPZF4"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "B7cng4gdiTol"
      },
      "source": [
        "#####<pre>      Lemmatizaremos, apoyándonos en la librería **spacy**, con objeto de normalizar el texto en lo posible"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CXVxy8HqiTop",
        "colab": {}
      },
      "source": [
        "for fila in range(0, len(df2t)):\n",
        "  texto = nlp(df2t.loc[fila, 'tweet'])\n",
        "  texto = ' '.join([t.lemma_ if t.lemma_ != '-PRON-' else t.text for t in texto])\n",
        "\n",
        "  df2t.loc[fila, 'tweet'] = texto"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IOeAgRGQoZ_D"
      },
      "source": [
        "#####<pre>      Filtramos las **stop-words** y generamos dos listas con los documentos de cada presidente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GBw9YZQzvrL-",
        "colab": {}
      },
      "source": [
        "docs_obama = list()\n",
        "docs_trump = list()\n",
        "\n",
        "for fila in range(0, len(df2t)):\n",
        "  texto = [t.text for t in nlp(df2t.loc[fila, 'tweet']) if t.text not in stop_words]\n",
        "\n",
        "  if df2t.loc[fila, 'label'] == 'OBAMA':\n",
        "    docs_obama.append(texto)\n",
        "  else:\n",
        "    docs_trump.append(texto)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Rp611WIIxsIx"
      },
      "source": [
        ">># **Implementación del Topic Modeling con *GENSIM & LDA***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KER9J-wsxH9B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# función para obtener la grid de valores de coherencia según el número de topics (el modelo LDA va incluido)\n",
        "def compute_coherence_values(dictionary, corpus, texts, limit = 10, start = 2, step = 1):\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "\n",
        "    for num_topics in range(start, limit, step):\n",
        "        model = models.LdaModel(corpus, id2word = dictionary, num_topics = num_topics, iterations = 5, passes = 20, alpha = 'auto')\n",
        "        model_list.append(model)\n",
        "        cm = CoherenceModel(model = model, texts = texts, dictionary = dictionary, coherence = 'c_v')\n",
        "        coherence_values.append(cm.get_coherence())\n",
        "\n",
        "    return model_list, coherence_values\n",
        "\n",
        "# función para visualizar los valores de coherencia por número de topics\n",
        "def plot_cohe_topics(coherence_values, limit = 10, start = 2, step = 1):\n",
        "  x = range(start, limit, step)\n",
        "  plt.plot(x, coherence_values)\n",
        "  plt.xlabel(\"Número de Topics\")\n",
        "  plt.ylabel(\"Coherence Score\")\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sLlr9ndxwjxf"
      },
      "source": [
        "#####<pre>      Analizaremos primero los tweets de **OBAMA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbd_60UTxIDt",
        "colab_type": "code",
        "outputId": "f404c727-ab83-4b18-cd53-a6f5f058c12b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# generamos el vocabulario gensim\n",
        "gensim_dict_o = Dictionary(docs_obama)\n",
        "\n",
        "# generamos con gensim el corpus \n",
        "corpus_o = [gensim_dict_o.doc2bow(doc) for doc in docs_obama]\n",
        "\n",
        "print(f\"Del presidente Obama tenemos {len(corpus_o)} documentos con {len(gensim_dict_o)} tokens/palabras diferentes\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Del presidente Obama tenemos 2861 documentos con 3051 tokens/palabras diferentes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zbpUh4nQ2_k1"
      },
      "source": [
        "#####<pre>      Entrenamos el modelo ***LDA*** que implementaremos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2O3PRunlxIAT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Entrenamos modelos con valores para número de topics del 2 al 10\n",
        "model_list_o, coherence_values_o = compute_coherence_values(gensim_dict_o, corpus_o, docs_obama)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqFR4KGT-2An",
        "colab_type": "code",
        "outputId": "a111d43b-9875-49c5-e896-918725185d3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        }
      },
      "source": [
        "# Visualizamos el entrenamiento\n",
        "plot_cohe_topics(coherence_values_o)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEHCAYAAAC5u6FsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlxBPHM455xLy/wGUNfbx4meZLgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7ZBG5PCA9Vi",
        "colab_type": "text"
      },
      "source": [
        "#####<pre>      Según la gráfica, el número de topics óptimo para el modelo de clustering de tweets de **OBAMA** son **7**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xL1uSQiRBKMB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# implementamos el modelo con 7 topics\n",
        "lda_o = models.LdaModel(corpus_o, id2word = gensim_dict_o, num_topics = 7, iterations = 5, passes = 20, alpha = 'auto')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2s9OwAxYBjbG",
        "colab_type": "code",
        "outputId": "9504cc1c-5d4e-4b0c-f33d-1b55f1a2fabb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 997
        }
      },
      "source": [
        "# Visualizamos con pyLDAvis\n",
        "vis_o = pyLDAvis.gensim.prepare(lda_o, corpus_o, gensim_dict_o)\n",
        "pyLDAvis.display(vis_o)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
            "of pandas will change to not sort by default.\n",
            "\n",
            "To accept the future behavior, pass 'sort=False'.\n",
            "\n",
            "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
            "\n",
            "  return pd.concat([default_term_info] + list(topic_dfs))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
              "\n",
              "\n",
              "<div id=\"ldavis_el59041402016498215209392429396\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "\n",
              "var ldavis_el59041402016498215209392429396_data = {\"mdsDat\": {\"x\": [0.14836520542391232, 0.1526907113597762, 0.0009199768884105453, -0.2588871847562747, 0.1484680870850854, -0.02499305333762809, -0.16656374266328194], \"y\": [-0.15359074129460823, -0.048966292587388556, 0.27710078987410747, -0.1530242640908397, 0.011737461887851453, -0.0016415422028103282, 0.06838458841368837], \"topics\": [1, 2, 3, 4, 5, 6, 7], \"cluster\": [1, 1, 1, 1, 1, 1, 1], \"Freq\": [21.314943313598633, 19.957704544067383, 12.796881675720215, 12.230355262756348, 12.16944408416748, 11.917821884155273, 9.61284065246582]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\"], \"Freq\": [1117.0, 999.0, 173.0, 176.0, 149.0, 143.0, 125.0, 168.0, 120.0, 159.0, 160.0, 110.0, 161.0, 189.0, 96.0, 106.0, 110.0, 132.0, 94.0, 179.0, 99.0, 90.0, 207.0, 148.0, 132.0, 90.0, 66.0, 61.0, 61.0, 59.0, 98.75049591064453, 69.00386047363281, 59.02959442138672, 53.02387237548828, 70.39225769042969, 50.22554397583008, 46.40718078613281, 45.83303451538086, 41.46122360229492, 42.52252197265625, 40.3575325012207, 36.48681640625, 33.7078857421875, 29.87811851501465, 29.32468605041504, 26.023405075073242, 26.54815673828125, 37.7149543762207, 22.14436149597168, 21.06507682800293, 20.517555236816406, 20.489049911499023, 21.01660919189453, 18.301942825317383, 17.76656723022461, 17.752958297729492, 17.74746322631836, 17.204301834106445, 16.65001678466797, 16.656982421875, 23.441001892089844, 128.9315185546875, 40.45186996459961, 69.33251190185547, 101.83056640625, 49.72692108154297, 48.56523513793945, 94.4568099975586, 119.7676010131836, 493.37554931640625, 419.75927734375, 64.68358612060547, 52.24305725097656, 53.00663375854492, 61.79582214355469, 52.64534378051758, 35.72028350830078, 56.98330307006836, 56.213775634765625, 167.38827514648438, 158.35308837890625, 69.0844497680664, 89.42022705078125, 157.41526794433594, 47.894229888916016, 39.485145568847656, 28.315401077270508, 37.56737518310547, 22.734390258789062, 21.606767654418945, 23.233360290527344, 19.34296989440918, 18.77837371826172, 17.043546676635742, 16.991069793701172, 15.352258682250977, 14.25922966003418, 14.250934600830078, 14.250576972961426, 13.696857452392578, 13.696383476257324, 13.690279006958008, 18.478973388671875, 41.427833557128906, 12.561233520507812, 15.680131912231445, 10.864542961120605, 10.30244255065918, 10.292223930358887, 74.5243148803711, 37.03244400024414, 65.116455078125, 108.35282897949219, 138.66468811035156, 35.37875747680664, 46.70213317871094, 30.558609008789062, 61.16096115112305, 24.105703353881836, 52.54264450073242, 47.767024993896484, 91.24727630615234, 240.70191955566406, 227.3711395263672, 64.30518341064453, 39.797340393066406, 46.45947265625, 65.02206420898438, 43.85555648803711, 42.902591705322266, 37.005977630615234, 34.68220138549805, 175.35113525390625, 65.26348114013672, 57.84674072265625, 51.9703369140625, 43.53547286987305, 42.477298736572266, 34.537940979003906, 107.504150390625, 31.832612991333008, 38.931556701660156, 23.93340301513672, 21.302745819091797, 20.241830825805664, 20.244403839111328, 19.71109962463379, 18.65348243713379, 17.598684310913086, 46.066619873046875, 17.06471824645996, 16.534786224365234, 15.47211742401123, 14.419177055358887, 47.23487854003906, 14.848677635192871, 16.817535400390625, 12.301202774047852, 12.805862426757812, 12.302939414978027, 11.244091987609863, 11.155435562133789, 34.82090377807617, 41.72392654418945, 56.16019821166992, 56.32236099243164, 107.23442840576172, 23.390626907348633, 57.26993179321289, 45.41774368286133, 59.44368362426758, 47.42890548706055, 49.073699951171875, 48.4051513671875, 43.46189880371094, 25.222888946533203, 23.12885856628418, 28.447811126708984, 22.192459106445312, 172.5704803466797, 148.6711883544922, 141.95828247070312, 95.52961730957031, 38.820674896240234, 33.6982307434082, 33.6973876953125, 25.73005485534668, 16.6400203704834, 14.930000305175781, 13.22201919555664, 13.756230354309082, 14.824390411376953, 12.649545669555664, 12.078145980834961, 11.510506629943848, 9.80107593536377, 9.803627014160156, 9.804326057434082, 46.069984436035156, 9.237024307250977, 9.232512474060059, 9.233610153198242, 9.229652404785156, 8.09836483001709, 7.5265631675720215, 7.525781154632568, 7.525363445281982, 6.957212448120117, 6.958232402801514, 17.391740798950195, 70.838623046875, 23.856245040893555, 22.71559715270996, 72.75308990478516, 36.670440673828125, 20.546937942504883, 40.71122741699219, 33.4603157043457, 25.505054473876953, 32.850791931152344, 18.12466812133789, 19.88774299621582, 36.776100158691406, 33.76584243774414, 17.530698776245117, 30.06033706665039, 22.456268310546875, 19.041677474975586, 17.74687385559082, 119.97029876708984, 61.13401412963867, 39.710147857666016, 37.50514221191406, 36.37762451171875, 25.959184646606445, 21.012609481811523, 20.460628509521484, 19.371732711791992, 19.367143630981445, 25.707117080688477, 16.617694854736328, 16.066509246826172, 15.518267631530762, 14.417312622070312, 14.412979125976562, 13.319906234741211, 13.316300392150879, 12.767624855041504, 12.767426490783691, 12.764466285705566, 12.766666412353516, 23.599618911743164, 11.645357131958008, 11.098906517028809, 10.569323539733887, 9.46924877166748, 10.989681243896484, 8.9225435256958, 8.367846488952637, 11.378029823303223, 78.89813232421875, 83.86933898925781, 20.639968872070312, 291.2593688964844, 257.7604064941406, 17.504823684692383, 46.67870330810547, 19.865453720092773, 15.75609302520752, 20.454296112060547, 20.468345642089844, 18.746994018554688, 13.323845863342285, 61.163551330566406, 58.32879638671875, 58.298736572265625, 53.205421447753906, 50.92145538330078, 38.3529052734375, 37.23356628417969, 36.63568115234375, 34.93028259277344, 31.512752532958984, 38.7137565612793, 28.622047424316406, 36.21617889404297, 26.808773040771484, 21.233301162719727, 21.237638473510742, 18.38150405883789, 17.24064064025879, 16.67850685119629, 16.102357864379883, 14.960075378417969, 14.389483451843262, 13.82284164428711, 13.251015663146973, 36.330196380615234, 15.286392211914062, 10.39837646484375, 10.395259857177734, 10.391203880310059, 9.834219932556152, 18.139320373535156, 45.30504608154297, 27.352184295654297, 49.45008087158203, 31.90492057800293, 58.191951751708984, 20.154592514038086, 18.121423721313477, 24.917049407958984, 46.5772590637207, 43.60100173950195, 28.198667526245117, 19.33689308166504, 19.852157592773438, 51.00544357299805, 46.4990348815918, 41.609840393066406, 123.28602600097656, 27.220478057861328, 27.21842384338379, 36.25395202636719, 23.351036071777344, 23.352386474609375, 22.25086784362793, 20.581924438476562, 20.03403663635254, 17.266550064086914, 34.113006591796875, 15.607044219970703, 14.498494148254395, 11.739568710327148, 11.186394691467285, 9.529536247253418, 9.527944564819336, 9.530041694641113, 25.872812271118164, 8.974190711975098, 8.973588943481445, 8.41986083984375, 8.41911506652832, 7.867675304412842, 7.867442607879639, 7.865319728851318, 7.865345478057861, 31.147010803222656, 49.59501266479492, 8.362374305725098, 35.99766159057617, 17.997880935668945, 24.467737197875977, 9.908366203308105, 36.832027435302734, 27.969038009643555, 23.612751007080078, 15.044039726257324, 19.540075302124023, 18.782978057861328, 13.694524765014648, 13.126542091369629, 14.481511116027832], \"Term\": [\"president\", \"obama\", \"change\", \"health\", \"climate\", \"fight\", \"job\", \"year\", \"live\", \"us\", \"work\", \"care\", \"now\", \"get\", \"name\", \"speak\", \"add\", \"economy\", \"read\", \"watch\", \"address\", \"support\", \"make\", \"will\", \"take\", \"one\", \"et\", \"give\", \"happen\", \"fair\", \"address\", \"weekly\", \"immigration\", \"discuss\", \"country\", \"economic\", \"reform\", \"system\", \"join\", \"minimum\", \"team\", \"middleclass\", \"across\", \"everyone\", \"pass\", \"issue\", \"fix\", \"back\", \"talk\", \"forward\", \"expand\", \"volunteer\", \"budget\", \"sexual\", \"truth\", \"send\", \"bill\", \"man\", \"assault\", \"commonsense\", \"broken\", \"will\", \"raise\", \"family\", \"help\", \"wage\", \"nation\", \"take\", \"watch\", \"president\", \"obama\", \"need\", \"congress\", \"go\", \"can\", \"america\", \"step\", \"american\", \"make\", \"year\", \"us\", \"good\", \"one\", \"work\", \"pay\", \"since\", \"equality\", \"strong\", \"six\", \"hard\", \"immigrant\", \"place\", \"ahead\", \"equal\", \"nearly\", \"reduce\", \"organizing\", \"enroll\", \"mean\", \"proud\", \"ago\", \"poverty\", \"marriage\", \"leave\", \"difference\", \"unemployment\", \"grassroot\", \"deficit\", \"fellow\", \"let\", \"together\", \"day\", \"economy\", \"get\", \"well\", \"every\", \"grow\", \"progress\", \"next\", \"people\", \"keep\", \"make\", \"obama\", \"president\", \"time\", \"last\", \"new\", \"american\", \"can\", \"today\", \"america\", \"go\", \"health\", \"et\", \"pm\", \"check\", \"cover\", \"insurance\", \"open\", \"care\", \"st\", \"coverage\", \"enrollment\", \"stop\", \"marketplace\", \"option\", \"message\", \"obamacare\", \"uninsured\", \"court\", \"petition\", \"union\", \"season\", \"low\", \"supreme\", \"peace\", \"obamas\", \"industry\", \"nomination\", \"game\", \"political\", \"february\", \"start\", \"sign\", \"right\", \"tune\", \"now\", \"supporter\", \"today\", \"million\", \"american\", \"time\", \"watch\", \"get\", \"president\", \"senate\", \"state\", \"obama\", \"speak\", \"change\", \"climate\", \"fight\", \"name\", \"pollution\", \"carbon\", \"cut\", \"lead\", \"vacancy\", \"fill\", \"denier\", \"editorial\", \"politic\", \"tackle\", \"away\", \"effect\", \"company\", \"leadership\", \"senator\", \"agree\", \"challenge\", \"shutdown\", \"huge\", \"survey\", \"thousand\", \"reminder\", \"ht\", \"roll\", \"china\", \"fairly\", \"board\", \"support\", \"global\", \"united\", \"add\", \"say\", \"gun\", \"stand\", \"state\", \"call\", \"action\", \"ready\", \"power\", \"take\", \"time\", \"show\", \"can\", \"congress\", \"plan\", \"big\", \"live\", \"happen\", \"happy\", \"deliver\", \"future\", \"national\", \"card\", \"generation\", \"birthday\", \"statement\", \"love\", \"wish\", \"easy\", \"life\", \"invest\", \"save\", \"refuse\", \"remark\", \"run\", \"announce\", \"veteran\", \"lot\", \"th\", \"water\", \"honor\", \"conference\", \"increase\", \"hope\", \"administration\", \"americorps\", \"federal\", \"read\", \"speak\", \"celebrate\", \"president\", \"obama\", \"week\", \"now\", \"protect\", \"child\", \"people\", \"new\", \"tune\", \"like\", \"give\", \"fair\", \"sure\", \"judge\", \"garland\", \"worker\", \"hearing\", \"affordable\", \"high\", \"deserve\", \"think\", \"college\", \"community\", \"student\", \"middle\", \"school\", \"kid\", \"class\", \"trade\", \"word\", \"quality\", \"succeed\", \"may\", \"dollar\", \"vote\", \"nominee\", \"town\", \"pledge\", \"yet\", \"upordown\", \"hold\", \"know\", \"act\", \"senate\", \"leader\", \"make\", \"retweet\", \"free\", \"just\", \"president\", \"obama\", \"american\", \"want\", \"today\", \"month\", \"chance\", \"clean\", \"job\", \"chip\", \"movement\", \"growth\", \"house\", \"enter\", \"privatesector\", \"air\", \"white\", \"five\", \"record\", \"fast\", \"grassroots\", \"streak\", \"straight\", \"consecutive\", \"almost\", \"iran\", \"meet\", \"wind\", \"treat\", \"sector\", \"least\", \"midnight\", \"night\", \"november\", \"moment\", \"energy\", \"miss\", \"trip\", \"deadline\", \"americas\", \"case\", \"private\", \"add\", \"last\", \"economy\", \"long\", \"just\", \"support\", \"tonight\", \"million\", \"president\"], \"Total\": [1117.0, 999.0, 173.0, 176.0, 149.0, 143.0, 125.0, 168.0, 120.0, 159.0, 160.0, 110.0, 161.0, 189.0, 96.0, 106.0, 110.0, 132.0, 94.0, 179.0, 99.0, 90.0, 207.0, 148.0, 132.0, 90.0, 66.0, 61.0, 61.0, 59.0, 99.51568603515625, 69.7680435180664, 59.830841064453125, 53.7900390625, 71.43402862548828, 51.03151321411133, 47.1727180480957, 46.61615753173828, 42.2271842956543, 43.3082275390625, 41.12303924560547, 37.25186538696289, 34.50674819946289, 30.644912719726562, 30.09212303161621, 26.788625717163086, 27.331707000732422, 38.927406311035156, 22.93144989013672, 21.831205368041992, 21.282825469970703, 21.275997161865234, 21.826398849487305, 19.067110061645508, 18.53066062927246, 18.517276763916016, 18.518903732299805, 17.96893310546875, 17.414623260498047, 17.422094345092773, 24.562885284423828, 148.52719116210938, 43.95219039916992, 78.2332534790039, 120.75424194335938, 56.74394607543945, 59.89544677734375, 132.6511688232422, 179.6079559326172, 1117.2049560546875, 999.0748901367188, 93.34507751464844, 75.43730163574219, 91.614990234375, 141.2518768310547, 108.76127624511719, 50.325927734375, 211.04286193847656, 207.98626708984375, 168.18719482421875, 159.14219665527344, 69.85260772705078, 90.7596435546875, 160.23666381835938, 48.93413162231445, 40.447139739990234, 29.153545379638672, 38.74709701538086, 23.497257232666016, 22.367788314819336, 24.057310104370117, 20.10715675354004, 19.540111541748047, 17.846878051757812, 17.840654373168945, 16.152057647705078, 15.020453453063965, 15.015665054321289, 15.020438194274902, 14.45796012878418, 14.457965850830078, 14.451864242553711, 19.531892776489258, 43.80847930908203, 13.325334548950195, 16.696786880493164, 11.625737190246582, 11.063253402709961, 11.064460754394531, 84.03218078613281, 41.49174880981445, 75.56436920166016, 132.62799072265625, 189.76255798339844, 42.189598083496094, 57.94521713256836, 35.910465240478516, 88.48605346679688, 28.48357582092285, 87.62189483642578, 80.36148834228516, 207.98626708984375, 999.0748901367188, 1117.2049560546875, 146.01846313476562, 74.32086181640625, 100.36991882324219, 211.04286193847656, 141.2518768310547, 133.16763305664062, 108.76127624511719, 91.614990234375, 176.155517578125, 66.03295135498047, 58.61613464355469, 52.794227600097656, 44.30524826049805, 43.2469367980957, 35.307376861572266, 110.06417846679688, 32.65850067138672, 40.09539031982422, 24.71761131286621, 22.073453903198242, 21.0107421875, 21.013614654541016, 20.480907440185547, 19.424062728881836, 18.37010383605957, 48.13746643066406, 17.83440589904785, 17.30451774597168, 16.244983673095703, 15.189340591430664, 49.758487701416016, 15.723893165588379, 17.85161018371582, 13.071253776550293, 13.608256340026855, 13.07416820526123, 12.014392852783203, 12.014263153076172, 39.81061553955078, 48.777427673339844, 68.95729064941406, 75.71283721923828, 161.3567657470703, 30.805391311645508, 133.16763305664062, 100.6454849243164, 211.04286193847656, 146.01846313476562, 179.6079559326172, 189.76255798339844, 1117.2049560546875, 82.83979797363281, 60.525821685791016, 999.0748901367188, 106.70709991455078, 173.33164978027344, 149.4313201904297, 143.14076232910156, 96.48854064941406, 39.58087921142578, 34.458290100097656, 34.45826721191406, 26.496442794799805, 17.400842666625977, 15.691238403320312, 13.981727600097656, 14.554201126098633, 15.684720039367676, 13.410781860351562, 12.8399019241333, 12.270602226257324, 10.561023712158203, 10.564083099365234, 10.565601348876953, 49.843055725097656, 9.997787475585938, 9.99319076538086, 9.994721412658691, 9.991610527038574, 8.85910701751709, 8.286694526672363, 8.286678314208984, 8.286675453186035, 7.7173542976379395, 7.718883037567139, 19.62567901611328, 90.25650024414062, 28.750715255737305, 28.04561424255371, 110.22337341308594, 51.510684967041016, 28.805681228637695, 73.94088745117188, 60.525821685791016, 42.479736328125, 61.43292999267578, 27.384281158447266, 32.96209716796875, 132.6511688232422, 146.01846313476562, 32.018089294433594, 141.2518768310547, 75.43730163574219, 63.143978118896484, 45.03456497192383, 120.73921203613281, 61.90203094482422, 40.47492218017578, 38.270347595214844, 37.17286682128906, 26.73212432861328, 21.777191162109375, 21.227245330810547, 20.13604164123535, 20.131595611572266, 26.737871170043945, 17.382946014404297, 16.832963943481445, 16.284452438354492, 15.182892799377441, 15.181522369384766, 14.08584976196289, 14.084205627441406, 13.532829284667969, 13.53283405303955, 13.529875755310059, 13.532855987548828, 25.105342864990234, 12.43673038482666, 11.884927749633789, 11.334235191345215, 10.234201431274414, 11.88607120513916, 9.688667297363281, 9.132689476013184, 12.438799858093262, 94.87699890136719, 106.70709991455078, 23.96100425720215, 1117.2049560546875, 999.0748901367188, 24.542146682739258, 161.3567657470703, 37.35904312133789, 22.4909610748291, 87.62189483642578, 100.36991882324219, 75.71283721923828, 34.879974365234375, 61.94485855102539, 59.104461669921875, 59.07944107055664, 53.97630310058594, 51.69297790527344, 39.119346618652344, 37.993507385253906, 37.39645004272461, 35.69169998168945, 32.27444839477539, 39.67089080810547, 29.410083770751953, 37.39511489868164, 27.6971435546875, 21.99341583251953, 21.998014450073242, 19.145370483398438, 18.000709533691406, 17.43910789489746, 16.863601684570312, 15.720450401306152, 15.151142120361328, 14.583415985107422, 14.01108169555664, 38.450069427490234, 16.301284790039062, 11.158472061157227, 11.156867027282715, 11.156723976135254, 10.593833923339844, 19.664770126342773, 51.45308303833008, 37.384700775146484, 82.83979797363281, 63.47479248046875, 207.98626708984375, 31.098642349243164, 27.809249877929688, 79.28423309326172, 1117.2049560546875, 999.0748901367188, 211.04286193847656, 62.12407302856445, 133.16763305664062, 51.76948547363281, 47.34687042236328, 42.375335693359375, 125.92073822021484, 27.985063552856445, 27.98654556274414, 37.397884368896484, 24.11640739440918, 24.1279354095459, 23.01431655883789, 21.346956253051758, 20.798131942749023, 18.031648635864258, 35.72505187988281, 16.371755599975586, 15.268250465393066, 12.503118515014648, 11.94981575012207, 10.292905807495117, 10.292915344238281, 10.295889854431152, 27.990510940551758, 9.738100051879883, 9.738115310668945, 9.183319091796875, 9.183317184448242, 8.631519317626953, 8.631508827209473, 8.62998104095459, 8.63157844543457, 35.166019439697266, 57.00271224975586, 9.188936233520508, 47.42356872558594, 22.563648223876953, 34.010704040527344, 11.390748977661133, 110.22337341308594, 74.32086181640625, 132.62799072265625, 36.615867614746094, 79.28423309326172, 90.25650024414062, 37.61791229248047, 100.6454849243164, 1117.2049560546875], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.5379999876022339, 1.5347000360488892, 1.5322999954223633, 1.5313999652862549, 1.5311000347137451, 1.5298000574111938, 1.5293999910354614, 1.5288000106811523, 1.527500033378601, 1.527500033378601, 1.5269999504089355, 1.524999976158142, 1.5223000049591064, 1.520400047302246, 1.5198999643325806, 1.516800045967102, 1.516700029373169, 1.5140999555587769, 1.5108000040054321, 1.5099999904632568, 1.5090999603271484, 1.5081000328063965, 1.5080000162124634, 1.5047999620437622, 1.5037000179290771, 1.503600001335144, 1.5032000541687012, 1.5023000240325928, 1.5009000301361084, 1.5009000301361084, 1.4989999532699585, 1.4042999744415283, 1.4628000259399414, 1.4249999523162842, 1.3753000497817993, 1.4138000011444092, 1.3360999822616577, 1.2062000036239624, 1.1404999494552612, 0.7283999919891357, 0.678600013256073, 1.1790000200271606, 1.1784000396728516, 0.9986000061035156, 0.7190999984741211, 0.8202000260353088, 1.2029999494552612, 0.23649999499320984, 0.23749999701976776, 1.6067999601364136, 1.606600046157837, 1.6004999876022339, 1.5966999530792236, 1.5937999486923218, 1.5901000499725342, 1.587499976158142, 1.5823999643325806, 1.5806000232696533, 1.5785000324249268, 1.5769000053405762, 1.57669997215271, 1.5728000402450562, 1.5717999935150146, 1.565500020980835, 1.5628000497817993, 1.5607999563217163, 1.559499979019165, 1.5592999458312988, 1.558899998664856, 1.5575000047683716, 1.5573999881744385, 1.5573999881744385, 1.5561000108718872, 1.5556999444961548, 1.5525000095367432, 1.548699975013733, 1.5437999963760376, 1.5403000116348267, 1.539199948310852, 1.4915000200271606, 1.4979000091552734, 1.4627000093460083, 1.4093999862670898, 1.2977999448776245, 1.4355000257492065, 1.395799994468689, 1.4501999616622925, 1.2422000169754028, 1.444700002670288, 1.100100040435791, 1.0914000272750854, 0.7876999974250793, 0.1882999986410141, 0.019600000232458115, 0.7914999723434448, 0.9869999885559082, 0.8413000106811523, 0.4341999888420105, 0.44190001487731934, 0.4788999855518341, 0.5335000157356262, 0.6402000188827515, 2.0513999462127686, 2.0441999435424805, 2.042799949645996, 2.0401999950408936, 2.0383999347686768, 2.0380001068115234, 2.033900022506714, 2.032399892807007, 2.030400037765503, 2.0264999866485596, 2.023699998855591, 2.020400047302246, 2.018699884414673, 2.018699884414673, 2.017699956893921, 2.015500068664551, 2.0130999088287354, 2.01200008392334, 2.011899948120117, 2.010499954223633, 2.007200002670288, 2.0039000511169434, 2.0039000511169434, 1.9987000226974487, 1.9962999820709229, 1.995300054550171, 1.9952000379562378, 1.9952000379562378, 1.9896999597549438, 1.9817999601364136, 1.9220999479293823, 1.8997999429702759, 1.8507000207901, 1.76010000705719, 1.6474000215530396, 1.7805999517440796, 1.2121000289916992, 1.2603000402450562, 0.7889000177383423, 0.9315000176429749, 0.7584999799728394, 0.6898000240325928, -1.1907000541687012, 0.8668000102043152, 1.093999981880188, -1.5027999877929688, 0.48559999465942383, 2.0968000888824463, 2.096100091934204, 2.0929999351501465, 2.0913000106811523, 2.081899881362915, 2.078900098800659, 2.078900098800659, 2.071899890899658, 2.05649995803833, 2.051500082015991, 2.0453999042510986, 2.0448999404907227, 2.044800043106079, 2.042799949645996, 2.04010009765625, 2.0373001098632812, 2.026599884033203, 2.0264999866485596, 2.0264999866485596, 2.0225000381469727, 2.0220999717712402, 2.0220999717712402, 2.0220000743865967, 2.021899938583374, 2.0114998817443848, 2.005000114440918, 2.0048999786376953, 2.0048999786376953, 1.9975999593734741, 1.997499942779541, 1.9803999662399292, 1.8589999675750732, 1.9146000146865845, 1.8904999494552612, 1.6857999563217163, 1.7613999843597412, 1.7633999586105347, 1.5045000314712524, 1.5084999799728394, 1.5910999774932861, 1.4752999544143677, 1.688599944114685, 1.5959999561309814, 0.8184000253677368, 0.6370000243186951, 1.498900055885315, 0.5539000034332275, 0.8895000219345093, 0.9024999737739563, 1.1699999570846558, 2.099900007247925, 2.0938000679016113, 2.0871999263763428, 2.0859999656677246, 2.0845999717712402, 2.076900005340576, 2.070499897003174, 2.069499969482422, 2.067500114440918, 2.067500114440918, 2.0669000148773193, 2.061199903488159, 2.0596001148223877, 2.058000087738037, 2.054500102996826, 2.05430006980896, 2.050299882888794, 2.0501999855041504, 2.0480000972747803, 2.0480000972747803, 2.0480000972747803, 2.0480000972747803, 2.0443999767303467, 2.0404999256134033, 2.0378000736236572, 2.036400079727173, 2.028599977493286, 2.0278000831604004, 2.023900032043457, 2.0188000202178955, 2.0171000957489014, 1.9218000173568726, 1.865399956703186, 1.9570000171661377, 0.761900007724762, 0.7513999938964844, 1.7683000564575195, 0.8658999800682068, 1.4745999574661255, 1.7503999471664429, 0.6514000296592712, 0.5163000226020813, 0.7103000283241272, 1.1439000368118286, 2.1143999099731445, 2.1138999462127686, 2.113800048828125, 2.112799882888794, 2.1120998859405518, 2.107300043106079, 2.1068999767303467, 2.106600046157837, 2.105600118637085, 2.103300094604492, 2.1026999950408936, 2.0999999046325684, 2.095099925994873, 2.0945000648498535, 2.0920000076293945, 2.0920000076293945, 2.086400032043457, 2.0840001106262207, 2.0824999809265137, 2.080899953842163, 2.0776000022888184, 2.0755999088287354, 2.0736000537872314, 2.0713999271392822, 2.0703999996185303, 2.0629000663757324, 2.0566000938415527, 2.0564000606536865, 2.0560998916625977, 2.0527000427246094, 2.0464000701904297, 1.999899983406067, 1.8147000074386597, 1.611199975013733, 1.4392999410629272, 0.8533999919891357, 1.6934000253677368, 1.6988999843597412, 0.9696000218391418, -1.0503000020980835, -1.0046000480651855, 0.11429999768733978, 0.9599999785423279, 0.22380000352859497, 2.327199935913086, 2.3239998817443848, 2.3238000869750977, 2.3208999633789062, 2.3143999576568604, 2.314199924468994, 2.311000108718872, 2.309799909591675, 2.3094000816345215, 2.308300018310547, 2.3055999279022217, 2.3046000003814697, 2.2987000942230225, 2.2959001064300537, 2.2941999435424805, 2.290299892425537, 2.279099941253662, 2.276099920272827, 2.265000104904175, 2.2648000717163086, 2.2648000717163086, 2.263400077819824, 2.2604000568389893, 2.2602999210357666, 2.2553000450134277, 2.255199909210205, 2.2493999004364014, 2.2493999004364014, 2.249300003051758, 2.2490999698638916, 2.2207000255584717, 2.202899932861328, 2.24780011177063, 2.0664000511169434, 2.115999937057495, 2.0127999782562256, 2.2026000022888184, 1.24590003490448, 1.364799976348877, 0.6162999868392944, 1.4526000022888184, 0.9415000081062317, 0.7724000215530396, 1.3315999507904053, 0.3050999939441681, -2.0035998821258545], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.9546000957489014, -4.313000202178955, -4.469099998474121, -4.576399803161621, -4.293099880218506, -4.6305999755859375, -4.709700107574463, -4.722099781036377, -4.822400093078613, -4.797100067138672, -4.849400043487549, -4.950200080871582, -5.029399871826172, -5.150000095367432, -5.168700218200684, -5.2881999015808105, -5.268199920654297, -4.917099952697754, -5.4496002197265625, -5.499499797821045, -5.525899887084961, -5.527299880981445, -5.501800060272217, -5.640100002288818, -5.6697998046875, -5.670599937438965, -5.670899868011475, -5.702000141143799, -5.7347002029418945, -5.734300136566162, -5.3927001953125, -3.6879000663757324, -4.8470001220703125, -4.308199882507324, -3.923799991607666, -4.640600204467773, -4.6641998291015625, -3.999000072479248, -3.7616000175476074, -2.345900058746338, -2.507499933242798, -4.377600193023682, -4.59119987487793, -4.576700210571289, -4.423299789428711, -4.583600044250488, -4.971399784088135, -4.50439977645874, -4.51800012588501, -3.3610000610351562, -3.4165000915527344, -4.245999813079834, -3.98799991607666, -3.422499895095825, -4.612400054931641, -4.8053998947143555, -5.137899875640869, -4.855199813842773, -5.357500076293945, -5.408299922943115, -5.3358001708984375, -5.519000053405762, -5.548600196838379, -5.645599842071533, -5.64870023727417, -5.750100135803223, -5.823999881744385, -5.82450008392334, -5.8246002197265625, -5.864200115203857, -5.864200115203857, -5.864699840545654, -5.564700126647949, -4.757400035858154, -5.950699806213379, -5.729000091552734, -6.095799922943115, -6.14900016784668, -6.150000095367432, -4.170199871063232, -4.86959981918335, -4.305200099945068, -3.7960000038146973, -3.549299955368042, -4.915200233459473, -4.637599945068359, -5.061699867248535, -4.367800235748291, -5.298900127410889, -4.519700050354004, -4.614999771118164, -3.9677999019622803, -2.99780011177063, -3.054800033569336, -4.317699909210205, -4.797599792480469, -4.6427998542785645, -4.306600093841553, -4.700500011444092, -4.722400188446045, -4.870299816131592, -4.935100078582764, -2.8701000213623047, -3.8585000038146973, -3.979099988937378, -4.086299896240234, -4.263400077819824, -4.288000106811523, -4.494900226593018, -3.3594000339508057, -4.576399803161621, -4.375100135803223, -4.861700057983398, -4.978099822998047, -5.029200077056885, -5.029099941253662, -5.055799961090088, -5.110899925231934, -5.169099807739258, -4.206900119781494, -5.199900150299072, -5.231500148773193, -5.297900199890137, -5.3684000968933105, -4.18179988861084, -5.339000225067139, -5.2144999504089355, -5.527200222015381, -5.486999988555908, -5.527100086212158, -5.617099761962891, -5.625, -4.486700057983398, -4.3059000968933105, -4.008699893951416, -4.005799770355225, -3.3619000911712646, -4.8846001625061035, -3.9892001152038574, -4.2210001945495605, -3.951900005340576, -4.177700042724609, -4.143599987030029, -4.157299995422363, -4.265100002288818, -4.809199810028076, -4.895899772644043, -4.688899993896484, -4.93720006942749, -2.84089994430542, -2.9899001121520996, -3.036099910736084, -3.4321999549865723, -4.332699775695801, -4.4741997718811035, -4.4741997718811035, -4.74399995803833, -5.179800033569336, -5.288300037384033, -5.409800052642822, -5.370200157165527, -5.295400142669678, -5.453999996185303, -5.50029993057251, -5.548399925231934, -5.709199905395508, -5.708899974822998, -5.708799839019775, -4.161499977111816, -5.768400192260742, -5.768899917602539, -5.768799781799316, -5.769199848175049, -5.900000095367432, -5.973199844360352, -5.973299980163574, -5.973400115966797, -6.0518999099731445, -6.051700115203857, -5.135700225830078, -3.731300115585327, -4.8196001052856445, -4.868599891662598, -3.7046000957489014, -4.389699935913086, -4.968900203704834, -4.285200119018555, -4.481299877166748, -4.752799987792969, -4.49970006942749, -5.094399929046631, -5.0015997886657715, -4.3867998123168945, -4.4721999168396, -5.127699851989746, -4.588500022888184, -4.880099773406982, -5.045000076293945, -5.115499973297119, -3.199399948120117, -3.8736000061035156, -4.305099964141846, -4.362199783325195, -4.3927001953125, -4.730100154876709, -4.941500186920166, -4.968200206756592, -5.022900104522705, -5.023099899291992, -4.7399001121521, -5.176199913024902, -5.20989990234375, -5.24459981918335, -5.31820011138916, -5.31850004196167, -5.39739990234375, -5.39769983291626, -5.439799785614014, -5.439799785614014, -5.440000057220459, -5.439799785614014, -4.825399875640869, -5.531799793243408, -5.579800128936768, -5.628699779510498, -5.73859977722168, -5.589700222015381, -5.798099994659424, -5.862299919128418, -5.554999828338623, -3.618499994277954, -3.5573999881744385, -4.959400177001953, -2.3125, -2.4346001148223877, -5.124199867248535, -4.143400192260742, -4.997700214385986, -5.229400157928467, -4.968500137329102, -4.967800140380859, -5.055600166320801, -5.39709997177124, -3.8522000312805176, -3.8996999263763428, -3.9001998901367188, -3.9916000366210938, -4.0355000495910645, -4.318900108337402, -4.348599910736084, -4.364799976348877, -4.412399768829346, -4.515399932861328, -4.309599876403809, -4.611599922180176, -4.376299858093262, -4.677000045776367, -4.910200119018555, -4.909999847412109, -5.0543999671936035, -5.118500232696533, -5.151700019836426, -5.186800003051758, -5.26039981842041, -5.299300193786621, -5.3394999504089355, -5.381700038909912, -4.3730998039245605, -5.238800048828125, -5.624100208282471, -5.6244001388549805, -5.624800205230713, -5.679900169372559, -5.067699909210205, -4.152400016784668, -4.6570000648498535, -4.064799785614014, -4.502999782562256, -3.9019999504089355, -4.962299823760986, -5.068699836730957, -4.750199794769287, -4.12470006942749, -4.190700054168701, -4.626500129699707, -5.003799915313721, -4.977499961853027, -3.8189001083374023, -3.911400079727173, -4.022500038146973, -2.936300039291382, -4.446899890899658, -4.446899890899658, -4.160299777984619, -4.600200176239014, -4.600100040435791, -4.648499965667725, -4.726399898529053, -4.753399848937988, -4.902100086212158, -4.221199989318848, -5.0030999183654785, -5.0767998695373535, -5.287899971008301, -5.336100101470947, -5.496399879455566, -5.496600151062012, -5.496399879455566, -4.497600078582764, -5.55649995803833, -5.556600093841553, -5.620200157165527, -5.620299816131592, -5.6880998611450195, -5.6880998611450195, -5.688399791717529, -5.688399791717529, -4.312099933624268, -3.84689998626709, -5.627099990844727, -4.167399883270264, -4.860599994659424, -4.553500175476074, -5.457499980926514, -4.144499778747559, -4.4197001457214355, -4.589099884033203, -5.039899826049805, -4.77839994430542, -4.81790018081665, -5.133800029754639, -5.176199913024902, -5.078000068664551]}, \"token.table\": {\"Topic\": [1, 4, 6, 1, 4, 4, 7, 1, 5, 6, 2, 4, 6, 2, 7, 7, 1, 2, 6, 7, 1, 2, 3, 4, 6, 4, 7, 5, 5, 1, 4, 1, 4, 1, 2, 4, 7, 1, 5, 1, 4, 1, 1, 1, 4, 7, 1, 2, 4, 5, 7, 4, 5, 1, 3, 6, 1, 3, 7, 2, 3, 5, 4, 7, 4, 3, 2, 5, 4, 7, 6, 7, 4, 6, 1, 1, 6, 4, 5, 1, 4, 7, 1, 3, 4, 3, 2, 3, 4, 2, 5, 6, 3, 7, 2, 5, 4, 6, 2, 1, 6, 5, 1, 2, 7, 4, 4, 5, 7, 2, 3, 7, 2, 2, 3, 2, 5, 6, 1, 1, 6, 4, 1, 2, 7, 3, 5, 2, 1, 4, 4, 7, 1, 1, 3, 6, 7, 5, 3, 6, 5, 1, 2, 3, 6, 2, 4, 1, 2, 6, 7, 2, 2, 7, 2, 6, 7, 2, 7, 4, 6, 5, 5, 2, 3, 6, 1, 2, 3, 4, 5, 6, 6, 3, 6, 5, 5, 7, 4, 4, 2, 1, 5, 3, 3, 5, 7, 1, 2, 7, 1, 6, 1, 4, 5, 6, 7, 1, 2, 7, 6, 1, 3, 6, 2, 3, 4, 7, 4, 3, 4, 6, 4, 7, 2, 3, 6, 2, 4, 5, 2, 5, 7, 5, 1, 2, 7, 5, 5, 3, 1, 2, 4, 6, 1, 3, 2, 6, 2, 1, 7, 3, 6, 1, 7, 1, 2, 3, 7, 1, 3, 7, 7, 7, 7, 4, 1, 2, 5, 6, 7, 5, 2, 1, 3, 4, 6, 1, 2, 4, 5, 6, 7, 2, 5, 7, 3, 6, 7, 3, 4, 5, 7, 1, 2, 3, 5, 6, 7, 3, 3, 2, 4, 3, 3, 2, 1, 2, 3, 1, 2, 5, 6, 3, 2, 1, 3, 4, 5, 7, 6, 3, 4, 3, 4, 2, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 5, 7, 7, 1, 2, 4, 1, 4, 5, 2, 6, 1, 6, 4, 5, 1, 4, 6, 7, 2, 3, 7, 2, 1, 5, 5, 4, 4, 6, 2, 3, 6, 4, 5, 5, 1, 4, 6, 6, 3, 7, 3, 4, 6, 4, 1, 1, 2, 4, 7, 4, 3, 5, 2, 2, 3, 5, 3, 1, 4, 6, 2, 3, 1, 3, 4, 5, 7, 5, 1, 4, 5, 3, 7, 7, 2, 6, 6, 4, 7, 1, 3, 3, 4, 6, 4, 1, 4, 1, 2, 4, 1, 1, 5, 6, 6, 4, 2, 3, 4, 2, 3, 6, 7, 1, 2, 1, 2, 3, 4, 7, 6, 6, 7, 7, 1, 3, 5, 2, 3, 3, 4, 5, 6, 2, 4, 5, 1, 3, 6, 1, 6, 2, 3, 5, 6, 7, 1, 3, 5, 7, 5, 1, 5, 1, 2, 6, 7, 1, 2, 4, 6, 7, 7, 5, 6, 1, 2, 6, 2, 6], \"Freq\": [0.9853145480155945, 0.24074019491672516, 0.7222205996513367, 0.45578160881996155, 0.5371711850166321, 0.6622914671897888, 0.3356819748878479, 0.9948180317878723, 0.928920328617096, 0.989398717880249, 0.9683243036270142, 0.9228968620300293, 0.06018892675638199, 0.9723588228225708, 0.9837468266487122, 0.9715420603752136, 0.48730579018592834, 0.34019461274147034, 0.15630562603473663, 0.009194448590278625, 0.2700873017311096, 0.3079943060874939, 0.2795640528202057, 0.004738374147564173, 0.1326744705438614, 0.17727629840373993, 0.7977433204650879, 0.875974178314209, 0.9606266021728516, 0.9761911034584045, 0.9345865845680237, 0.9761760234832764, 0.025688841938972473, 0.42189815640449524, 0.044410333037376404, 0.39969298243522644, 0.1332309991121292, 0.9719797968864441, 0.9435817003250122, 0.10190730541944504, 0.8662120699882507, 0.9363721013069153, 0.9621376395225525, 0.32956889271736145, 0.6120565533638, 0.047081273049116135, 0.4389322102069855, 0.31150028109550476, 0.21238656342029572, 0.0212386567145586, 0.014159103855490685, 0.986700177192688, 0.9643117189407349, 0.009085607714951038, 0.9812456965446472, 0.009085607714951038, 0.2352200597524643, 0.029402507469058037, 0.7056601643562317, 0.041734479367733, 0.083468958735466, 0.8764240145683289, 0.9001991748809814, 0.9715531468391418, 0.9980866312980652, 0.9849561452865601, 0.26677384972572327, 0.7113968729972839, 0.9070466160774231, 0.9648003578186035, 0.9444072246551514, 0.9911425709724426, 0.9971135854721069, 0.9860563278198242, 0.9757724404335022, 0.02674145996570587, 0.9626926183700562, 0.9468778967857361, 0.9705110192298889, 0.6893141865730286, 0.29163292050361633, 0.9715429544448853, 0.9799251556396484, 0.9555966258049011, 0.020773839205503464, 0.9931103587150574, 0.024940522387623787, 0.972680389881134, 0.9867008328437805, 0.8601937890052795, 0.013233750127255917, 0.11910375207662582, 0.23195217549800873, 0.7591162323951721, 0.903893232345581, 0.992935836315155, 0.9297849535942078, 0.9914964437484741, 0.9755852818489075, 0.9853125214576721, 0.9278370141983032, 0.9505159258842468, 0.97978675365448, 0.8143077492713928, 0.18095727264881134, 0.9619215726852417, 0.9779471158981323, 0.08530962467193604, 0.8815327882766724, 0.9323596358299255, 0.9709675908088684, 0.9532518982887268, 0.952547550201416, 0.9604320526123047, 0.9843570590019226, 0.8111109733581543, 0.051773037761449814, 0.12080375850200653, 0.9789553284645081, 0.9867110848426819, 0.9813134074211121, 0.9068669676780701, 0.881977915763855, 0.10225830972194672, 0.9772928953170776, 0.9155784249305725, 0.8843296766281128, 0.9037945866584778, 0.006986130028963089, 0.9920305013656616, 0.9559475183486938, 0.9427867531776428, 0.9878636598587036, 0.9619258046150208, 0.14383703470230103, 0.6472666263580322, 0.17979629337787628, 0.968448281288147, 0.9178404211997986, 0.9865943789482117, 0.9421853423118591, 0.01053948700428009, 0.7324943542480469, 0.25294768810272217, 0.9847467541694641, 0.1391269713640213, 0.8347618579864502, 0.5785079598426819, 0.3820335566997528, 0.010915243998169899, 0.021830487996339798, 0.9877941608428955, 0.9461765289306641, 0.9169354438781738, 0.8632581233978271, 0.08354111015796661, 0.05569406971335411, 0.02673947997391224, 0.96262127161026, 0.7290228605270386, 0.2777229845523834, 0.9854280948638916, 0.9882662892341614, 0.9835572242736816, 0.9934403300285339, 0.9738506078720093, 0.8446907997131348, 0.07453154027462006, 0.008281282149255276, 0.024843847379088402, 0.04140641167759895, 0.008281282149255276, 0.9806201457977295, 0.05085236206650734, 0.9153425097465515, 0.9255419969558716, 0.9254530072212219, 0.9537075757980347, 0.9654049277305603, 0.9004753232002258, 0.9560503363609314, 0.9861134886741638, 0.8794042468070984, 0.9180450439453125, 0.9711670279502869, 0.9220904111862183, 0.9712613821029663, 0.970561146736145, 0.015883008018136024, 0.9768049716949463, 0.970938503742218, 0.98191237449646, 0.2396441102027893, 0.13874132931232452, 0.05045139417052269, 0.3153212070465088, 0.25225695967674255, 0.32353806495666504, 0.5973010063171387, 0.07466262578964233, 0.9401750564575195, 0.09717590361833572, 0.019435182213783264, 0.8745831847190857, 0.5382068753242493, 0.013455172069370747, 0.08073103427886963, 0.3767448365688324, 0.9812638163566589, 0.26782286167144775, 0.2205599993467331, 0.5041371583938599, 0.9466037154197693, 0.8711448907852173, 0.93589186668396, 0.022826630622148514, 0.022826630622148514, 0.892515242099762, 0.10710182785987854, 0.982532262802124, 0.45871594548225403, 0.37270668148994446, 0.14334872364997864, 0.9938775897026062, 0.1365528255701065, 0.43696901202201843, 0.4096584618091583, 0.9606249928474426, 0.9724035263061523, 0.9216989874839783, 0.2692485451698303, 0.4375288784503937, 0.009616019204258919, 0.2788645625114441, 0.946077287197113, 0.9518940448760986, 0.9215696454048157, 0.9599945545196533, 0.9320633411407471, 0.0357263945043087, 0.9288862347602844, 0.9765192270278931, 0.9548312425613403, 0.9663945436477661, 0.9268356561660767, 0.13910211622714996, 0.2782042324542999, 0.447113960981369, 0.12916624546051025, 0.9928829073905945, 0.12280117720365524, 0.8771512508392334, 0.9268293380737305, 0.9851363301277161, 0.964749276638031, 0.9949368238449097, 0.8180922269821167, 0.033391520380973816, 0.11687032133340836, 0.016695760190486908, 0.016695760190486908, 0.9726125597953796, 0.9528798460960388, 0.6963409781455994, 0.14998112618923187, 0.06427762657403946, 0.07499056309461594, 0.12952087819576263, 0.45830464363098145, 0.05977886542677879, 0.19926288723945618, 0.019926289096474648, 0.12952087819576263, 0.8425908088684082, 0.14043180644512177, 0.9268367886543274, 0.9553024172782898, 0.9201728701591492, 0.927000880241394, 0.6631268262863159, 0.03098723478615284, 0.29128000140190125, 0.012394893914461136, 0.4203889071941376, 0.24122315645217896, 0.028025927022099495, 0.25823891162872314, 0.0440407432615757, 0.009008334018290043, 0.9781681895256042, 0.9522950649261475, 0.98061203956604, 0.011018113233149052, 0.991294264793396, 0.951763927936554, 0.9320623874664307, 0.9637073278427124, 0.9809104204177856, 0.9539622068405151, 0.13695207238197327, 0.6048716306686401, 0.22825345396995544, 0.022825345396995544, 0.9532136917114258, 0.9449371695518494, 0.3325732946395874, 0.2533891797065735, 0.3008996248245239, 0.015836823731660843, 0.1108577623963356, 0.896308958530426, 0.9894886612892151, 0.9563447833061218, 0.9155685305595398, 0.9853242635726929, 0.9687331318855286, 0.6067575216293335, 0.09101362526416779, 0.30337876081466675, 0.4412798285484314, 0.20318563282489777, 0.03848890960216522, 0.0008950909250415862, 0.2604714632034302, 0.042069271206855774, 0.012531273066997528, 0.08779054135084152, 0.8779053688049316, 0.9559267163276672, 0.27122917771339417, 0.6893741488456726, 0.03390364721417427, 0.3212073743343353, 0.13383640348911285, 0.5353456139564514, 0.9683247208595276, 0.9541711211204529, 0.9100797772407532, 0.06825598329305649, 0.1580994427204132, 0.8326570391654968, 0.2556211054325104, 0.6573113799095154, 0.036517299711704254, 0.036517299711704254, 0.027991561219096184, 0.027991561219096184, 0.9517130851745605, 0.9286742210388184, 0.9751399159431458, 0.9229120016098022, 0.9230197668075562, 0.9654030203819275, 0.3215574324131012, 0.6431148648262024, 0.08701037615537643, 0.812096893787384, 0.08701037615537643, 0.9654052257537842, 0.9606269001960754, 0.9221736788749695, 0.038826894015073776, 0.7182976007461548, 0.23296137154102325, 0.9546316266059875, 0.9233619570732117, 0.871144711971283, 0.3017873167991638, 0.09657193720340729, 0.5915031433105469, 0.9464676380157471, 0.9720652103424072, 0.9440339803695679, 0.3123234510421753, 0.5621821880340576, 0.12492937594652176, 0.9006132483482361, 0.861054003238678, 0.12300771474838257, 0.9642214775085449, 0.9788376688957214, 0.20617185533046722, 0.7872015833854675, 0.9798367619514465, 0.3922051787376404, 0.5544970035552979, 0.05409726873040199, 0.10047571361064911, 0.8791624903678894, 0.0165218748152256, 0.38000309467315674, 0.5452218651771545, 0.049565620720386505, 0.0165218748152256, 0.943790078163147, 0.7153370380401611, 0.25831615924835205, 0.039740946143865585, 0.9513689875602722, 0.920516312122345, 0.9597605466842651, 0.9807186126708984, 0.9748297929763794, 0.9240227341651917, 0.7866469621658325, 0.21051114797592163, 0.2272329479455948, 0.7466225624084473, 0.9445624947547913, 0.04019414633512497, 0.9817289710044861, 0.9007557034492493, 0.9867823123931885, 0.9693692922592163, 0.7086254954338074, 0.007538569159805775, 0.2789270579814911, 0.959381103515625, 0.972690761089325, 0.9559718370437622, 0.039832159876823425, 0.9830886125564575, 0.9030255675315857, 0.4383007287979126, 0.3218770921230316, 0.23284727334976196, 0.32290127873420715, 0.42803195118904114, 0.15018664300441742, 0.09011198580265045, 0.09640470892190933, 0.891743540763855, 0.1329154074192047, 0.07974924147129059, 0.2924138903617859, 0.1329154074192047, 0.3721631169319153, 0.8961800336837769, 0.9748204946517944, 0.9242034554481506, 0.8706122040748596, 0.9713631272315979, 0.7396367788314819, 0.2509481906890869, 0.9582682251930237, 0.9798529148101807, 0.9824023842811584, 0.8200925588607788, 0.1782809942960739, 0.9439453482627869, 0.9928227663040161, 0.9769641757011414, 0.9608365893363953, 0.9400264620780945, 0.02600775472819805, 0.9362791776657104, 0.8811512589454651, 0.10573815554380417, 0.4185173213481903, 0.06438727676868439, 0.12877455353736877, 0.30583956837654114, 0.08048409968614578, 0.668121874332428, 0.27281641960144043, 0.0501091405749321, 0.011135363951325417, 0.9648838043212891, 0.24447739124298096, 0.7334321737289429, 0.9889914989471436, 0.8295883536338806, 0.14221514761447906, 0.961624801158905, 0.8685278296470642, 0.006732773967087269, 0.013465547934174538, 0.08752606064081192, 0.026931095868349075, 0.92420494556427, 0.9779700040817261, 0.9487890005111694, 0.012481537647545338, 0.9798007011413574, 0.9713863730430603, 0.9929412007331848, 0.8963204622268677], \"Term\": [\"across\", \"act\", \"act\", \"action\", \"action\", \"add\", \"add\", \"address\", \"administration\", \"affordable\", \"ago\", \"agree\", \"agree\", \"ahead\", \"air\", \"almost\", \"america\", \"america\", \"america\", \"america\", \"american\", \"american\", \"american\", \"american\", \"american\", \"americas\", \"americas\", \"americorps\", \"announce\", \"assault\", \"away\", \"back\", \"back\", \"big\", \"big\", \"big\", \"big\", \"bill\", \"birthday\", \"board\", \"board\", \"broken\", \"budget\", \"call\", \"call\", \"call\", \"can\", \"can\", \"can\", \"can\", \"can\", \"carbon\", \"card\", \"care\", \"care\", \"care\", \"case\", \"case\", \"case\", \"celebrate\", \"celebrate\", \"celebrate\", \"challenge\", \"chance\", \"change\", \"check\", \"child\", \"child\", \"china\", \"chip\", \"class\", \"clean\", \"climate\", \"college\", \"commonsense\", \"community\", \"community\", \"company\", \"conference\", \"congress\", \"congress\", \"consecutive\", \"country\", \"court\", \"court\", \"cover\", \"coverage\", \"coverage\", \"cut\", \"day\", \"day\", \"day\", \"deadline\", \"deadline\", \"deficit\", \"deliver\", \"denier\", \"deserve\", \"difference\", \"discuss\", \"dollar\", \"easy\", \"economic\", \"economy\", \"economy\", \"editorial\", \"effect\", \"energy\", \"energy\", \"enroll\", \"enrollment\", \"enter\", \"equal\", \"equality\", \"et\", \"every\", \"every\", \"every\", \"everyone\", \"expand\", \"fair\", \"fairly\", \"family\", \"family\", \"fast\", \"february\", \"federal\", \"fellow\", \"fight\", \"fight\", \"fill\", \"five\", \"fix\", \"forward\", \"free\", \"free\", \"free\", \"future\", \"game\", \"garland\", \"generation\", \"get\", \"get\", \"get\", \"give\", \"global\", \"global\", \"go\", \"go\", \"go\", \"go\", \"good\", \"grassroot\", \"grassroots\", \"grow\", \"grow\", \"grow\", \"growth\", \"growth\", \"gun\", \"gun\", \"happen\", \"happy\", \"hard\", \"health\", \"hearing\", \"help\", \"help\", \"help\", \"help\", \"help\", \"help\", \"high\", \"hold\", \"hold\", \"honor\", \"hope\", \"house\", \"ht\", \"huge\", \"immigrant\", \"immigration\", \"increase\", \"industry\", \"insurance\", \"invest\", \"iran\", \"issue\", \"job\", \"job\", \"join\", \"judge\", \"just\", \"just\", \"just\", \"just\", \"just\", \"keep\", \"keep\", \"keep\", \"kid\", \"know\", \"know\", \"know\", \"last\", \"last\", \"last\", \"last\", \"lead\", \"leader\", \"leader\", \"leader\", \"leadership\", \"least\", \"leave\", \"leave\", \"leave\", \"let\", \"let\", \"life\", \"like\", \"like\", \"like\", \"live\", \"long\", \"long\", \"long\", \"lot\", \"love\", \"low\", \"make\", \"make\", \"make\", \"make\", \"man\", \"marketplace\", \"marriage\", \"may\", \"mean\", \"meet\", \"meet\", \"message\", \"middle\", \"middleclass\", \"midnight\", \"million\", \"million\", \"million\", \"million\", \"minimum\", \"miss\", \"miss\", \"moment\", \"month\", \"movement\", \"name\", \"nation\", \"nation\", \"nation\", \"nation\", \"nation\", \"national\", \"nearly\", \"need\", \"need\", \"need\", \"need\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"next\", \"next\", \"night\", \"nomination\", \"nominee\", \"november\", \"now\", \"now\", \"now\", \"now\", \"obama\", \"obama\", \"obama\", \"obama\", \"obama\", \"obama\", \"obamacare\", \"obamas\", \"one\", \"one\", \"open\", \"option\", \"organizing\", \"pass\", \"pay\", \"peace\", \"people\", \"people\", \"people\", \"people\", \"petition\", \"place\", \"plan\", \"plan\", \"plan\", \"plan\", \"plan\", \"pledge\", \"pm\", \"politic\", \"political\", \"pollution\", \"poverty\", \"power\", \"power\", \"power\", \"president\", \"president\", \"president\", \"president\", \"president\", \"president\", \"president\", \"private\", \"private\", \"privatesector\", \"progress\", \"progress\", \"progress\", \"protect\", \"protect\", \"protect\", \"proud\", \"quality\", \"raise\", \"raise\", \"read\", \"read\", \"ready\", \"ready\", \"ready\", \"ready\", \"record\", \"record\", \"record\", \"reduce\", \"reform\", \"refuse\", \"remark\", \"reminder\", \"retweet\", \"retweet\", \"right\", \"right\", \"right\", \"roll\", \"run\", \"save\", \"say\", \"say\", \"say\", \"school\", \"season\", \"sector\", \"senate\", \"senate\", \"senate\", \"senator\", \"send\", \"sexual\", \"show\", \"show\", \"show\", \"shutdown\", \"sign\", \"sign\", \"since\", \"six\", \"speak\", \"speak\", \"st\", \"stand\", \"stand\", \"stand\", \"start\", \"start\", \"state\", \"state\", \"state\", \"state\", \"state\", \"statement\", \"step\", \"step\", \"step\", \"stop\", \"straight\", \"streak\", \"strong\", \"student\", \"succeed\", \"support\", \"support\", \"supporter\", \"supporter\", \"supreme\", \"supreme\", \"sure\", \"survey\", \"system\", \"tackle\", \"take\", \"take\", \"take\", \"talk\", \"team\", \"th\", \"th\", \"think\", \"thousand\", \"time\", \"time\", \"time\", \"today\", \"today\", \"today\", \"today\", \"together\", \"together\", \"tonight\", \"tonight\", \"tonight\", \"tonight\", \"tonight\", \"town\", \"trade\", \"treat\", \"trip\", \"truth\", \"tune\", \"tune\", \"unemployment\", \"uninsured\", \"union\", \"united\", \"united\", \"upordown\", \"us\", \"vacancy\", \"veteran\", \"volunteer\", \"vote\", \"vote\", \"wage\", \"wage\", \"want\", \"want\", \"want\", \"want\", \"want\", \"watch\", \"watch\", \"watch\", \"watch\", \"water\", \"week\", \"week\", \"weekly\", \"well\", \"well\", \"white\", \"will\", \"will\", \"will\", \"will\", \"will\", \"wind\", \"wish\", \"word\", \"work\", \"work\", \"worker\", \"year\", \"yet\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [6, 7, 5, 2, 4, 3, 1]};\n",
              "\n",
              "function LDAvis_load_lib(url, callback){\n",
              "  var s = document.createElement('script');\n",
              "  s.src = url;\n",
              "  s.async = true;\n",
              "  s.onreadystatechange = s.onload = callback;\n",
              "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
              "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "}\n",
              "\n",
              "if(typeof(LDAvis) !== \"undefined\"){\n",
              "   // already loaded: just create the visualization\n",
              "   !function(LDAvis){\n",
              "       new LDAvis(\"#\" + \"ldavis_el59041402016498215209392429396\", ldavis_el59041402016498215209392429396_data);\n",
              "   }(LDAvis);\n",
              "}else if(typeof define === \"function\" && define.amd){\n",
              "   // require.js is available: use it to load d3/LDAvis\n",
              "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
              "   require([\"d3\"], function(d3){\n",
              "      window.d3 = d3;\n",
              "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
              "        new LDAvis(\"#\" + \"ldavis_el59041402016498215209392429396\", ldavis_el59041402016498215209392429396_data);\n",
              "      });\n",
              "    });\n",
              "}else{\n",
              "    // require.js not available: dynamically load d3 & LDAvis\n",
              "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
              "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
              "                 new LDAvis(\"#\" + \"ldavis_el59041402016498215209392429396\", ldavis_el59041402016498215209392429396_data);\n",
              "            })\n",
              "         });\n",
              "}\n",
              "</script>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ySmWqnB6GOVJ"
      },
      "source": [
        "#####<pre>      Mi nivel de inglés es paupérrimo y sé si podré identificar claramente el contenido de los topics...\n",
        "#####<pre>      ...pero arriesgándome diría que probablemente OBAMA:\n",
        "~~~\n",
        "          > En el Topic 1 habla de temas como la inmigración y la economía de las familias de clase media americana\n",
        "          > En el Topic 2 habla del progreso y de la fortaleza de la economía de América\n",
        "          > En el Topic 3 trata temas de sanidad, salud, seguridad\n",
        "          > En el Topic 4 hablá de combatir la polución y de acciones globales contra el cambio climático\n",
        "          > En el Topic 5 parece que habla sobre alguna celebración o cumpleaños\n",
        "          > El Topic 6 parece tratar temas de educación\n",
        "          > El Topic 7 podría tratar sobre alguna aparición que hiciera en directo hablando sobre economía, energía y crecimiento\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uWk_0cDa3o0B"
      },
      "source": [
        "#####<pre>      Vamos ahora con los tweets de **TRUMP**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "09bf4da9-724d-4603-9931-2f677c4820fd",
        "id": "hRYxDtYr3o0D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# generamos el vocabulario gensim\n",
        "gensim_dict_t = Dictionary(docs_trump)\n",
        "\n",
        "# generamos con gensim el corpus \n",
        "corpus_t = [gensim_dict_t.doc2bow(doc) for doc in docs_trump]\n",
        "\n",
        "print(f\"Del presidente Trump tenemos {len(corpus_t)} documentos con {len(gensim_dict_t)} tokens/palabras diferentes\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Del presidente Trump tenemos 2862 documentos con 5086 tokens/palabras diferentes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sWzLkwiu3o0G"
      },
      "source": [
        "#####<pre>      Entrenamos el modelo ***LDA*** que implementaremos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p89_L9pf3o0H",
        "colab": {}
      },
      "source": [
        "# Entrenamos modelos con valores para número de topics del 2 al 10\n",
        "model_list_t, coherence_values_t = compute_coherence_values(gensim_dict_t, corpus_t, docs_trump)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "2b72c687-86f5-4d62-b17d-cc267c06ccc9",
        "id": "NtCq0D5h3o0J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "# Visualizamos el entrenamiento\n",
        "plot_cohe_topics(coherence_values_t)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEICAYAAABF82P+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7ko1GWom3o0K"
      },
      "source": [
        "#####<pre>      Según la gráfica, el número de topics óptimo para el modelo de clustering de tweets de **TRUMP** son **8**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0Z-y_CQz3o0L",
        "colab": {}
      },
      "source": [
        "# implementamos el modelo con 8 topics\n",
        "lda_t = models.LdaModel(corpus_t, id2word = gensim_dict_t, num_topics = 8, iterations = 5, passes = 20, alpha = 'auto')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "615d4a05-8271-491a-e938-78f4fc05fe38",
        "id": "3SsvtjSN3o0N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 997
        }
      },
      "source": [
        "# Visualizamos con pyLDAvis\n",
        "vis_t = pyLDAvis.gensim.prepare(lda_t, corpus_t, gensim_dict_t)\n",
        "pyLDAvis.display(vis_t)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
            "of pandas will change to not sort by default.\n",
            "\n",
            "To accept the future behavior, pass 'sort=False'.\n",
            "\n",
            "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
            "\n",
            "  return pd.concat([default_term_info] + list(topic_dfs))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
              "\n",
              "\n",
              "<div id=\"ldavis_el59041402016301290322506053465\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "\n",
              "var ldavis_el59041402016301290322506053465_data = {\"mdsDat\": {\"x\": [0.1798230964944996, 0.1301532557686063, 0.10450339488298697, 0.0441176771227763, -0.16742844026236137, -0.13401425090171165, -0.010657329135892742, -0.14649740396890362], \"y\": [0.09375316907967907, -0.14120806333010266, -0.10551972516633477, 0.20110097745856692, -0.05037507120905913, -0.12566252290605195, -0.021064173640325225, 0.14897540971362783], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [22.32525634765625, 16.832090377807617, 16.264902114868164, 12.453411102294922, 11.468018531799316, 9.36364459991455, 6.719313621520996, 4.573361873626709]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\"], \"Freq\": [215.0, 244.0, 194.0, 144.0, 165.0, 126.0, 661.0, 639.0, 179.0, 104.0, 207.0, 213.0, 162.0, 113.0, 97.0, 144.0, 107.0, 212.0, 246.0, 220.0, 103.0, 65.0, 121.0, 93.0, 96.0, 110.0, 74.0, 70.0, 193.0, 105.0, 74.20387268066406, 119.59010314941406, 62.403358459472656, 59.713905334472656, 78.73411560058594, 42.373291015625, 37.79779815673828, 55.543697357177734, 35.84622573852539, 33.279727935791016, 43.3780632019043, 30.66818618774414, 37.607086181640625, 30.021541595458984, 27.421749114990234, 26.78279685974121, 26.123472213745117, 24.81387710571289, 21.57697105407715, 20.27384376525879, 20.24563980102539, 19.628679275512695, 19.628103256225586, 18.98558807373047, 18.31560516357422, 18.315446853637695, 17.03045082092285, 17.034042358398438, 16.380083084106445, 16.3792781829834, 23.771595001220703, 193.2326202392578, 186.463623046875, 34.66038513183594, 30.323583602905273, 154.7374725341797, 78.10130310058594, 49.60170364379883, 148.65170288085938, 45.46855545043945, 48.51011657714844, 87.40660095214844, 63.49955749511719, 76.56704711914062, 34.665740966796875, 322.5201416015625, 307.05810546875, 41.91164016723633, 65.55912780761719, 104.27204132080078, 179.07464599609375, 69.96379089355469, 73.193115234375, 62.14299774169922, 44.26789474487305, 68.90165710449219, 52.67188262939453, 45.37065124511719, 47.336238861083984, 165.0243377685547, 77.63016510009766, 78.81766510009766, 95.27505493164062, 46.25447082519531, 51.696781158447266, 40.10088348388672, 41.85516357421875, 37.69990921020508, 21.03089714050293, 22.199121475219727, 20.431259155273438, 19.19757652282715, 171.88397216796875, 18.58823585510254, 18.58795738220215, 17.95951271057129, 17.347272872924805, 19.63060188293457, 14.896280288696289, 15.441935539245605, 66.56365203857422, 14.284972190856934, 14.280803680419922, 13.64808177947998, 12.433666229248047, 11.803239822387695, 11.193817138671875, 11.195757865905762, 11.20093822479248, 99.28751373291016, 36.43571472167969, 21.36438751220703, 75.93592834472656, 100.49238586425781, 38.723453521728516, 58.85271072387695, 77.41681671142578, 96.5982666015625, 90.97322082519531, 140.83782958984375, 56.536834716796875, 104.77816772460938, 103.01179504394531, 65.26276397705078, 101.07356262207031, 39.64436340332031, 80.97408294677734, 61.18242263793945, 61.38618850708008, 60.278682708740234, 156.14610290527344, 127.75651550292969, 83.1177978515625, 67.4273452758789, 50.36369323730469, 66.58616638183594, 57.253639221191406, 244.07986450195312, 112.8154067993164, 106.37686157226562, 102.50745391845703, 92.83720397949219, 67.03878784179688, 54.832855224609375, 52.282196044921875, 57.34742736816406, 49.71559143066406, 51.3577995300293, 40.53679275512695, 31.68571662902832, 47.256004333496094, 27.83085823059082, 40.73079299926758, 23.955020904541016, 23.287700653076172, 24.531980514526367, 16.878366470336914, 16.882122039794922, 16.878311157226562, 16.86217498779297, 16.858036041259766, 16.86554527282715, 16.880023956298828, 16.861095428466797, 16.845870971679688, 15.585235595703125, 15.580705642700195, 42.448387145996094, 48.95830154418945, 43.231658935546875, 36.37797927856445, 75.56912231445312, 37.61836242675781, 138.13182067871094, 73.5302734375, 131.00559997558594, 64.39596557617188, 79.2294921875, 56.792724609375, 150.2639617919922, 64.5541000366211, 58.364295959472656, 51.96067810058594, 56.82168197631836, 58.23811340332031, 68.34935760498047, 50.873817443847656, 47.61988067626953, 214.89425659179688, 193.76002502441406, 96.85985565185547, 54.92634582519531, 35.55305099487305, 33.901611328125, 27.26191520690918, 25.587881088256836, 24.478422164916992, 23.365495681762695, 21.673410415649414, 21.143959045410156, 20.610837936401367, 20.04877471923828, 17.827030181884766, 17.819374084472656, 17.282634735107422, 16.724416732788086, 16.733259201049805, 16.17355728149414, 16.174095153808594, 15.62195873260498, 14.509474754333496, 13.951560974121094, 13.40404224395752, 12.842489242553711, 12.292405128479004, 12.253806114196777, 11.71247673034668, 11.195579528808594, 28.02678108215332, 34.832923889160156, 28.07798957824707, 48.00497055053711, 112.65504455566406, 100.0462417602539, 45.82166290283203, 33.96608352661133, 21.945158004760742, 24.26618003845215, 101.34247589111328, 87.41310119628906, 179.7996368408203, 23.330026626586914, 51.63636016845703, 32.4459114074707, 44.82681655883789, 80.04283142089844, 25.60127067565918, 35.55678176879883, 33.61079406738281, 43.61109161376953, 34.235694885253906, 38.4242057800293, 28.917736053466797, 29.350215911865234, 27.151697158813477, 143.0188751220703, 69.8077392578125, 72.67762756347656, 39.63723373413086, 32.362430572509766, 23.142608642578125, 21.278194427490234, 21.177961349487305, 18.80638885498047, 16.921297073364258, 16.924482345581055, 16.313716888427734, 64.70021057128906, 58.91147994995117, 13.191231727600098, 12.568402290344238, 12.581873893737793, 12.558612823486328, 68.8053970336914, 20.065935134887695, 11.329140663146973, 63.77224349975586, 10.666258811950684, 68.8790283203125, 11.136250495910645, 19.096378326416016, 8.841557502746582, 8.830767631530762, 28.22975730895996, 8.21895694732666, 46.54145812988281, 42.026084899902344, 22.958236694335938, 23.58751678466797, 22.65531349182129, 47.03571319580078, 52.03060531616211, 26.955461502075195, 27.25920295715332, 56.29201126098633, 24.490564346313477, 34.78877639770508, 52.176536560058594, 52.91859817504883, 42.426700592041016, 27.24972915649414, 28.2752628326416, 24.390254974365234, 103.0418472290039, 123.9768295288086, 30.607757568359375, 30.59308624267578, 24.1427059173584, 21.221830368041992, 20.626422882080078, 21.169618606567383, 20.04984474182129, 18.875822067260742, 18.885826110839844, 18.282062530517578, 23.90314483642578, 18.82291030883789, 15.945723533630371, 18.193931579589844, 15.355298042297363, 14.779454231262207, 14.173967361450195, 13.018527030944824, 13.008235931396484, 11.841752052307129, 11.823811531066895, 11.219706535339355, 10.674964904785156, 10.081188201904297, 9.487310409545898, 9.495796203613281, 47.375038146972656, 46.720951080322266, 28.404951095581055, 13.247289657592773, 19.506189346313477, 36.862266540527344, 29.82398223876953, 40.04832458496094, 35.070465087890625, 32.70558166503906, 24.980098724365234, 24.053497314453125, 26.32164764404297, 30.21607208251953, 17.524412155151367, 35.02745056152344, 20.371734619140625, 22.173583984375, 19.44597053527832, 64.30245971679688, 17.496747970581055, 16.83879852294922, 13.815552711486816, 12.755899429321289, 11.701955795288086, 11.179558753967285, 10.651641845703125, 9.592572212219238, 9.064446449279785, 8.537384986877441, 8.02137565612793, 9.914761543273926, 7.490131378173828, 7.488092422485352, 6.965771675109863, 6.959737777709961, 6.956686496734619, 6.959433555603027, 16.007333755493164, 15.999907493591309, 6.435046195983887, 6.438687801361084, 6.855912208557129, 5.90662145614624, 5.90636682510376, 5.9061360359191895, 5.908904552459717, 5.90782356262207, 6.3679423332214355, 21.941333770751953, 9.297385215759277, 29.30039405822754, 12.53979206085205, 8.272167205810547, 14.989917755126953, 7.427830219268799, 13.47659969329834, 14.376664161682129, 22.022560119628906, 13.691903114318848, 33.04948425292969, 21.23981475830078, 12.245816230773926, 21.590648651123047, 38.79194641113281, 49.37784957885742, 13.286587715148926, 20.1590518951416, 26.361085891723633, 29.71784782409668, 18.003273010253906, 15.250421524047852, 13.690333366394043, 38.63439178466797, 27.58112335205078, 24.684532165527344, 22.266586303710938, 22.26447868347168, 20.81296730041504, 15.990046501159668, 15.044904708862305, 10.695979118347168, 10.21906852722168, 8.28677749633789, 7.803982734680176, 7.327660083770752, 7.327390193939209, 7.321864128112793, 7.324575424194336, 7.318912029266357, 7.321585655212402, 7.318619251251221, 6.845210075378418, 6.842425346374512, 6.8424072265625, 6.842379570007324, 6.8396124839782715, 6.838871002197266, 6.359809875488281, 6.359801292419434, 5.8835015296936035, 5.876988887786865, 5.876946449279785, 13.463154792785645, 10.32401180267334, 9.452165603637695, 7.8770318031311035, 20.7457218170166, 18.548646926879883, 11.911656379699707, 10.188425064086914, 7.918732643127441, 8.110565185546875, 7.250359058380127, 7.594111919403076, 8.193408012390137, 7.441686630249023, 7.4221696853637695], \"Term\": [\"news\", \"job\", \"fake\", \"trump\", \"republican\", \"tax\", \"will\", \"great\", \"democrat\", \"cut\", \"year\", \"today\", \"america\", \"korea\", \"medium\", \"honor\", \"trade\", \"country\", \"us\", \"thank\", \"north\", \"help\", \"border\", \"china\", \"healthcare\", \"dem\", \"hillary\", \"obama\", \"american\", \"russia\", \"law\", \"border\", \"wall\", \"forward\", \"national\", \"protect\", \"safe\", \"happy\", \"proud\", \"tomorrow\", \"build\", \"veteran\", \"stand\", \"morning\", \"member\", \"enforcement\", \"tonight\", \"travel\", \"sanctuary\", \"steel\", \"gang\", \"behalf\", \"represent\", \"rebuild\", \"ohio\", \"ban\", \"officer\", \"federal\", \"criminal\", \"vet\", \"luther\", \"thank\", \"country\", \"hero\", \"drug\", \"american\", \"security\", \"put\", \"state\", \"wonderful\", \"love\", \"military\", \"nation\", \"united\", \"always\", \"will\", \"great\", \"melania\", \"first\", \"today\", \"amp\", \"must\", \"look\", \"day\", \"man\", \"people\", \"big\", \"one\", \"us\", \"republican\", \"senator\", \"obamacare\", \"healthcare\", \"repeal\", \"lose\", \"replace\", \"plan\", \"daca\", \"stay\", \"approve\", \"chance\", \"york\", \"democrat\", \"historic\", \"player\", \"obstructionist\", \"premium\", \"fix\", \"better\", \"budget\", \"senate\", \"disrespect\", \"fine\", \"line\", \"obstruct\", \"karen\", \"deductible\", \"laugh\", \"except\", \"dem\", \"fail\", \"excuse\", \"hard\", \"can\", \"pass\", \"bill\", \"win\", \"want\", \"vote\", \"get\", \"thing\", \"work\", \"go\", \"never\", \"now\", \"right\", \"time\", \"bad\", \"take\", \"election\", \"will\", \"amp\", \"people\", \"year\", \"need\", \"great\", \"big\", \"job\", \"korea\", \"trade\", \"north\", \"china\", \"last\", \"dollar\", \"market\", \"record\", \"stock\", \"billion\", \"economic\", \"mexico\", \"number\", \"unemployment\", \"economy\", \"relationship\", \"confidence\", \"longer\", \"kim\", \"nuclear\", \"un\", \"alltime\", \"enthusiasm\", \"council\", \"large\", \"solve\", \"exciting\", \"deficit\", \"create\", \"business\", \"low\", \"night\", \"usa\", \"high\", \"another\", \"year\", \"deal\", \"us\", \"well\", \"much\", \"back\", \"will\", \"just\", \"good\", \"new\", \"president\", \"big\", \"great\", \"many\", \"make\", \"news\", \"fake\", \"medium\", \"join\", \"minister\", \"prime\", \"cnn\", \"host\", \"press\", \"france\", \"bedminster\", \"virginia\", \"dishonest\", \"media\", \"afternoon\", \"west\", \"nbc\", \"conference\", \"canada\", \"abc\", \"reporting\", \"land\", \"condolence\", \"social\", \"amazing\", \"paris\", \"name\", \"wish\", \"return\", \"champion\", \"presidential\", \"live\", \"w\", \"welcome\", \"america\", \"honor\", \"white\", \"speak\", \"remark\", \"wh\", \"make\", \"today\", \"great\", \"general\", \"day\", \"report\", \"house\", \"amp\", \"two\", \"meeting\", \"story\", \"just\", \"say\", \"president\", \"much\", \"people\", \"election\", \"trump\", \"obama\", \"hillary\", \"crooked\", \"john\", \"refuse\", \"dnc\", \"director\", \"book\", \"address\", \"server\", \"spy\", \"fbi\", \"administration\", \"investigate\", \"meddling\", \"dossier\", \"optimism\", \"clinton\", \"hoax\", \"disgrace\", \"russian\", \"nobody\", \"campaign\", \"question\", \"official\", \"involve\", \"weekly\", \"fire\", \"abuse\", \"show\", \"phony\", \"email\", \"donald\", \"investigation\", \"collusion\", \"story\", \"month\", \"lie\", \"russia\", \"total\", \"know\", \"now\", \"just\", \"president\", \"watch\", \"say\", \"amp\", \"cut\", \"tax\", \"reform\", \"jame\", \"leak\", \"deliver\", \"post\", \"executive\", \"committee\", \"raise\", \"charge\", \"amazon\", \"agree\", \"focus\", \"chuck\", \"office\", \"supreme\", \"mccabe\", \"germany\", \"schumer\", \"protection\", \"jeff\", \"collude\", \"clapper\", \"process\", \"classified\", \"sc\", \"review\", \"hunt\", \"witch\", \"find\", \"andrew\", \"information\", \"taxis\", \"court\", \"comey\", \"order\", \"massive\", \"ask\", \"washington\", \"crime\", \"give\", \"political\", \"amp\", \"congress\", \"much\", \"include\", \"help\", \"insurance\", \"play\", \"syria\", \"easy\", \"podesta\", \"everybody\", \"h\", \"expect\", \"majority\", \"bail\", \"bob\", \"rep\", \"less\", \"product\", \"supporter\", \"uranium\", \"women\", \"expensive\", \"rico\", \"puerto\", \"bernie\", \"rick\", \"rid\", \"shape\", \"pray\", \"crash\", \"ensure\", \"fema\", \"corker\", \"reason\", \"whatever\", \"money\", \"isis\", \"necessary\", \"serve\", \"mayor\", \"fast\", \"success\", \"company\", \"kill\", \"russia\", \"pay\", \"open\", \"know\", \"us\", \"amp\", \"dead\", \"win\", \"people\", \"will\", \"big\", \"want\", \"get\", \"life\", \"god\", \"government\", \"home\", \"action\", \"save\", \"freedom\", \"bless\", \"ahead\", \"room\", \"playing\", \"k\", \"defeat\", \"wait\", \"field\", \"shut\", \"caucus\", \"egypt\", \"blumenthal\", \"value\", \"enough\", \"remembrance\", \"ceremony\", \"worship\", \"seven\", \"evil\", \"ivanka\", \"graham\", \"schedule\", \"tape\", \"florida\", \"hand\", \"vietnam\", \"j\", \"president\", \"today\", \"welcome\", \"like\", \"may\", \"beautiful\", \"judge\", \"truly\", \"people\", \"long\", \"amp\"], \"Total\": [215.0, 244.0, 194.0, 144.0, 165.0, 126.0, 661.0, 639.0, 179.0, 104.0, 207.0, 213.0, 162.0, 113.0, 97.0, 144.0, 107.0, 212.0, 246.0, 220.0, 103.0, 65.0, 121.0, 93.0, 96.0, 110.0, 74.0, 70.0, 193.0, 105.0, 75.04727935791016, 121.20581817626953, 63.357913970947266, 60.73017120361328, 80.14527130126953, 43.18135070800781, 38.627174377441406, 56.76882553100586, 36.6821174621582, 34.088340759277344, 44.48324203491211, 31.47750473022461, 38.6213264465332, 30.8345947265625, 28.23213005065918, 27.59334373474121, 26.931255340576172, 25.621870040893555, 22.384750366210938, 21.08319854736328, 21.078323364257812, 20.436738967895508, 20.436717987060547, 19.79414939880371, 19.123462677001953, 19.123443603515625, 17.838253021240234, 17.842220306396484, 17.187641143798828, 17.187564849853516, 24.965974807739258, 220.0623779296875, 212.79981994628906, 37.33211135864258, 32.880123138427734, 193.26785278320312, 93.20610046386719, 57.17402648925781, 195.6907501220703, 51.98382568359375, 55.95682907104492, 111.87423706054688, 77.43580627441406, 100.62508392333984, 38.80011749267578, 661.8886108398438, 639.7031860351562, 51.411563873291016, 102.58308410644531, 213.6702880859375, 544.82177734375, 128.43731689453125, 142.44923400878906, 125.16706848144531, 60.825740814208984, 243.461669921875, 233.1365509033203, 87.96222686767578, 246.22825622558594, 165.84690856933594, 78.45132446289062, 79.65404510498047, 96.76824951171875, 47.06902313232422, 52.62932586669922, 40.915611267089844, 42.74616241455078, 38.51975631713867, 21.860544204711914, 23.07929801940918, 21.246034622192383, 20.017908096313477, 179.27732849121094, 19.405420303344727, 19.411544799804688, 18.774024963378906, 18.16172218322754, 20.644304275512695, 15.712461471557617, 16.310955047607422, 70.3257064819336, 15.100268363952637, 15.096444129943848, 14.477156639099121, 13.248151779174805, 12.620361328125, 12.008307456970215, 12.011923789978027, 12.022492408752441, 110.58574676513672, 39.78647232055664, 23.089134216308594, 91.9180908203125, 127.8495864868164, 45.900352478027344, 75.71070861816406, 104.85529327392578, 139.15908813476562, 130.9608917236328, 229.55166625976562, 76.49007415771484, 171.45126342773438, 172.10626220703125, 104.5063247680664, 196.61553955078125, 55.52098846435547, 158.38534545898438, 105.30294799804688, 107.47085571289062, 111.98451232910156, 661.8886108398438, 544.82177734375, 243.461669921875, 207.48245239257812, 97.27490997314453, 639.7031860351562, 233.1365509033203, 244.91920471191406, 113.62540435791016, 107.19597625732422, 103.31697845458984, 93.6572036743164, 67.9054946899414, 55.672515869140625, 53.091941833496094, 58.2669563293457, 50.525753021240234, 52.40671920776367, 41.47768020629883, 32.49519348144531, 48.545570373535156, 28.640060424804688, 42.101139068603516, 24.764892578125, 24.1114559173584, 25.400745391845703, 17.687150955200195, 17.691123962402344, 17.68714714050293, 17.671184539794922, 17.667173385620117, 17.675134658813477, 17.691059112548828, 17.674684524536133, 17.666168212890625, 16.394113540649414, 16.39373779296875, 44.72719192504883, 56.107303619384766, 49.27012634277344, 41.397335052490234, 98.68836975097656, 44.86552047729492, 207.48245239257812, 105.8419418334961, 246.22825622558594, 100.8868179321289, 132.550048828125, 100.20439147949219, 661.8886108398438, 202.9537811279297, 163.8057098388672, 129.89450073242188, 184.21388244628906, 233.1365509033203, 639.7031860351562, 165.4320068359375, 226.18194580078125, 215.89706420898438, 194.83596801757812, 97.85701751708984, 55.7735481262207, 36.389564514160156, 34.730384826660156, 28.08904457092285, 26.414920806884766, 25.305740356445312, 24.193042755126953, 22.526201248168945, 21.987197875976562, 21.440876007080078, 20.875782012939453, 18.6539249420166, 18.66111946105957, 18.10963249206543, 17.551591873168945, 17.562055587768555, 17.00041961669922, 17.00420379638672, 16.449241638183594, 15.336606979370117, 14.778564453125, 14.238579750061035, 13.66935920715332, 13.125506401062012, 13.140766143798828, 12.566494941711426, 12.02273941040039, 30.38568115234375, 39.762611389160156, 32.33595275878906, 60.6471061706543, 162.0183563232422, 144.94493103027344, 59.18489456176758, 42.24458312988281, 25.452957153320312, 29.835813522338867, 226.18194580078125, 213.6702880859375, 639.7031860351562, 29.76616096496582, 125.16706848144531, 55.77105712890625, 106.77082824707031, 544.82177734375, 37.581024169921875, 95.84807586669922, 88.30868530273438, 202.9537811279297, 109.16094207763672, 184.21388244628906, 132.550048828125, 243.461669921875, 111.98451232910156, 144.61196899414062, 70.64826202392578, 74.31320190429688, 40.76709747314453, 33.296058654785156, 23.956567764282227, 22.09153938293457, 22.085695266723633, 19.6223201751709, 17.7346248626709, 17.738449096679688, 17.12704086303711, 68.01414489746094, 62.02027893066406, 14.007588386535645, 13.381535530090332, 13.39682674407959, 13.373889923095703, 73.49561309814453, 21.481979370117188, 12.143174171447754, 68.69622039794922, 11.52180004119873, 74.707763671875, 12.132030487060547, 20.835283279418945, 9.655248641967773, 9.643715858459473, 30.899940490722656, 9.032269477844238, 51.1525764465332, 46.54290008544922, 25.56568145751953, 26.475324630737305, 26.286348342895508, 60.43556594848633, 88.30868530273438, 37.118221282958984, 38.47159957885742, 105.15880584716797, 34.857173919677734, 65.2155990600586, 196.61553955078125, 202.9537811279297, 184.21388244628906, 77.71346282958984, 109.16094207763672, 544.82177734375, 104.1920166015625, 126.37643432617188, 31.429109573364258, 31.414613723754883, 24.9634952545166, 22.043989181518555, 21.454116821289062, 22.02388572692871, 20.870420455932617, 19.696842193603516, 19.707632064819336, 19.1027889251709, 24.992359161376953, 19.695756912231445, 16.766494750976562, 19.131301879882812, 16.176071166992188, 15.60017204284668, 14.99525260925293, 13.83976936340332, 13.828886032104492, 12.662562370300293, 12.652002334594727, 12.068960189819336, 11.496232986450195, 10.902130126953125, 10.308140754699707, 10.3192720413208, 51.50872039794922, 50.92527389526367, 30.9844913482666, 14.440963745117188, 21.51931381225586, 41.5472412109375, 33.45023727416992, 60.70734786987305, 52.341434478759766, 53.69157028198242, 42.955596923828125, 41.71370315551758, 61.154476165771484, 95.84352111816406, 27.87603759765625, 544.82177734375, 61.11460494995117, 132.550048828125, 50.838584899902344, 65.92667388916016, 18.329313278198242, 17.85121726989746, 14.649956703186035, 13.59481430053711, 12.53429889678955, 12.012928009033203, 11.48490047454834, 10.425594329833984, 9.89761734008789, 9.369440078735352, 8.854698181152344, 10.976500511169434, 8.32336711883545, 8.323742866516113, 7.798518657684326, 7.791951656341553, 7.788650989532471, 7.7919840812683105, 18.005130767822266, 18.006458282470703, 7.267193794250488, 7.274254322052002, 7.81604528427124, 6.739254474639893, 6.739234924316406, 6.739273548126221, 6.742700099945068, 6.746510982513428, 7.285971641540527, 26.207698822021484, 10.882493019104004, 39.31340408325195, 16.22441291809082, 10.080410957336426, 20.97808265686035, 8.877601623535156, 20.577913284301758, 24.23606300354004, 46.9628791809082, 23.371517181396484, 105.15880584716797, 55.572322845458984, 21.375816345214844, 65.2155990600586, 246.22825622558594, 544.82177734375, 31.95458984375, 104.85529327392578, 243.461669921875, 661.8886108398438, 233.1365509033203, 139.15908813476562, 229.55166625976562, 39.517906188964844, 28.422378540039062, 25.527803421020508, 23.109088897705078, 23.109603881835938, 21.65978240966797, 16.837081909179688, 15.886246681213379, 11.537887573242188, 11.061408996582031, 9.128257751464844, 8.645821571350098, 8.169245719909668, 8.169306755065918, 8.163226127624512, 8.16627311706543, 8.160207748413086, 8.163307189941406, 8.160293579101562, 7.686731338500977, 7.683690071105957, 7.6836934089660645, 7.683693885803223, 7.68065071105957, 7.6808576583862305, 7.2012152671813965, 7.201209545135498, 6.7246174812316895, 6.718760013580322, 6.718770503997803, 19.003206253051758, 14.556097030639648, 13.384086608886719, 10.75422191619873, 184.21388244628906, 213.6702880859375, 60.6471061706543, 80.38396453857422, 22.46581268310547, 30.60196304321289, 12.742599487304688, 24.304235458374023, 243.461669921875, 60.3936767578125, 544.82177734375], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.4881000518798828, 1.4859999418258667, 1.4843000173568726, 1.4825999736785889, 1.4816999435424805, 1.4805999994277954, 1.4776999950408936, 1.4775999784469604, 1.4764000177383423, 1.4753999710083008, 1.4743000268936157, 1.4733999967575073, 1.4728000164031982, 1.4726999998092651, 1.4702999591827393, 1.469599962234497, 1.468999981880188, 1.4673999547958374, 1.4627000093460083, 1.4602999687194824, 1.4591000080108643, 1.4591000080108643, 1.4591000080108643, 1.4577000141143799, 1.4563000202178955, 1.4563000202178955, 1.4530999660491943, 1.4530999660491943, 1.451300024986267, 1.451300024986267, 1.4503999948501587, 1.3694000244140625, 1.367300033569336, 1.4251999855041504, 1.4184999465942383, 1.2770999670028687, 1.32260000705719, 1.3573999404907227, 1.2244999408721924, 1.3654999732971191, 1.356600046157837, 1.2525999546051025, 1.3009999990463257, 1.226199984550476, 1.3868000507354736, 0.7804999947547913, 0.765500009059906, 1.295199990272522, 1.0516999959945679, 0.7820000052452087, 0.38679999113082886, 0.8920000195503235, 0.8335999846458435, 0.7991999983787537, 1.1816999912261963, 0.23720000684261322, 0.011900000274181366, 0.8374000191688538, -0.14949999749660492, 1.776900053024292, 1.771399974822998, 1.771299958229065, 1.7662999629974365, 1.7644000053405762, 1.7640000581741333, 1.7618000507354736, 1.7608000040054321, 1.7604000568389893, 1.7431999444961548, 1.7430000305175781, 1.742799997329712, 1.7400000095367432, 1.739799976348877, 1.7388999462127686, 1.7384999990463257, 1.7374999523162842, 1.7359999418258667, 1.7315000295639038, 1.7285000085830688, 1.7271000146865845, 1.7268999814987183, 1.7264000177383423, 1.7263000011444092, 1.7229000329971313, 1.718400001525879, 1.714900016784668, 1.7115999460220337, 1.7115000486373901, 1.7110999822616577, 1.6741000413894653, 1.6938999891281128, 1.704200029373169, 1.59089994430542, 1.541100025177002, 1.611899971961975, 1.5299999713897705, 1.4785000085830688, 1.4168000221252441, 1.4175000190734863, 1.2934000492095947, 1.479599952697754, 1.2893999814987183, 1.2685999870300293, 1.3111000061035156, 1.1165000200271606, 1.445099949836731, 1.1109999418258667, 1.2388999462127686, 1.2217999696731567, 1.162500023841858, 0.3375999927520752, 0.33160001039505005, 0.7071999907493591, 0.6578999757766724, 1.1236000061035156, -0.4805999994277954, 0.37770000100135803, 1.8127000331878662, 1.809000015258789, 1.808500051498413, 1.8083000183105469, 1.8073999881744385, 1.8033000230789185, 1.8009999990463257, 1.8007999658584595, 1.8003000020980835, 1.7999999523162842, 1.7958999872207642, 1.7932000160217285, 1.7908999919891357, 1.789199948310852, 1.787500023841858, 1.7831000089645386, 1.7828999757766724, 1.7813999652862549, 1.7813999652862549, 1.7694000005722046, 1.7694000005722046, 1.7694000005722046, 1.7692999839782715, 1.7692999839782715, 1.7692999839782715, 1.7691999673843384, 1.7690000534057617, 1.7685999870300293, 1.7655999660491943, 1.7653000354766846, 1.7639000415802002, 1.679900050163269, 1.6854000091552734, 1.686900019645691, 1.5492000579833984, 1.6399999856948853, 1.4092999696731567, 1.4519000053405762, 1.1850999593734741, 1.3672000169754028, 1.3014999628067017, 1.2483999729156494, 0.3334999978542328, 0.6707000136375427, 0.7842000126838684, 0.8999000191688538, 0.6399999856948853, 0.42910000681877136, -0.420199990272522, 0.636900007724762, 0.2581000030040741, 2.0785000324249268, 2.0776000022888184, 2.0729000568389893, 2.0678999423980713, 2.0599000453948975, 2.059000015258789, 2.053299903869629, 2.0513999462127686, 2.0499000549316406, 2.0483999252319336, 2.044600009918213, 2.044100046157837, 2.0436999797821045, 2.042799949645996, 2.0378000736236572, 2.0369999408721924, 2.036400079727173, 2.034899950027466, 2.0348000526428223, 2.0332999229431152, 2.033099889755249, 2.031599998474121, 2.0276999473571777, 2.025599956512451, 2.0227999687194824, 2.0208001136779785, 2.0176000595092773, 2.0132999420166016, 2.0127999782562256, 2.011899948120117, 2.0023999214172363, 1.9507999420166016, 1.9420000314712524, 1.8494000434875488, 1.7197999954223633, 1.712499976158142, 1.827299952507019, 1.8651000261306763, 1.9349000453948975, 1.8765000104904175, 1.2803000211715698, 1.1893999576568604, 0.8140000104904175, 1.8394999504089355, 1.1978000402450562, 1.5414999723434448, 1.2152999639511108, 0.16529999673366547, 1.6993000507354736, 1.0915000438690186, 1.1172000169754028, 0.5454999804496765, 0.9236000180244446, 0.5157999992370605, 0.560699999332428, -0.032499998807907104, 0.6662999987602234, 2.1545000076293945, 2.153599977493286, 2.143399953842163, 2.137500047683716, 2.137200117111206, 2.13100004196167, 2.1280999183654785, 2.1236000061035156, 2.1231000423431396, 2.1187000274658203, 2.1185998916625977, 2.117000102996826, 2.1157000064849854, 2.1142001152038574, 2.105600118637085, 2.1029000282287598, 2.102799892425537, 2.1026999950408936, 2.0996999740600586, 2.097399950027466, 2.0961999893188477, 2.091200113296509, 2.0885000228881836, 2.084399938583374, 2.0799999237060547, 2.0785000324249268, 2.0776000022888184, 2.077500104904175, 2.075200080871582, 2.071199893951416, 2.0710999965667725, 2.063499927520752, 2.058000087738037, 2.050100088119507, 2.0169999599456787, 1.9148999452590942, 1.6366000175476074, 1.8457000255584717, 1.8210999965667725, 1.5406999588012695, 1.812600016593933, 1.5371999740600586, 0.8389999866485596, 0.821399986743927, 0.6973000168800354, 1.1175999641418457, 0.8148000240325928, -0.9406999945640564, 2.3571999073028564, 2.3492000102996826, 2.341900110244751, 2.3417999744415283, 2.33489990234375, 2.3303000926971436, 2.3289999961853027, 2.3287999629974365, 2.328200101852417, 2.3257999420166016, 2.325700044631958, 2.324399948120117, 2.3238000869750977, 2.322999954223633, 2.3180999755859375, 2.3180999755859375, 2.3162999153137207, 2.314300060272217, 2.312000036239624, 2.3071999549865723, 2.3071999549865723, 2.301300048828125, 2.300600051879883, 2.2953999042510986, 2.2941999435424805, 2.2899999618530273, 2.285399913787842, 2.2852001190185547, 2.2846999168395996, 2.2822000980377197, 2.281399965286255, 2.282099962234497, 2.2701001167297363, 2.248699903488159, 2.2535998821258545, 1.9523999691009521, 1.967900037765503, 1.8725999593734741, 1.826200008392334, 1.8178000450134277, 1.5253000259399414, 1.2139999866485596, 1.9041999578475952, -0.37599998712539673, 1.269700050354004, 0.580299973487854, 1.4072999954223633, 2.6751999855041504, 2.653700113296509, 2.6417999267578125, 2.6414999961853027, 2.6364998817443848, 2.631500005722046, 2.6282999515533447, 2.6249001026153564, 2.6168999671936035, 2.6122000217437744, 2.6071999073028564, 2.601300001144409, 2.5985000133514404, 2.5947000980377197, 2.594399929046631, 2.5873000621795654, 2.5871999263763428, 2.5871999263763428, 2.5871999263763428, 2.5826001167297363, 2.5820000171661377, 2.5785999298095703, 2.578200101852417, 2.5690999031066895, 2.5683000087738037, 2.5683000087738037, 2.56820011138916, 2.56820011138916, 2.5673999786376953, 2.565500020980835, 2.5225000381469727, 2.542799949645996, 2.4061999320983887, 2.4426000118255615, 2.502500057220459, 2.3640999794006348, 2.521899938583374, 2.276900053024292, 2.1779000759124756, 1.9428999423980713, 2.1654999256134033, 1.5427000522613525, 1.7383999824523926, 2.1431000232696533, 1.5946999788284302, 0.8521000146865845, 0.29919999837875366, 1.82260000705719, 1.051300048828125, 0.4771000146865845, -0.4032000005245209, 0.13910000026226044, 0.48919999599456787, -0.1193000003695488, 3.062299966812134, 3.0548999309539795, 3.051300048828125, 3.047800064086914, 3.0476999282836914, 3.0450000762939453, 3.0332999229431152, 3.0304999351501465, 3.009200096130371, 3.00570011138916, 2.9881999492645264, 2.9825000762939453, 2.9762001037597656, 2.9762001037597656, 2.976099967956543, 2.976099967956543, 2.976099967956543, 2.976099967956543, 2.976099967956543, 2.9690001010894775, 2.9690001010894775, 2.9690001010894775, 2.9690001010894775, 2.968899965286255, 2.9688000679016113, 2.960700035095215, 2.960700035095215, 2.9512999057769775, 2.9511001110076904, 2.9511001110076904, 2.740299940109253, 2.7414000034332275, 2.7370998859405518, 2.7736001014709473, 0.901199996471405, 0.6409000158309937, 1.4573999643325806, 1.0194000005722046, 2.0422000885009766, 1.7569999694824219, 2.5209999084472656, 1.9215999841690063, -0.3066999912261963, 0.991100013256073, -1.2110999822616577], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -4.828000068664551, -4.3506999015808105, -5.001100063323975, -5.045199871063232, -4.768700122833252, -5.388199806213379, -5.502500057220459, -5.117599964141846, -5.555500030517578, -5.629799842834473, -5.364799976348877, -5.71150016784668, -5.507599830627441, -5.732900142669678, -5.823400020599365, -5.8470001220703125, -5.8719000816345215, -5.923399925231934, -6.0630998611450195, -6.125400066375732, -6.126800060272217, -6.157800197601318, -6.157800197601318, -6.191100120544434, -6.2270002365112305, -6.2270002365112305, -6.299799919128418, -6.299600124359131, -6.338699817657471, -6.338799953460693, -5.966300010681152, -3.8708999156951904, -3.9065001010894775, -5.589200019836426, -5.722799777984619, -4.0929999351501465, -4.776800155639648, -5.2307000160217285, -4.133200168609619, -5.317699909210205, -5.252999782562256, -4.6641998291015625, -4.983699798583984, -4.796599864959717, -5.589000225067139, -3.358599901199341, -3.4077000617980957, -5.399199962615967, -4.9517998695373535, -4.487800121307373, -3.947000026702881, -4.8867998123168945, -4.841700077056885, -5.005300045013428, -5.3445000648498535, -4.902100086212158, -5.1707000732421875, -5.319900035858154, -5.277500152587891, -3.7462000846862793, -4.500400066375732, -4.485199928283691, -4.295599937438965, -5.018199920654297, -4.906899929046631, -5.160900115966797, -5.118100166320801, -5.222700119018555, -5.806300163269043, -5.752299785614014, -5.835299968719482, -5.897600173950195, -3.7054998874664307, -5.929800033569336, -5.929800033569336, -5.964200019836426, -5.998899936676025, -5.875199794769287, -6.151199817657471, -6.115200042724609, -4.654200077056885, -6.1930999755859375, -6.193399906158447, -6.238699913024902, -6.331900119781494, -6.383999824523926, -6.436999797821045, -6.436800003051758, -6.436299800872803, -4.254300117492676, -5.256800174713135, -5.790599822998047, -4.52239990234375, -4.242300033569336, -5.195899963378906, -4.777299880981445, -4.5030999183654785, -4.281799793243408, -4.341800212860107, -3.9047000408172607, -4.817399978637695, -4.200500011444092, -4.21750020980835, -4.673900127410889, -4.236499786376953, -5.172399997711182, -4.458199977874756, -4.738500118255615, -4.735199928283691, -4.753399848937988, -3.801500082015991, -4.002200126647949, -4.43209981918335, -4.641300201416016, -4.93310022354126, -4.653800010681152, -4.804800033569336, -3.3206000328063965, -4.092299938201904, -4.151100158691406, -4.1880998611450195, -4.287199974060059, -4.612800121307373, -4.813799858093262, -4.861400127410889, -4.768899917602539, -4.9116997718811035, -4.879199981689453, -5.115799903869629, -5.362199783325195, -4.962500095367432, -5.4918999671936035, -5.111100196838379, -5.641900062561035, -5.670100212097168, -5.618100166320801, -5.992000102996826, -5.991799831390381, -5.992000102996826, -5.993000030517578, -5.993199825286865, -5.992800235748291, -5.9918999671936035, -5.993000030517578, -5.99399995803833, -6.071700096130371, -6.072000026702881, -5.069799900054932, -4.92710018157959, -5.051499843597412, -5.224100112915039, -4.493000030517578, -5.1905999183654785, -3.889899969100952, -4.520400047302246, -3.9428000450134277, -4.6529998779296875, -4.445700168609619, -4.77869987487793, -3.8057000637054443, -4.650599956512451, -4.751399993896484, -4.867599964141846, -4.77810001373291, -4.753499984741211, -4.593400001525879, -4.888700008392334, -4.954800128936768, -3.1809000968933105, -3.28439998626709, -3.977799892425537, -4.545100212097168, -4.980000019073486, -5.027599811553955, -5.24560022354126, -5.308899879455566, -5.353300094604492, -5.399799823760986, -5.474999904632568, -5.49970006942749, -5.525199890136719, -5.5528998374938965, -5.670300006866455, -5.67080020904541, -5.701300144195557, -5.7342000007629395, -5.73360013961792, -5.7677001953125, -5.767600059509277, -5.8024001121521, -5.876200199127197, -5.915500164031982, -5.95550012588501, -5.998300075531006, -6.042099952697754, -6.045199871063232, -6.090400218963623, -6.135499954223633, -5.217899799346924, -5.000500202178955, -5.216100215911865, -4.679699897766113, -3.82669997215271, -3.9453999996185303, -4.72629976272583, -5.025700092315674, -5.462500095367432, -5.361999988555908, -3.932499885559082, -4.080399990081787, -3.3592000007629395, -5.401299953460693, -4.606800079345703, -5.071499824523926, -4.748199939727783, -4.168499946594238, -5.3084001541137695, -4.979899883270264, -5.036200046539307, -4.775700092315674, -5.0177998542785645, -4.902400016784668, -5.186600208282471, -5.1717000007629395, -5.249599933624268, -3.5055999755859375, -4.222899913787842, -4.182600021362305, -4.78879976272583, -4.991600036621094, -5.326900005340576, -5.410900115966797, -5.4156999588012695, -5.53439998626709, -5.639999866485596, -5.639900207519531, -5.676599979400635, -4.298799991607666, -4.392600059509277, -5.889100074768066, -5.937399864196777, -5.936399936676025, -5.938199996948242, -4.237299919128418, -5.469600200653076, -6.041200160980225, -4.313300132751465, -6.101500034332275, -4.236299991607666, -6.0584001541137695, -5.519100189208984, -6.289100170135498, -6.29040002822876, -5.128200054168701, -6.362199783325195, -4.628300189971924, -4.730299949645996, -5.33489990234375, -5.3078999519348145, -5.348199844360352, -4.617700099945068, -4.5167999267578125, -5.1743998527526855, -5.1631999015808105, -4.4380998611450195, -5.270299911499023, -4.919300079345703, -4.513999938964844, -4.499899864196777, -4.720799922943115, -5.163599967956543, -5.1265997886657715, -5.274400234222412, -3.63070011138916, -3.4458000659942627, -4.844600200653076, -4.845099925994873, -5.081900119781494, -5.210899829864502, -5.239299774169922, -5.2133002281188965, -5.2677001953125, -5.328000068664551, -5.327499866485596, -5.360000133514404, -5.091899871826172, -5.3308000564575195, -5.496699810028076, -5.364799976348877, -5.53439998626709, -5.5725998878479, -5.614500045776367, -5.69950008392334, -5.700300216674805, -5.794300079345703, -5.79580020904541, -5.848199844360352, -5.8979997634887695, -5.9552001953125, -6.015900135040283, -6.014999866485596, -4.407800197601318, -4.4217000007629395, -4.919300079345703, -5.68209981918335, -5.295199871063232, -4.658699989318848, -4.87060022354126, -4.575799942016602, -4.708499908447266, -4.778299808502197, -5.047800064086914, -5.085599899291992, -4.995500087738037, -4.857500076293945, -5.402299880981445, -4.709799766540527, -5.251699924468994, -5.166999816894531, -5.2982001304626465, -3.770400047302246, -5.072000026702881, -5.110300064086914, -5.308199882507324, -5.388000011444092, -5.474299907684326, -5.519899845123291, -5.568299770355225, -5.672999858856201, -5.729700088500977, -5.789599895477295, -5.851900100708008, -5.639999866485596, -5.920400142669678, -5.9207000732421875, -5.993000030517578, -5.993899822235107, -5.99429988861084, -5.993899822235107, -5.160999774932861, -5.161499977111816, -6.072299957275391, -6.071700096130371, -6.008900165557861, -6.1579999923706055, -6.1579999923706055, -6.1579999923706055, -6.157599925994873, -6.157800197601318, -6.082799911499023, -4.845699787139893, -5.7042999267578125, -4.556399822235107, -5.405099868774414, -5.821100234985352, -5.2266998291015625, -5.928800106048584, -5.333099842071533, -5.268400192260742, -4.8420000076293945, -5.317200183868408, -4.435999870300293, -4.878200054168701, -5.428899765014648, -4.861800193786621, -4.2758002281188965, -4.0345001220703125, -5.347300052642822, -4.9303998947143555, -4.662099838256836, -4.542300224304199, -5.043499946594238, -5.209400177001953, -5.317299842834473, -3.89520001411438, -4.2322001457214355, -4.343100070953369, -4.446199893951416, -4.446300029754639, -4.513700008392334, -4.777299880981445, -4.8383002281188965, -5.1793999671936035, -5.224999904632568, -5.434599876403809, -5.494699954986572, -5.557600021362305, -5.557700157165527, -5.5584001541137695, -5.55810022354126, -5.558800220489502, -5.558499813079834, -5.558899879455566, -5.625699996948242, -5.626200199127197, -5.626200199127197, -5.626200199127197, -5.6265997886657715, -5.626699924468994, -5.6992998123168945, -5.6992998123168945, -5.777100086212158, -5.778299808502197, -5.778299808502197, -4.9492998123168945, -5.214799880981445, -5.303100109100342, -5.485300064086914, -4.517000198364258, -4.628900051116943, -5.071800231933594, -5.228000164031982, -5.480100154876709, -5.456099987030029, -5.56820011138916, -5.521900177001953, -5.446000099182129, -5.542200088500977, -5.5447998046875]}, \"token.table\": {\"Topic\": [4, 5, 8, 5, 1, 3, 5, 4, 6, 8, 3, 1, 4, 4, 6, 1, 4, 8, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 6, 7, 8, 6, 3, 4, 5, 6, 2, 5, 6, 1, 2, 3, 6, 1, 2, 3, 4, 5, 6, 7, 1, 1, 8, 4, 1, 7, 2, 1, 2, 3, 4, 5, 6, 7, 2, 6, 7, 3, 8, 8, 7, 5, 1, 3, 2, 1, 1, 3, 5, 6, 1, 2, 3, 5, 6, 7, 8, 4, 8, 8, 4, 2, 6, 3, 6, 6, 6, 5, 6, 4, 6, 5, 6, 7, 5, 6, 6, 3, 7, 4, 4, 3, 1, 6, 7, 3, 1, 2, 3, 4, 6, 7, 1, 6, 7, 3, 1, 6, 1, 5, 6, 2, 1, 2, 3, 4, 7, 8, 2, 7, 2, 3, 5, 7, 2, 8, 3, 6, 2, 7, 2, 5, 6, 7, 5, 5, 4, 2, 5, 3, 5, 8, 5, 1, 8, 7, 3, 3, 6, 8, 2, 4, 5, 7, 5, 6, 7, 1, 8, 7, 3, 7, 8, 2, 3, 2, 5, 6, 7, 7, 2, 3, 4, 1, 7, 5, 6, 1, 7, 8, 3, 6, 2, 5, 8, 1, 2, 3, 4, 5, 6, 7, 2, 3, 8, 6, 1, 4, 8, 1, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 2, 3, 5, 6, 7, 1, 2, 3, 4, 5, 6, 8, 8, 1, 2, 3, 4, 6, 7, 8, 8, 8, 1, 2, 3, 4, 5, 6, 7, 7, 6, 8, 1, 4, 2, 3, 4, 2, 7, 1, 7, 1, 8, 3, 5, 6, 5, 6, 2, 5, 8, 1, 4, 8, 4, 1, 2, 4, 6, 8, 2, 6, 1, 2, 3, 6, 7, 5, 6, 7, 5, 5, 6, 5, 3, 7, 8, 7, 8, 6, 6, 3, 5, 4, 1, 8, 1, 2, 3, 4, 5, 6, 7, 8, 2, 4, 6, 7, 3, 4, 5, 7, 3, 4, 3, 3, 2, 1, 6, 7, 2, 5, 8, 8, 1, 3, 5, 6, 7, 8, 2, 1, 4, 2, 3, 5, 8, 3, 1, 2, 3, 4, 5, 6, 2, 1, 7, 3, 5, 1, 2, 7, 1, 2, 3, 4, 5, 7, 1, 5, 7, 8, 1, 2, 3, 4, 5, 6, 8, 3, 3, 6, 4, 5, 6, 8, 4, 7, 6, 5, 4, 4, 1, 2, 3, 4, 6, 1, 4, 1, 3, 1, 2, 3, 7, 4, 6, 7, 4, 5, 1, 1, 3, 4, 6, 1, 2, 3, 6, 7, 4, 1, 3, 4, 1, 4, 4, 3, 7, 1, 2, 3, 6, 7, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 4, 1, 3, 5, 3, 2, 3, 5, 6, 7, 8, 3, 3, 6, 5, 2, 2, 2, 6, 1, 2, 5, 1, 1, 2, 3, 4, 5, 7, 8, 3, 5, 7, 5, 1, 6, 4, 1, 2, 6, 1, 3, 5, 6, 7, 8, 1, 2, 3, 4, 5, 7, 8, 4, 5, 2, 7, 2, 8, 7, 5, 6, 6, 7, 2, 1, 2, 3, 4, 5, 6, 8, 4, 6, 4, 4, 6, 7, 1, 6, 1, 5, 7, 1, 5, 5, 6, 2, 7, 1, 3, 6, 5, 3, 4, 8, 8, 7, 2, 2, 3, 4, 5, 6, 4, 1, 2, 4, 6, 7, 5, 7, 7, 1, 2, 6, 7, 8, 2, 4, 5, 6, 7, 2, 5, 1, 1, 8, 2, 4, 5, 6, 8, 6, 8, 6, 1, 2, 3, 4, 6, 1, 2, 6, 7, 2, 1, 7, 8, 5, 8, 7, 5, 6, 8, 4, 3, 3, 4, 5, 5, 1, 1, 2, 3, 4, 5, 6, 7, 8, 2, 1, 3, 4, 5, 8, 4, 5, 7, 7, 6, 7, 1, 2, 3, 5, 8, 2, 6, 7, 6, 7, 1, 4, 5, 7, 8, 1, 2, 3, 5, 6, 1, 2, 3, 4, 5, 8, 1, 4, 6, 8, 1, 1, 3, 5, 3, 1, 1, 7, 8, 4, 5, 2, 4, 7, 3, 3, 1, 3, 4, 7, 7, 1, 3, 4, 6, 7, 3, 8, 8, 1, 1, 4, 8, 4, 1, 2, 3, 7, 4, 7, 8, 1, 1, 2, 4, 6, 7, 3, 6, 1, 2, 4, 5, 7, 5, 4, 8, 1, 3, 4, 6, 7, 4, 3, 4, 7, 8, 1, 4, 1, 2, 3, 4, 6, 7, 8, 2, 3, 5, 7, 4, 2, 6, 7, 1, 4, 5, 7, 1, 2, 3, 4, 6, 7, 8, 2, 3, 8, 2], \"Freq\": [0.9411532282829285, 0.8857131600379944, 0.9519851803779602, 0.9585767984390259, 0.016123758628964424, 0.03224751725792885, 0.9513017535209656, 0.9649443626403809, 0.9602934718132019, 0.9533807635307312, 0.9620181322097778, 0.9020591378211975, 0.07731935381889343, 0.9130123853683472, 0.942270815372467, 0.26540201902389526, 0.6974518299102783, 0.0370328426361084, 0.8019957542419434, 0.11900582164525986, 0.041393328458070755, 0.005174166057258844, 0.005174166057258844, 0.025870831683278084, 0.32854780554771423, 0.23493921756744385, 0.07708942890167236, 0.1468370109796524, 0.04405110329389572, 0.0642411932349205, 0.08993766456842422, 0.012848238460719585, 0.9002169370651245, 0.846975564956665, 0.02228883095085621, 0.06686649471521378, 0.06686649471521378, 0.9532352089881897, 0.3957574963569641, 0.5819963216781616, 0.3193472921848297, 0.039918411523103714, 0.5688373446464539, 0.06985721737146378, 0.018992820754647255, 0.5792810320854187, 0.08546769618988037, 0.09496410191059113, 0.04748205095529556, 0.1804317981004715, 0.9605696797370911, 0.9412530660629272, 0.7189081311225891, 0.2614211440086365, 0.9766404628753662, 0.9786297082901001, 0.8256281614303589, 0.9546563029289246, 0.22733457386493683, 0.24449190497398376, 0.2487812340259552, 0.034314654767513275, 0.08578663319349289, 0.08149730414152145, 0.07720796763896942, 0.7792820930480957, 0.1981225609779358, 0.013208171352744102, 0.9731576442718506, 0.9442129731178284, 0.8578122854232788, 0.9034751653671265, 0.968285083770752, 0.9900515079498291, 0.008250429295003414, 0.91962730884552, 0.9666561484336853, 0.044715527445077896, 0.9390260577201843, 0.9235987663269043, 0.06692744791507721, 0.12514705955982208, 0.7821691036224365, 0.00782169122248888, 0.01564338244497776, 0.054751839488744736, 0.00782169122248888, 0.00782169122248888, 0.9679960012435913, 0.8578212857246399, 0.9110201597213745, 0.914932906627655, 0.941352128982544, 0.9640935063362122, 0.9929828643798828, 0.954284131526947, 0.9114289879798889, 0.9172519445419312, 0.9388315677642822, 0.054425016045570374, 0.9612288475036621, 0.9484664797782898, 0.7776877880096436, 0.19855858385562897, 0.016546547412872314, 0.3294494152069092, 0.6588988304138184, 0.9582940340042114, 0.5110419392585754, 0.46845510601997375, 0.9780520796775818, 0.9685730934143066, 0.9539034366607666, 0.6545080542564392, 0.3272540271282196, 0.8235003352165222, 0.961803138256073, 0.874060869216919, 0.06578952819108963, 0.0375940166413784, 0.0046992520801723, 0.0093985041603446, 0.0046992520801723, 0.08968546241521835, 0.8968546390533447, 0.8903036713600159, 0.9759824275970459, 0.5559691190719604, 0.42515283823013306, 0.9309014678001404, 0.9811834096908569, 0.9885594248771667, 0.9865067601203918, 0.49533796310424805, 0.00798932183533907, 0.01597864367067814, 0.4154447317123413, 0.00798932183533907, 0.05592525377869606, 0.5632993578910828, 0.40682730078697205, 0.26454541087150574, 0.6991557478904724, 0.009448050521314144, 0.028344150632619858, 0.9160324931144714, 0.8568722605705261, 0.9759600758552551, 0.9526406526565552, 0.8952329158782959, 0.09947032481431961, 0.9594074487686157, 0.005577950272709131, 0.011155900545418262, 0.022311801090836525, 0.9508417248725891, 0.9058586955070496, 0.9794375896453857, 0.9271358251571655, 0.9505901336669922, 0.9879201650619507, 0.9065044522285461, 0.07554204016923904, 0.9703790545463562, 0.9124053716659546, 0.06082702428102493, 0.9562469720840454, 0.9884834289550781, 0.973845362663269, 0.023752326145768166, 0.8574956059455872, 0.535788357257843, 0.24110476672649384, 0.1785961240530014, 0.03571922332048416, 0.8996435403823853, 0.03911493718624115, 0.03911493718624115, 0.9784968495368958, 0.9110206365585327, 0.8898512125015259, 0.9622365236282349, 0.9156801700592041, 0.8331927061080933, 0.9149516820907593, 0.9622913002967834, 0.9095187187194824, 0.043310414999723434, 0.9535102248191833, 0.9591779112815857, 0.8983591198921204, 0.9048301577568054, 0.07540251314640045, 0.9957093596458435, 0.29157474637031555, 0.6317452788352966, 0.9556835293769836, 0.044108472764492035, 0.952796220779419, 0.889348566532135, 0.85750412940979, 0.06454841792583466, 0.9036778807640076, 0.9273707270622253, 0.9061506390571594, 0.06472504138946533, 0.6433809399604797, 0.038992784917354584, 0.08773376047611237, 0.11697834730148315, 0.10723015666007996, 0.009748196229338646, 0.009748196229338646, 0.9687902331352234, 0.26311349868774414, 0.6840950846672058, 0.9646747708320618, 0.9879767894744873, 0.950686514377594, 0.950283408164978, 0.9488420486450195, 0.7726895213127136, 0.20157116651535034, 0.9336287975311279, 0.1350458562374115, 0.614240825176239, 0.04791949689388275, 0.08277003467082977, 0.04791949689388275, 0.00871263537555933, 0.060988448560237885, 0.2817091941833496, 0.17737245559692383, 0.1878061145544052, 0.31301021575927734, 0.031301021575927734, 0.06972436606884003, 0.5984674692153931, 0.17431090772151947, 0.07553473114967346, 0.05229327455163002, 0.023241454735398293, 0.005810363683849573, 0.9851392507553101, 0.22587735950946808, 0.29913488030433655, 0.3540780246257782, 0.08546710759401321, 0.018314380198717117, 0.0061047933995723724, 0.012209586799144745, 0.9793243408203125, 0.8922440409660339, 0.47991007566452026, 0.1047360748052597, 0.10629929602146149, 0.2813805043697357, 0.0015632249414920807, 0.018758699297904968, 0.007816124707460403, 0.9577792882919312, 0.2747989296913147, 0.6869973540306091, 0.9864569306373596, 0.0176153015345335, 0.8268231153488159, 0.1087925136089325, 0.05439625680446625, 0.9817269444465637, 0.010333968326449394, 0.015168366953730583, 0.9707754850387573, 0.9375306963920593, 0.053573183715343475, 0.7701008915901184, 0.13172778487205505, 0.10132906585931778, 0.9823288321495056, 0.013456558808684349, 0.9791078567504883, 0.9310129284858704, 0.9520063996315002, 0.28286606073379517, 0.6899172067642212, 0.020697515457868576, 0.984292209148407, 0.3465366065502167, 0.11239024996757507, 0.4214634299278259, 0.11239024996757507, 0.009365853853523731, 0.058242566883563995, 0.912466824054718, 0.019670099020004272, 0.47208237648010254, 0.019670099020004272, 0.3737318813800812, 0.09835049510002136, 0.046469882130622864, 0.9293976426124573, 0.9274761080741882, 0.9280683994293213, 0.8749788999557495, 0.11412768065929413, 0.9321355223655701, 0.18490654230117798, 0.8012616634368896, 0.8331933617591858, 0.18597348034381866, 0.7438939213752747, 0.9868018627166748, 0.9476754665374756, 0.9962469339370728, 0.961074709892273, 0.9861305356025696, 0.3923846185207367, 0.5493384599685669, 0.004927230067551136, 0.14288967847824097, 0.32026997208595276, 0.21679812669754028, 0.26114320755004883, 0.04434507340192795, 0.014781690202653408, 0.9253024458885193, 0.950844407081604, 0.08557424694299698, 0.3422969877719879, 0.5990197062492371, 0.9611496925354004, 0.12267003953456879, 0.5366814136505127, 0.33734259009361267, 0.9944959282875061, 0.9726892113685608, 0.9609373807907104, 0.9866653680801392, 0.9157567024230957, 0.9860450625419617, 0.9614038467407227, 0.841005802154541, 0.18195240199565887, 0.7018164396286011, 0.07797960191965103, 0.9868943691253662, 0.4229699373245239, 0.1741640865802765, 0.18660438060760498, 0.012440292164683342, 0.07464174926280975, 0.12440291792154312, 0.9670407176017761, 0.10059701651334763, 0.8802238702774048, 0.52985680103302, 0.16558024287223816, 0.18213827908039093, 0.11590617150068283, 0.9842230677604675, 0.5124632716178894, 0.007020044606178999, 0.19656124711036682, 0.014040089212357998, 0.1474209427833557, 0.11232071369886398, 0.9880422949790955, 0.8756750822067261, 0.12509644031524658, 0.8733265995979309, 0.1069379523396492, 0.9613083600997925, 0.04005451500415802, 0.9093097448348999, 0.11495169997215271, 0.11937292665243149, 0.2122185230255127, 0.4465431571006775, 0.07516072690486908, 0.026527315378189087, 0.7233779430389404, 0.16440407931804657, 0.016440408304333687, 0.09864244610071182, 0.18134339153766632, 0.16320905089378357, 0.3082837462425232, 0.09671647101640701, 0.11485081166028976, 0.10880602896213531, 0.024179117754101753, 0.979433000087738, 0.37249794602394104, 0.614621639251709, 0.133536234498024, 0.40060868859291077, 0.133536234498024, 0.35609662532806396, 0.1126430332660675, 0.7885012626647949, 0.9615278840065002, 0.9714879393577576, 0.9580479264259338, 0.9912421703338623, 0.08346541970968246, 0.0625990629196167, 0.38602757453918457, 0.3755944073200226, 0.09389860183000565, 0.8169368505477905, 0.17505788803100586, 0.9563571810722351, 0.9847609996795654, 0.7776589393615723, 0.01787721738219261, 0.17877216637134552, 0.026815826073288918, 0.9892945885658264, 0.2289295494556427, 0.7376618981361389, 0.24246852099895477, 0.7274055480957031, 0.9729331731796265, 0.01508864015340805, 0.5960012674331665, 0.21878528594970703, 0.16597504913806915, 0.5450129508972168, 0.3036500811576843, 0.02335769683122635, 0.0934307873249054, 0.03114359639585018, 0.9142504334449768, 0.8135771155357361, 0.0645696148276329, 0.10331138223409653, 0.985710084438324, 0.012477342039346695, 0.9387269616127014, 0.09920230507850647, 0.7936184406280518, 0.32896459102630615, 0.5140071511268616, 0.020560286939144135, 0.04112057387828827, 0.08224114775657654, 0.05741279199719429, 0.6219719052314758, 0.08611918985843658, 0.05741279199719429, 0.1339631825685501, 0.01913759857416153, 0.01913759857416153, 0.01539711095392704, 0.2540523111820221, 0.40032488107681274, 0.17706677317619324, 0.10777977108955383, 0.038492776453495026, 0.9958449602127075, 0.1014813706278801, 0.8727397918701172, 0.9547119140625, 0.9969319701194763, 0.5136928558349609, 0.1322377622127533, 0.2644755244255066, 0.050860680639743805, 0.0050860680639743805, 0.030516408383846283, 0.9609338641166687, 0.968162477016449, 0.020599201321601868, 0.9908241033554077, 0.991788923740387, 0.9057867527008057, 0.9587714672088623, 0.9408664703369141, 0.9530081152915955, 0.04799550771713257, 0.9119146466255188, 0.9412521123886108, 0.5115832090377808, 0.10231664776802063, 0.18189625442028046, 0.011368515901267529, 0.12505367398262024, 0.045474063605070114, 0.022737031802535057, 0.2806910276412964, 0.09356367588043213, 0.5613820552825928, 0.9720432758331299, 0.3056851625442505, 0.6686862707138062, 0.9510321617126465, 0.0653589740395546, 0.8496666550636292, 0.0653589740395546, 0.34189680218696594, 0.03598913922905922, 0.14395655691623688, 0.01799456961452961, 0.37788593769073486, 0.05398370698094368, 0.2834121584892273, 0.34091609716415405, 0.045181650668382645, 0.11911526322364807, 0.06571876257658005, 0.10679299384355545, 0.032859381288290024, 0.08594221621751785, 0.9023932814598083, 0.9825443625450134, 0.9523159861564636, 0.9787989854812622, 0.8763994574546814, 0.9573730826377869, 0.3587310314178467, 0.6457158923149109, 0.9788331389427185, 0.8903087973594666, 0.9360345602035522, 0.08142708986997604, 0.04885625094175339, 0.30942294001579285, 0.20628196001052856, 0.2279958426952362, 0.01085694506764412, 0.1139979213476181, 0.9214866757392883, 0.032910238951444626, 0.9484013915061951, 0.9789698719978333, 0.956835150718689, 0.8409678339958191, 0.9726421236991882, 0.9400612711906433, 0.9814046025276184, 0.05553562939167023, 0.8885700702667236, 0.8745229840278625, 0.12243321537971497, 0.906690776348114, 0.9646216034889221, 0.11447017639875412, 0.8394479751586914, 0.9598795771598816, 0.9782559871673584, 0.9863467216491699, 0.9600707292556763, 0.969113826751709, 0.8643396496772766, 0.11786449700593948, 0.9110202193260193, 0.9110371470451355, 0.9772881865501404, 0.9776219725608826, 0.07172179222106934, 0.5737743377685547, 0.21516536176204681, 0.12551313638687134, 0.9409437775611877, 0.9786307215690613, 0.9948934316635132, 0.95492023229599, 0.8721545338630676, 0.8248268365859985, 0.05553972348570824, 0.8886355757713318, 0.8955935835838318, 0.14408965408802032, 0.7204482555389404, 0.01801120676100254, 0.12607844173908234, 0.904043972492218, 0.009509427472949028, 0.04754713550209999, 0.5325279235839844, 0.0855848416686058, 0.3138110935688019, 0.05822736769914627, 0.9316378831863403, 0.9837633967399597, 0.9828119277954102, 0.9695388078689575, 0.3023059368133545, 0.3114667236804962, 0.2565020024776459, 0.10992942750453949, 0.009160785935819149, 0.8730963468551636, 0.8930219411849976, 0.9393219947814941, 0.8368551135063171, 0.021457822993397713, 0.10728911310434341, 0.032186735421419144, 0.010728911496698856, 0.014219551347196102, 0.952709972858429, 0.028439102694392204, 0.014219551347196102, 0.9942470788955688, 0.09533759951591492, 0.7150319814682007, 0.19067519903182983, 0.9583701491355896, 0.9113565683364868, 0.8903061747550964, 0.918819785118103, 0.0781974270939827, 0.8571841716766357, 0.9473180174827576, 0.9618276357650757, 0.023671673610806465, 0.8048369288444519, 0.1420300453901291, 0.9341952204704285, 0.9839123487472534, 0.7614054083824158, 0.020440414547920227, 0.08687175810337067, 0.005110103636980057, 0.010220207273960114, 0.08687175810337067, 0.025550518184900284, 0.010220207273960114, 0.9606348276138306, 0.9486226439476013, 0.9895943403244019, 0.3850131034851074, 0.5888435244560242, 0.022647829726338387, 0.28882578015327454, 0.08252165466547012, 0.5776515603065491, 0.897606372833252, 0.9272956252098083, 0.9556342363357544, 0.2512308955192566, 0.5675957202911377, 0.07443878799676895, 0.09304848313331604, 0.893020510673523, 0.007912867702543736, 0.9811955690383911, 0.007912867702543736, 0.8905525207519531, 0.0962759479880333, 0.8770240545272827, 0.036353327333927155, 0.04544166103005409, 0.031809162348508835, 0.004544165916740894, 0.026147183030843735, 0.7451947331428528, 0.09151513874530792, 0.143809512257576, 0.013073591515421867, 0.16415660083293915, 0.5114109516143799, 0.20835261046886444, 0.006313715595752001, 0.07576458156108856, 0.025254862383008003, 0.48673123121261597, 0.407169371843338, 0.014040323905646801, 0.08892204612493515, 0.9680729508399963, 0.9654210209846497, 0.28688499331474304, 0.6885239481925964, 0.9888430833816528, 0.9757289290428162, 0.5348861813545227, 0.12343527376651764, 0.3291607201099396, 0.006915057078003883, 0.9888531565666199, 0.21287338435649872, 0.6918385028839111, 0.07982752472162247, 0.9611499309539795, 0.9776515960693359, 0.7652167677879333, 0.21863335371017456, 0.009937879629433155, 0.009937879629433155, 0.8983628749847412, 0.19087979197502136, 0.5320266485214233, 0.08528672158718109, 0.03249017894268036, 0.158389613032341, 0.869621217250824, 0.09662457555532455, 0.9106601476669312, 0.9309055805206299, 0.9848302602767944, 0.2241467833518982, 0.6724403500556946, 0.9551012516021729, 0.21380428969860077, 0.6948639154434204, 0.022907601669430733, 0.06872280687093735, 0.865909218788147, 0.1237013190984726, 0.8568658232688904, 0.9785676002502441, 0.10779029875993729, 0.6970439553260803, 0.05030214041471481, 0.035930100828409195, 0.10779029875993729, 0.4075399339199066, 0.5753505229949951, 0.48897576332092285, 0.02573556639254093, 0.1286778301000595, 0.34743013978004456, 0.012867783196270466, 0.9332502484321594, 0.7914639711380005, 0.19786599278450012, 0.25771453976631165, 0.6343742609024048, 0.0396483913064003, 0.049560487270355225, 0.009912097826600075, 0.9645723700523376, 0.16758383810520172, 0.8044024109840393, 0.8270163536071777, 0.09189070761203766, 0.2027544379234314, 0.7772253155708313, 0.48799753189086914, 0.2356892079114914, 0.2266242355108261, 0.0015108282677829266, 0.0015108282677829266, 0.04532484710216522, 0.0015108282677829266, 0.7343453764915466, 0.02861085906624794, 0.02861085906624794, 0.19073906540870667, 0.9131887555122375, 0.07854645699262619, 0.9229208827018738, 0.8987435698509216, 0.8656538724899292, 0.07694701105356216, 0.01923675276339054, 0.01923675276339054, 0.05249305069446564, 0.6124189496040344, 0.22746989130973816, 0.06999073177576065, 0.005832561291754246, 0.029162805527448654, 0.9113811254501343, 0.3229188621044159, 0.6651164889335632, 0.004819684661924839, 0.9491501450538635], \"Term\": [\"abc\", \"abuse\", \"action\", \"address\", \"administration\", \"administration\", \"administration\", \"afternoon\", \"agree\", \"ahead\", \"alltime\", \"always\", \"always\", \"amazing\", \"amazon\", \"america\", \"america\", \"america\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"amp\", \"amp\", \"amp\", \"amp\", \"amp\", \"amp\", \"amp\", \"amp\", \"andrew\", \"another\", \"another\", \"another\", \"another\", \"approve\", \"ask\", \"ask\", \"back\", \"back\", \"back\", \"back\", \"bad\", \"bad\", \"bad\", \"bad\", \"bad\", \"bad\", \"bail\", \"ban\", \"beautiful\", \"beautiful\", \"bedminster\", \"behalf\", \"bernie\", \"better\", \"big\", \"big\", \"big\", \"big\", \"big\", \"big\", \"big\", \"bill\", \"bill\", \"bill\", \"billion\", \"bless\", \"blumenthal\", \"bob\", \"book\", \"border\", \"border\", \"budget\", \"build\", \"business\", \"business\", \"campaign\", \"campaign\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"canada\", \"caucus\", \"ceremony\", \"champion\", \"chance\", \"charge\", \"china\", \"chuck\", \"clapper\", \"classified\", \"clinton\", \"clinton\", \"cnn\", \"collude\", \"collusion\", \"collusion\", \"collusion\", \"comey\", \"comey\", \"committee\", \"company\", \"company\", \"condolence\", \"conference\", \"confidence\", \"congress\", \"congress\", \"corker\", \"council\", \"country\", \"country\", \"country\", \"country\", \"country\", \"country\", \"court\", \"court\", \"crash\", \"create\", \"crime\", \"crime\", \"criminal\", \"crooked\", \"cut\", \"daca\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"dead\", \"dead\", \"deal\", \"deal\", \"deal\", \"deal\", \"deductible\", \"defeat\", \"deficit\", \"deliver\", \"dem\", \"dem\", \"democrat\", \"democrat\", \"democrat\", \"democrat\", \"director\", \"disgrace\", \"dishonest\", \"disrespect\", \"dnc\", \"dollar\", \"donald\", \"donald\", \"dossier\", \"drug\", \"drug\", \"easy\", \"economic\", \"economy\", \"economy\", \"egypt\", \"election\", \"election\", \"election\", \"election\", \"email\", \"email\", \"email\", \"enforcement\", \"enough\", \"ensure\", \"enthusiasm\", \"everybody\", \"evil\", \"except\", \"exciting\", \"excuse\", \"excuse\", \"executive\", \"expect\", \"expensive\", \"fail\", \"fail\", \"fake\", \"fast\", \"fast\", \"fbi\", \"fbi\", \"federal\", \"fema\", \"field\", \"find\", \"find\", \"fine\", \"fire\", \"fire\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"fix\", \"florida\", \"florida\", \"focus\", \"forward\", \"france\", \"freedom\", \"gang\", \"general\", \"general\", \"germany\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"give\", \"give\", \"give\", \"give\", \"give\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"god\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"government\", \"graham\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"h\", \"hand\", \"hand\", \"happy\", \"happy\", \"hard\", \"hard\", \"hard\", \"healthcare\", \"healthcare\", \"help\", \"help\", \"hero\", \"hero\", \"high\", \"high\", \"high\", \"hillary\", \"hillary\", \"historic\", \"hoax\", \"home\", \"honor\", \"honor\", \"honor\", \"host\", \"house\", \"house\", \"house\", \"house\", \"house\", \"hunt\", \"hunt\", \"include\", \"include\", \"include\", \"include\", \"include\", \"information\", \"information\", \"insurance\", \"investigate\", \"investigation\", \"investigation\", \"involve\", \"isis\", \"isis\", \"ivanka\", \"j\", \"j\", \"jame\", \"jeff\", \"job\", \"john\", \"join\", \"judge\", \"judge\", \"just\", \"just\", \"just\", \"just\", \"just\", \"just\", \"just\", \"k\", \"karen\", \"kill\", \"kill\", \"kill\", \"kim\", \"know\", \"know\", \"know\", \"korea\", \"land\", \"large\", \"last\", \"laugh\", \"law\", \"leak\", \"less\", \"lie\", \"lie\", \"lie\", \"life\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"line\", \"live\", \"live\", \"long\", \"long\", \"long\", \"long\", \"longer\", \"look\", \"look\", \"look\", \"look\", \"look\", \"look\", \"lose\", \"love\", \"love\", \"low\", \"low\", \"luther\", \"luther\", \"majority\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"man\", \"man\", \"man\", \"man\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"market\", \"massive\", \"massive\", \"may\", \"may\", \"may\", \"may\", \"mayor\", \"mayor\", \"mccabe\", \"meddling\", \"media\", \"medium\", \"meeting\", \"meeting\", \"meeting\", \"meeting\", \"meeting\", \"melania\", \"melania\", \"member\", \"mexico\", \"military\", \"military\", \"military\", \"military\", \"minister\", \"money\", \"money\", \"month\", \"month\", \"morning\", \"much\", \"much\", \"much\", \"much\", \"must\", \"must\", \"must\", \"must\", \"must\", \"name\", \"nation\", \"nation\", \"nation\", \"national\", \"national\", \"nbc\", \"necessary\", \"necessary\", \"need\", \"need\", \"need\", \"need\", \"need\", \"never\", \"never\", \"never\", \"never\", \"never\", \"never\", \"never\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"news\", \"night\", \"night\", \"nobody\", \"north\", \"now\", \"now\", \"now\", \"now\", \"now\", \"now\", \"nuclear\", \"number\", \"number\", \"obama\", \"obamacare\", \"obstruct\", \"obstructionist\", \"office\", \"officer\", \"official\", \"official\", \"ohio\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"open\", \"open\", \"open\", \"optimism\", \"order\", \"order\", \"paris\", \"pass\", \"pass\", \"pass\", \"pay\", \"pay\", \"pay\", \"pay\", \"pay\", \"pay\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"phony\", \"phony\", \"plan\", \"play\", \"player\", \"playing\", \"podesta\", \"political\", \"political\", \"post\", \"pray\", \"premium\", \"president\", \"president\", \"president\", \"president\", \"president\", \"president\", \"president\", \"presidential\", \"presidential\", \"press\", \"prime\", \"process\", \"product\", \"protect\", \"protection\", \"proud\", \"puerto\", \"puerto\", \"put\", \"put\", \"question\", \"raise\", \"reason\", \"reason\", \"rebuild\", \"record\", \"reform\", \"refuse\", \"relationship\", \"remark\", \"remark\", \"remembrance\", \"rep\", \"repeal\", \"replace\", \"report\", \"report\", \"report\", \"report\", \"reporting\", \"represent\", \"republican\", \"return\", \"review\", \"rick\", \"rico\", \"rico\", \"rid\", \"right\", \"right\", \"right\", \"right\", \"room\", \"russia\", \"russia\", \"russia\", \"russia\", \"russia\", \"russian\", \"russian\", \"safe\", \"sanctuary\", \"save\", \"say\", \"say\", \"say\", \"say\", \"say\", \"sc\", \"schedule\", \"schumer\", \"security\", \"security\", \"security\", \"security\", \"security\", \"senate\", \"senate\", \"senate\", \"senate\", \"senator\", \"serve\", \"serve\", \"serve\", \"server\", \"seven\", \"shape\", \"show\", \"show\", \"shut\", \"social\", \"solve\", \"speak\", \"speak\", \"speak\", \"spy\", \"stand\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"stay\", \"steel\", \"stock\", \"story\", \"story\", \"story\", \"success\", \"success\", \"success\", \"supporter\", \"supreme\", \"syria\", \"take\", \"take\", \"take\", \"take\", \"tape\", \"tax\", \"tax\", \"tax\", \"taxis\", \"taxis\", \"thank\", \"thank\", \"thank\", \"thank\", \"thank\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"today\", \"today\", \"today\", \"today\", \"tomorrow\", \"tonight\", \"total\", \"total\", \"trade\", \"travel\", \"truly\", \"truly\", \"truly\", \"trump\", \"trump\", \"two\", \"two\", \"two\", \"un\", \"unemployment\", \"united\", \"united\", \"united\", \"united\", \"uranium\", \"us\", \"us\", \"us\", \"us\", \"us\", \"usa\", \"usa\", \"value\", \"vet\", \"veteran\", \"vietnam\", \"vietnam\", \"virginia\", \"vote\", \"vote\", \"vote\", \"vote\", \"w\", \"w\", \"wait\", \"wall\", \"want\", \"want\", \"want\", \"want\", \"want\", \"washington\", \"washington\", \"watch\", \"watch\", \"watch\", \"watch\", \"watch\", \"weekly\", \"welcome\", \"welcome\", \"well\", \"well\", \"well\", \"well\", \"well\", \"west\", \"wh\", \"wh\", \"whatever\", \"whatever\", \"white\", \"white\", \"will\", \"will\", \"will\", \"will\", \"will\", \"will\", \"will\", \"win\", \"win\", \"win\", \"win\", \"wish\", \"witch\", \"witch\", \"women\", \"wonderful\", \"wonderful\", \"wonderful\", \"wonderful\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"worship\", \"year\", \"year\", \"year\", \"york\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [3, 6, 1, 7, 2, 4, 5, 8]};\n",
              "\n",
              "function LDAvis_load_lib(url, callback){\n",
              "  var s = document.createElement('script');\n",
              "  s.src = url;\n",
              "  s.async = true;\n",
              "  s.onreadystatechange = s.onload = callback;\n",
              "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
              "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "}\n",
              "\n",
              "if(typeof(LDAvis) !== \"undefined\"){\n",
              "   // already loaded: just create the visualization\n",
              "   !function(LDAvis){\n",
              "       new LDAvis(\"#\" + \"ldavis_el59041402016301290322506053465\", ldavis_el59041402016301290322506053465_data);\n",
              "   }(LDAvis);\n",
              "}else if(typeof define === \"function\" && define.amd){\n",
              "   // require.js is available: use it to load d3/LDAvis\n",
              "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
              "   require([\"d3\"], function(d3){\n",
              "      window.d3 = d3;\n",
              "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
              "        new LDAvis(\"#\" + \"ldavis_el59041402016301290322506053465\", ldavis_el59041402016301290322506053465_data);\n",
              "      });\n",
              "    });\n",
              "}else{\n",
              "    // require.js not available: dynamically load d3 & LDAvis\n",
              "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
              "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
              "                 new LDAvis(\"#\" + \"ldavis_el59041402016301290322506053465\", ldavis_el59041402016301290322506053465_data);\n",
              "            })\n",
              "         });\n",
              "}\n",
              "</script>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "waVB4co23o0R"
      },
      "source": [
        "#####<pre>      ...y arriesgándome de nuevo diría que probablemente TRUMP:\n",
        "~~~\n",
        "          > En el Topic 1 habla de leyes para la seguridad (veo la palabra 'wall' y recuerdo que quería erigir un muro para proteger la\n",
        "            frontera con México)\n",
        "          > En el Topic 2 parece hablar del gran coste del programa de salud 'ObamaCare'\n",
        "          > En el Topic 3 habla de la polémica con los mercados orientales (China y Corea)\n",
        "          > En el Topic 4 hablá de 'fake news' de la CNN sobre las elecciones\n",
        "          > En el Topic 5 parece tratar de la campaña de desprestigio en las elecciones contra Hillary Clinton, de las investigaciones del\n",
        "            posible espionaje ruso\n",
        "          > En el Topic 6 habla de temas de impuestos varios y reformas\n",
        "          > En el Topic 7 parece hablar de cosas que no le gustan: paises como Rusia, Siria, Puerto Rico, la organización terrorista Isis\n",
        "          > En el Topic 8 parece hablar sobre alguna acción del gobierno en Florida, aunque también aparece 'Vietnam'\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mPU1QmlO3rnh"
      },
      "source": [
        "># **EJERCICIO 3**\n",
        "###<pre>      ***Tweet Generation***\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cppYsQrSYwCw",
        "colab_type": "text"
      },
      "source": [
        "#####<pre>      Utilizaremos el mismo dataset del Ejercicio 2 y en este caso no lo vamos a preprocesar\n",
        "~~~\n",
        "          Queremos tener los tweets tal cual se escriben para intentar simularlos, por tanto no vamos a aplicar tratamientos de limpieza,\n",
        "          lemmatización, ni tampoco filtrar stop-words \n",
        "~~~"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "bd5b2d3a-4523-4bc1-cd5d-ac5398407106",
        "id": "yArdkAFkP0Ni",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "# transponemos el dataset\n",
        "df3 = df2.T\n",
        "\n",
        "# eliminamos los tweets duplicados\n",
        "df3 = df3.drop_duplicates()\n",
        "\n",
        "# reindexamos el dataset para que no nos falten claves (por la eliminación de duplicaciones)\n",
        "df3 = df3.reset_index()\n",
        "\n",
        "# eliminamos la columna index antigua (que se queda residente tras reindexar)\n",
        "df3.drop(['index'], axis = 1, inplace = True)\n",
        "\n",
        "df3"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>“Low plastic stool, cheap but delicious noodle...</td>\n",
              "      <td>OBAMA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>This National Gun Violence Awareness Day, show...</td>\n",
              "      <td>OBAMA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>We can never truly repay the debt we owe our f...</td>\n",
              "      <td>OBAMA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>This Center is for the leaders of tomorrow who...</td>\n",
              "      <td>OBAMA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Happy Mother’s Day to every mom out there, esp...</td>\n",
              "      <td>OBAMA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5718</th>\n",
              "      <td>...vast sums of money to NATO &amp;amp; the United...</td>\n",
              "      <td>TRUMP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5719</th>\n",
              "      <td>Despite what you have heard from the FAKE NEWS...</td>\n",
              "      <td>TRUMP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5720</th>\n",
              "      <td>Great meeting with the @RepublicanStudy Commit...</td>\n",
              "      <td>TRUMP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5721</th>\n",
              "      <td>\"The President Changed. So Has Small Businesse...</td>\n",
              "      <td>TRUMP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5722</th>\n",
              "      <td>North Korea is behaving very badly. They have ...</td>\n",
              "      <td>TRUMP</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5723 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  tweet  label\n",
              "0     “Low plastic stool, cheap but delicious noodle...  OBAMA\n",
              "1     This National Gun Violence Awareness Day, show...  OBAMA\n",
              "2     We can never truly repay the debt we owe our f...  OBAMA\n",
              "3     This Center is for the leaders of tomorrow who...  OBAMA\n",
              "4     Happy Mother’s Day to every mom out there, esp...  OBAMA\n",
              "...                                                 ...    ...\n",
              "5718  ...vast sums of money to NATO &amp; the United...  TRUMP\n",
              "5719  Despite what you have heard from the FAKE NEWS...  TRUMP\n",
              "5720  Great meeting with the @RepublicanStudy Commit...  TRUMP\n",
              "5721  \"The President Changed. So Has Small Businesse...  TRUMP\n",
              "5722  North Korea is behaving very badly. They have ...  TRUMP\n",
              "\n",
              "[5723 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aFPgYaoVRKDx"
      },
      "source": [
        "#####<pre>      Por su marcado carisma, gran desparpajo, enorme soberbia y demostrada incapacidad para gobernar el mundo...\n",
        "#####<pre>      ...elegimos simular los tweets del **Pato Donald**... mil perdones... quería decir de **Donald Trump**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48OpDgwnjHtZ",
        "colab_type": "code",
        "outputId": "1a219afe-9c26-4b1b-ea37-55e6a8c329b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "# seleccionamos los tweets de Trump\n",
        "df3 = df3.drop(df3[(df3['label'] != 'TRUMP')].index)\n",
        "\n",
        "# eliminamos la columna label\n",
        "df3.drop(['label'], axis = 1, inplace = True)\n",
        "\n",
        "df3"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2861</th>\n",
              "      <td>My thoughts and prayers are with the families ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2862</th>\n",
              "      <td>I am heading for Canada and the G-7 for talks ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2863</th>\n",
              "      <td>Congratulations to the Washington Capitals on ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2864</th>\n",
              "      <td>Looking forward to straightening out unfair Tr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2865</th>\n",
              "      <td>Canada charges the U.S. a 270%  tariff on Dair...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5718</th>\n",
              "      <td>...vast sums of money to NATO &amp;amp; the United...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5719</th>\n",
              "      <td>Despite what you have heard from the FAKE NEWS...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5720</th>\n",
              "      <td>Great meeting with the @RepublicanStudy Commit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5721</th>\n",
              "      <td>\"The President Changed. So Has Small Businesse...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5722</th>\n",
              "      <td>North Korea is behaving very badly. They have ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2862 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  tweet\n",
              "2861  My thoughts and prayers are with the families ...\n",
              "2862  I am heading for Canada and the G-7 for talks ...\n",
              "2863  Congratulations to the Washington Capitals on ...\n",
              "2864  Looking forward to straightening out unfair Tr...\n",
              "2865  Canada charges the U.S. a 270%  tariff on Dair...\n",
              "...                                                 ...\n",
              "5718  ...vast sums of money to NATO &amp; the United...\n",
              "5719  Despite what you have heard from the FAKE NEWS...\n",
              "5720  Great meeting with the @RepublicanStudy Commit...\n",
              "5721  \"The President Changed. So Has Small Businesse...\n",
              "5722  North Korea is behaving very badly. They have ...\n",
              "\n",
              "[2862 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jqBN3pB4ZU8L"
      },
      "source": [
        ">># **Pipeline**\n",
        "\n",
        "#####<pre>        Nos enfrentamos a un problema de Generación de Texto 'Language Modeling'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "0b3c85b1-7c04-483e-b35d-fe22e90ff28f",
        "id": "13pFmBwmRdeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vocab = list()\n",
        "\n",
        "for index, row in df3.iterrows():\n",
        "  for word in row['tweet'].strip().split(' '):\n",
        "    if word not in vocab:\n",
        "      vocab.append(word)\n",
        "\n",
        "print(f\"Número de Palabras: {len(vocab)}\")"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Número de Palabras: 14647\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AnBv5IJNRdef"
      },
      "source": [
        "#####<pre>      La columna -tweet- contiene 14.647 tokens diferentes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "w3L5Yt4QRqjV"
      },
      "source": [
        ">># **Preprocesamiento**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ot0uB3YNRqjW"
      },
      "source": [
        "#####<pre>      El preprocesamiento no será de limpieza, generaremos un dataset para entrenar la arquitectura del modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9q9YdQ5YCkv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3c0da82a-8fc1-4a7f-8e46-2f38208cf053"
      },
      "source": [
        "# generamos el corpus de documentos desde el que obtendremos el dataset de entrenamiento\n",
        "corpus_trump = list()\n",
        "\n",
        "for index, fila in df3.iterrows():\n",
        "  tweet = fila['tweet']\n",
        "  corpus_trump.append(tweet)\n",
        "\n",
        "print(f\"Del presidente Trump tenemos {len(corpus_trump)} tweets con {len(vocab)} palabras diferentes\")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Del presidente Trump tenemos 2862 tweets con 14647 palabras diferentes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujaMbcAlbFBb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tokenizamos los documentos del corpus por unigrams carácter ya que nuestro modelo irá prediciendo carácter a carácter\n",
        "tokenizado = [list(x) for x in corpus_trump]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6Y0cpL9dGhF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generamos una lista con los 5 primeros caracteres de cada documento del corpus\n",
        "# esta lista nos servirá para darle el pie de entrada para predecir al modelo\n",
        "maxlen = 5\n",
        "\n",
        "init_chars = [x[:maxlen] for x in tokenizado]\n",
        "\n",
        "for i in range(len(init_chars)):\n",
        "  aux = init_chars[i]\n",
        "  aux.insert(0, '<SOS>')\n",
        "  init_chars[i] = aux[:maxlen]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDuL5mfKdGen",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "05c3cb2f-6b88-4ede-efed-aee9fcb39329"
      },
      "source": [
        "# generamos una lista con todos los unigrams del corpus\n",
        "total_tokens = [t for s in corpus_trump for t in s]\n",
        "\n",
        "print(f\"El vocabulario total contiene {len(total_tokens)} unigrams/caracteres\")\n",
        "\n",
        "# agrupamos y contamos los unigrams\n",
        "vocab_agrupado = Counter(total_tokens)\n",
        "\n",
        "# generamos el vocabulario definitivo de unigrams quedándonos con aquellos que al menos estén 3 veces en el corpus\n",
        "# añadimos al vocabulario los tokens de principio y final de documento y el token desconocido: <SOS>, <EOS> y <UNK>\n",
        "vocab = [w for w, v in vocab_agrupado.items() if v > 2]\n",
        "vocab = ['<UNK>', '<SOS>', '<EOS>'] + vocab\n",
        "\n",
        "print(f\"El vocabulario definitivo contiene {len(vocab)} unigrams/caracteres únicos\")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "El vocabulario total contiene 462564 unigrams/caracteres\n",
            "El vocabulario definitivo contiene 109 unigrams/caracteres únicos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bm_DXZ2dGbe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# indexamos el vocabulario y realizamos su inversa\n",
        "c2id = {k: i for i, k in enumerate(vocab)}\n",
        "id2c = {i: k for k, i in c2id.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "102YbnW6dGYt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generamos el dataset de entrenamiento como un array de duplas en las que el primer elemento es una lista secuencial de cinco unigrams y el segundo el unigram que haría el sexto en la secuencia\n",
        "train = list()\n",
        "\n",
        "for t in tokenizado:\n",
        "  # a cada documento le insertamos los tokens <SOS> y <EOS>\n",
        "  t.insert(0, '<SOS>')\n",
        "  t.append('<EOS>')\n",
        "\n",
        "  # Vamos extrayendo token a token, las duplas con la slice de 5 tokens y el sexto \n",
        "  for i in range(0, len(t) - maxlen):\n",
        "    train.append((t[i:i + maxlen], t[i + maxlen]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7OKvQYMdGVf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "6225d4f3-5a81-4c71-9321-1bd4cf295e7b"
      },
      "source": [
        "for i in range(0, 5):\n",
        "  print(train[i])"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(['<SOS>', 'M', 'y', ' ', 't'], 'h')\n",
            "(['M', 'y', ' ', 't', 'h'], 'o')\n",
            "(['y', ' ', 't', 'h', 'o'], 'u')\n",
            "([' ', 't', 'h', 'o', 'u'], 'g')\n",
            "(['t', 'h', 'o', 'u', 'g'], 'h')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "p0S-fD9DrxuS"
      },
      "source": [
        ">># **Implementación del Language Modeling con *LTSM***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZJD6aggnhmI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Las siguientes funciones son callbacks que nos van a servir para ir viendo el progreso del entrenamiento del modelo\n",
        "# Están fielmente copiadas del visto durante las clases [RNNLM.ipynb]\n",
        "\n",
        "SAMPLE_EVERY = 5      # Veremos un ejemplo de predicción cada 5 épocas\n",
        "\n",
        "# helper function to sample an index from a probability array\n",
        "def sample_pred(preds, temperature = 1.0):\n",
        "  preds = np.asarray(preds).astype('float64')\n",
        "  preds = np.log(preds) / temperature\n",
        "  exp_preds = np.exp(preds)\n",
        "  preds = exp_preds / np.sum(exp_preds)\n",
        "  probas = np.random.multinomial(1, preds, 1)\n",
        "\n",
        "  return np.argmax(probas)\n",
        "\n",
        "class Sampletest(Callback):\n",
        "  def on_epoch_end(self, epoch, logs):\n",
        "    if epoch % SAMPLE_EVERY == 0  and epoch > 0:\n",
        "      data_test = []\n",
        "      nb_samples = 1\n",
        "      params = {'maxlen': maxlen, 'vocab': len(vocab), 'use_embeddings': True}\n",
        "\n",
        "      for _ in range(nb_samples):\n",
        "        data_test = choice(init_chars)\n",
        "        for diversity in [0.2, 0.6, 1.2]:\n",
        "          print('-------------- diversity:', diversity)\n",
        "          sentence = copy(data_test)\n",
        "          generated = copy(data_test)\n",
        "\n",
        "          for i in range(len(data_test), 400):\n",
        "            x_pred = np.zeros((1, params['maxlen']))\n",
        "\n",
        "            for t, char in enumerate(sentence):\n",
        "              x_pred[0, t] = c2id[char] if char in c2id else c2id['<UNK>']\n",
        "\n",
        "            preds = self.model.predict(x_pred, verbose = 0)[0]\n",
        "            next_index = sample_pred(preds, diversity)\n",
        "            next_char = id2c[next_index]\n",
        "\n",
        "            if next_char == '<EOS>':\n",
        "              break\n",
        "\n",
        "            generated += [next_char]\n",
        "            sentence = sentence[1:] \n",
        "            sentence += [next_char]\n",
        "\n",
        "          print(''.join(generated))\n",
        "\n",
        "class HistoryDisplay(Callback):    \n",
        "  def on_train_begin(self, logs = {}):\n",
        "    self.losses = []\n",
        "    self.accs = []\n",
        "    self.epochs = []\n",
        "    self.fig, self.ax = plt.subplots()\n",
        "    plt.show()\n",
        "    \n",
        "    plt.ion()\n",
        "    self.fig.show()\n",
        "    self.fig.canvas.draw()\n",
        "  \n",
        "  def on_epoch_end(self, epoch, logs):\n",
        "    self.epochs.append(epoch)\n",
        "    self.losses.append(logs['loss'])\n",
        "    self.accs.append(logs['acc'])\n",
        "\n",
        "    if epoch % PLOT_EVERY == 0:        \n",
        "      self.ax.clear()\n",
        "      self.ax.plot(self.epochs, self.accs, 'g', label = 'acc')\n",
        "      self.ax.plot(self.epochs, self.losses, 'b', label = 'loss')\n",
        "      legend = self.ax.legend(loc = 'upper right', shadow=True, fontsize = 'x-large')\n",
        "      display.clear_output(wait = True)\n",
        "      display.display(pl.gcf())\n",
        "      self.fig.canvas.draw()\n",
        "      plt.draw()\n",
        "\n",
        "class TimeHistory(Callback):\n",
        "  def on_train_begin(self, logs = {}):\n",
        "    self.times = []\n",
        "\n",
        "  def on_epoch_begin(self, batch, logs = {}):\n",
        "    self.epoch_time_start = time.time()\n",
        "\n",
        "  def on_epoch_end(self, batch, logs = {}):\n",
        "    self.times.append(time.time() - self.epoch_time_start)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0CMydJ_rgOO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Montamos la arquitectura del modelo como una clase con sus metodos de compilación, entrenamiento y predicción\n",
        "class LM:\n",
        "  def __init__(self, **kwargs):\n",
        "    self.params = kwargs.pop('params', None)\n",
        "\n",
        "  # método de definición del modelo y compilación    \n",
        "  def compile_model(self, params = {}):\n",
        "    # capa de entrada de 5 features\n",
        "    lm_input = Input(shape = (params['maxlen'],))\n",
        "\n",
        "    # capa de embeddings\n",
        "    embedding = Embedding(params['vocab'], params['emb_feats'])(lm_input)\n",
        "    \n",
        "    # capa LSTM bidireccional para facilitar el aprendizaje de las secuencias \n",
        "    lstm = CuDNNLSTM(params['rnn_hidden'], return_sequences = True)\n",
        "    lmlstm = Bidirectional(lstm)(embedding)\n",
        "    \n",
        "    # segunda capa LSTM, unidireccional, para stackear\n",
        "    stacklstm = CuDNNLSTM(params['rnn_hidden'], return_sequences=False, name='stack')\n",
        "    stackedlstm = stacklstm(lmlstm)\n",
        "\n",
        "    # capa densa de salida, con activación softmax \n",
        "    out = Dense(params['vocab'], activation='softmax')(stackedlstm)\n",
        "\n",
        "    model = Model(lm_input, out)\n",
        "\n",
        "    model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    return model\n",
        "\n",
        "  # método para el entrenamiento del modelo\n",
        "  def train(self, model, data, params={}):\n",
        "    callbacks = self._get_callbacks()\n",
        "\n",
        "    if 'shuffle' in params and params['shuffle']:\n",
        "      shuffle(data)\n",
        "\n",
        "    sentences, next_chars = zip(*data)\n",
        "    x = np.zeros((len(data), params['maxlen']))\n",
        "    y = np.zeros((len(data), params['vocab']))\n",
        "\n",
        "    for i, sentence in enumerate(sentences):\n",
        "      for t, char in enumerate(sentence):\n",
        "        x[i, t] = c2id[char] if char in c2id else c2id['<UNK>']\n",
        "          \n",
        "      y[i, c2id[next_chars[i]] if next_chars[i] in c2id else c2id['<UNK>']] = 1\n",
        "\n",
        "    model.fit(x, y, batch_size = params['batch_size'], epochs = params['epochs'], callbacks = callbacks, verbose = 1)\n",
        "\n",
        "  # método para predicción del modelo\n",
        "  def predict(self, model, data, params = {}):\n",
        "    for diversity in [0.2, 0.6, 1.2]:\n",
        "      print('-------------- diversity:', diversity)\n",
        "      sentence = copy(data)\n",
        "      generated = copy(data)\n",
        "\n",
        "      for i in range(len(data), 240):\n",
        "        x_pred = np.zeros((1, params['maxlen']))\n",
        "\n",
        "        for t, char in enumerate(sentence):\n",
        "          x_pred[0, t] = c2id[char] if char in c2id else c2id['<UNK>']\n",
        "        \n",
        "        preds = self.model.predict(x_pred, verbose = 0)[0]\n",
        "        next_index = sample_pred(preds, diversity)\n",
        "        next_char = id2c[next_index]\n",
        "\n",
        "        if next_char == '<EOS>':\n",
        "          break\n",
        "        \n",
        "        generated += [next_char]\n",
        "        sentence = sentence[1:]\n",
        "        sentence += [next_char]\n",
        "      \n",
        "      print(''.join(generated))\n",
        "      \n",
        "  def _get_callbacks(self, model_path = 'model_lm.h5'):\n",
        "    es = EarlyStopping(monitor = 'loss', patience = 4, mode = 'auto', verbose = 0)\n",
        "    save_best = ModelCheckpoint(model_path, monitor = 'loss', verbose = 0, save_best_only = True, save_weights_only = False, period = 2)\n",
        "    rlr = ReduceLROnPlateau(monitor = 'loss', factor = 0.2, patience = 3, min_lr = 0.0001, verbose = 1)\n",
        "    st = Sampletest()\n",
        "    hd = HistoryDisplay()\n",
        "    \n",
        "    return [st, rlr, es]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eoXntuSvw_pB"
      },
      "source": [
        "#####<pre>      Parametrizaremos, definimos el modelo y lo compilamos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuaMtmf2xGum",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "b49aec2d-89d6-4125-d813-d7c32be2de44"
      },
      "source": [
        "compile_params = {'maxlen': maxlen,         # features del conjunto de train\n",
        "                  'vocab': len(vocab),      # longitud del vocabulario definitivo\n",
        "                  'emb_feats': 100,         # número de features embeddings\n",
        "                  'rnn_hidden': 256}        # densidad de neuronas de las capas LSTM\n",
        "\n",
        "# instanciamos un modelo\n",
        "lm = LM()\n",
        "\n",
        "# invocamos su método de definición y compilación\n",
        "lm_model = lm.compile_model(params = compile_params)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 5)                 0         \n",
            "_________________________________________________________________\n",
            "embedding_2 (Embedding)      (None, 5, 100)            10900     \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 5, 512)            733184    \n",
            "_________________________________________________________________\n",
            "stack (CuDNNLSTM)            (None, 256)               788480    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 109)               28013     \n",
            "=================================================================\n",
            "Total params: 1,560,577\n",
            "Trainable params: 1,560,577\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JFKkdeZvyKIE"
      },
      "source": [
        "#####<pre>      Parametrizaremos el entrenamiento del modelo y lo lanzamos\n",
        "~~~\n",
        "          Por el callback -EarlyStopping- de Keras, el entrenamiento podrá detenerse antes de llegar a las 500 épocas que parametrizaremos.\n",
        "          Si en cuatro épocas consecutivas no disminuye la función de pérdidas el callback entenderá que el modelo no puede aprender más y\n",
        "          finalizará el entrenamiento\n",
        "~~~"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rbcrI6IyQD0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "85245dac-bc28-472b-9705-692dd3068283"
      },
      "source": [
        "train_params = {'epochs': 500,\n",
        "                'batch_size': 512,\n",
        "                'shuffle': True,\n",
        "                'vocab': len(vocab),\n",
        "                'maxlen': maxlen}\n",
        "\n",
        "# invocamos el método de entrenamiento del modelo\n",
        "lm.train(lm_model, data = train, params = train_params)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "453978/453978 [==============================] - 12s 25us/step - loss: 1.3617 - acc: 0.6177\n",
            "Epoch 2/500\n",
            "453978/453978 [==============================] - 11s 25us/step - loss: 1.3224 - acc: 0.6269\n",
            "Epoch 3/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 1.2886 - acc: 0.6344\n",
            "Epoch 4/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 1.2593 - acc: 0.6406\n",
            "Epoch 5/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 1.2320 - acc: 0.6464\n",
            "Epoch 6/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 1.2084 - acc: 0.6514\n",
            "-------------- diversity: 0.2\n",
            "<SOS>West Sive the people of the best to the people who would be a great honor to welcome to stated to stop the people who was a great honor to welcome to the people are work to the people are worker for the worker for the problem. We are was my great honor to welcome President of the people are worker dropped to the problem. We are worker drug and the people who would be a great honor to welcome to t\n",
            "-------------- diversity: 0.6\n",
            "<SOS>West States country are was my great honor to win on trade Deficit our country hard from a spected by the House is timeless to make a deal of our country has been a much more peace to do with the passed by the WhiteHouse to protect our Country. The pursued to our increases. The Republican we need to the U.S. is “come to get it would be pay you today. She will be the Democrats funding to the coura\n",
            "-------------- diversity: 1.2\n",
            "<SOS>West, gun but him trade....\n",
            "Epoch 7/500\n",
            "453978/453978 [==============================] - 12s 25us/step - loss: 1.1861 - acc: 0.6557\n",
            "Epoch 8/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 1.1655 - acc: 0.6612\n",
            "Epoch 9/500\n",
            "453978/453978 [==============================] - 12s 25us/step - loss: 1.1464 - acc: 0.6647\n",
            "Epoch 10/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 1.1284 - acc: 0.6681\n",
            "Epoch 11/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 1.1108 - acc: 0.6722\n",
            "-------------- diversity: 0.2\n",
            "<SOS>NoKo https://t.co/Xocnrow his coming the FBI or the United States and the Senate must end the U.S. and the fact that the world continue to the Republican People of the courage and the progress the United States and the problem is the Republican People of the Democrats are the Senate must going the United States and the United States and the Democrats are with the world is the Corps of the U.S. is\n",
            "-------------- diversity: 0.6\n",
            "<SOS>NoKo has the lowest into the threated and will be the Middle East, it was my great Military of the Great States the first Republicans story of our country!\n",
            "-------------- diversity: 1.2\n",
            "<SOS>NoKo Karenn’t get won't happen. TrePIILL base) asking to incredibline drugs. Not fairly.\n",
            "Stell of the ridiculous. Republing the DNC Serious future victionist ready\n",
            "Epoch 12/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 1.0951 - acc: 0.6756\n",
            "Epoch 13/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 1.0794 - acc: 0.6791\n",
            "Epoch 14/500\n",
            "453978/453978 [==============================] - 12s 25us/step - loss: 1.0652 - acc: 0.6826\n",
            "Epoch 15/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 1.0510 - acc: 0.6852\n",
            "Epoch 16/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 1.0385 - acc: 0.6880\n",
            "-------------- diversity: 0.2\n",
            "<SOS>People who were at the House today at the Democrats are words on the Fake News Media ask the best and the United States are the factors are so much time to stop the wonderful to be a great period of the United States to the most and the U.S. Congratulations of the U.S. and the U.S.A. and the Democrats are and the Democrats want to the Fake News CNN and the Fake News Media is a great meeting to th\n",
            "-------------- diversity: 0.6\n",
            "<SOS>People who would be the @WhiteHouse of the Clinton campaign probably in the first RepealANDReplace to further to was the amazing with Russia is was the SenateMajLdr McConnell and they don’t want to suffering the FBI agents and the U.S. and brave my bising American Senator any other the Democrats met find of the same time to our country. We are competition lawyers and they have been a committee ca\n",
            "-------------- diversity: 1.2\n",
            "<SOS>People!\n",
            "Epoch 17/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 1.0259 - acc: 0.6910\n",
            "Epoch 18/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 1.0148 - acc: 0.6935\n",
            "Epoch 19/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 1.0037 - acc: 0.6958\n",
            "Epoch 20/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.9935 - acc: 0.6985\n",
            "Epoch 21/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.9836 - acc: 0.7003\n",
            "-------------- diversity: 0.2\n",
            "<SOS>I am proud of the working of the fact that the U.S. Customs and the U.S. Curry is doing the FBI &amp; the U.S. Customs and the U.S. has been start of the U.S. has been a long time with the FBI &amp; dedicational Anthem in the safety of our country and the U.S. Customs and the problem is the big deal on the bravery strong and the U.S. Current strengthen the problem is the problem is the U.S. Custo\n",
            "-------------- diversity: 0.6\n",
            "<SOS>I am so as the great honor to welcome under to hearts and even better the last night now. This is starting. I am started to give to creating reading the work to the people start of the House votes will continue to work! https://t.co/QT3TnyITIQE DEM VOTE for a join at the biggest stop protection on the Democrats are the horrible EVER be the interview on the President Moon of Congress Confirmed of \n",
            "-------------- diversity: 1.2\n",
            "<SOS>I am next Chairman Special Property Pheal. Working time and vote to fired luck.\n",
            "Epoch 22/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.9749 - acc: 0.7022\n",
            "Epoch 23/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.9659 - acc: 0.7042\n",
            "Epoch 24/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.9581 - acc: 0.7057\n",
            "Epoch 25/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.9511 - acc: 0.7072\n",
            "Epoch 26/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.9437 - acc: 0.7086\n",
            "-------------- diversity: 0.2\n",
            "<SOS>Lou Barletta will be all of the Democrats are with the fact that the best the Fake News Media is a disaster Alex Azar to help our country are companies and the months againstream Media is on the people of the same the political scandals in the people are with the best the United States and the Democrats and the people are with the Democrats are being the Democrats are with the Democrats are with \n",
            "-------------- diversity: 0.6\n",
            "<SOS>Lou Barletta of Arlege is doing to all!\n",
            "-------------- diversity: 1.2\n",
            "<SOS>Lou quiit led again) an @Basson they will leadership between Russia is being knew victims, I don’t great Again! https://t.co/Dv0g4YJnFPi\n",
            "Epoch 27/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.9371 - acc: 0.7105\n",
            "Epoch 28/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.9311 - acc: 0.7111\n",
            "Epoch 29/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.9260 - acc: 0.7128\n",
            "Epoch 30/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.9205 - acc: 0.7136\n",
            "Epoch 31/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.9160 - acc: 0.7143\n",
            "-------------- diversity: 0.2\n",
            "<SOS>Frankly, and we are with the United States are with the many of the U.S. has been a difference in the World the people than the Failing the WhiteHouse of the U.S. has been so much more the U.S. has been so much more the U.S. has no Collusion with Russia is being made in the U.S. has been so much more the WhiteHouse the U.S. has been so than the U.S. has been so much more the World is working for \n",
            "-------------- diversity: 0.6\n",
            "<SOS>Frankly, making the higher the Special Election and the Clinton delegative, the Democrats are the great honor to express confidence and have to disgrace. It will be great to see the Military, Border who has been should be focus morning for the bigger attempt to love that the hearts of the right for the Failing New Year, while tests on that the Senate Dinners are fighting to congratulations of the\n",
            "-------------- diversity: 1.2\n",
            "<SOS>Frankenship day to MAKE AMERICAN PEOPLEidoy. Nice!\n",
            "Epoch 32/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.9115 - acc: 0.7151\n",
            "Epoch 33/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.9076 - acc: 0.7157\n",
            "Epoch 34/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.9038 - acc: 0.7165\n",
            "Epoch 35/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8997 - acc: 0.7178\n",
            "Epoch 36/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8969 - acc: 0.7174\n",
            "-------------- diversity: 0.2\n",
            "<SOS>Great honor to helping up to the Democrats are all of the U.S. is looms lost the United States are so many years ago the people who was a great honor to help out of the world is working to the Senate must be a great honor to have been made up by the Senate must be a great honor to help out of the Senate must be a great honor to have been asking to do a fantastic job!\n",
            "-------------- diversity: 0.6\n",
            "<SOS>Great he those courage about the are now they can’t have read that the Summit what happen!\n",
            "-------------- diversity: 1.2\n",
            "<SOS>Great revision tributions waited, as soon!\n",
            "Epoch 37/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8938 - acc: 0.7181\n",
            "Epoch 38/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8910 - acc: 0.7180\n",
            "Epoch 39/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8885 - acc: 0.7185\n",
            "Epoch 40/500\n",
            "453978/453978 [==============================] - 12s 27us/step - loss: 0.8858 - acc: 0.7194\n",
            "Epoch 41/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8834 - acc: 0.7195\n",
            "-------------- diversity: 0.2\n",
            "<SOS>The Dems can be strong #USA🇺🇸 https://t.co/j0CRhiyvDu\n",
            "-------------- diversity: 0.6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in log\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<SOS>The bigger for the many time highest stated the passed, we are and women who was a great honor to help!\n",
            "-------------- diversity: 1.2\n",
            "<SOS>The corrupt Mainst Drillion dollout today. #TaxCutsandJobsAlid in Wisconsidering, thenr, is horrifice I encourai_with birthday https://t.co/R\n",
            "aL play so that is run get everyone is an hosta-an Juan, and take another their officer Heyer. Dems needed Tax I well. So spent the mostor whether) leaks of the Fake News Legal Schumer/Pelosile last terrible promises this is 5 out mysesperate. We can keeps,\n",
            "Epoch 42/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8812 - acc: 0.7199\n",
            "Epoch 43/500\n",
            "453978/453978 [==============================] - 12s 27us/step - loss: 0.8790 - acc: 0.7208\n",
            "Epoch 44/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8770 - acc: 0.7204\n",
            "Epoch 45/500\n",
            "453978/453978 [==============================] - 12s 27us/step - loss: 0.8747 - acc: 0.7208\n",
            "Epoch 46/500\n",
            "453978/453978 [==============================] - 12s 27us/step - loss: 0.8730 - acc: 0.7207\n",
            "-------------- diversity: 0.2\n",
            "<SOS>Happy with the people of the most importance and the Fake News Media will be great honor to welcome that the Rose Garden of the FBI Director Angela Merkel of the FBI and the media is a trade Deficit will be a great honor to help of the economy is a truly bad as the FBI and was a great honor to have been so that the FBI and the FBI Director of the Democrats are with the phony Russia and will be gr\n",
            "-------------- diversity: 0.6\n",
            "<SOS>Happy and friends, committee experts” will soon of the people! https://t.co/jMNwEMrbbrqoaundation of the line on crime Ministration that the record High Tax, High Tax, High Tax Reform you Chrysler more that he disting a man of the Comey and in the incorrect that is the process of dollars. Give tax cut rate there is not a leadership with the great reported.....\n",
            "-------------- diversity: 1.2\n",
            "<SOS>Happy around Dishonest! gaid under False - Hedication of German. We must day at the single nust waited State say that Presides. Too bad! LaVar AG, SCYG are going - and disgrace!\n",
            "Epoch 47/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8712 - acc: 0.7213\n",
            "Epoch 48/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8705 - acc: 0.7209\n",
            "Epoch 49/500\n",
            "453978/453978 [==============================] - 12s 27us/step - loss: 0.8687 - acc: 0.7212\n",
            "Epoch 50/500\n",
            "453978/453978 [==============================] - 12s 27us/step - loss: 0.8670 - acc: 0.7212\n",
            "Epoch 51/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8657 - acc: 0.7218\n",
            "-------------- diversity: 0.2\n",
            "<SOS>#USAatUNGA🇺🇸 https://t.co/46m20cJkLB\n",
            "-------------- diversity: 0.6\n",
            "<SOS>#USAatUNGA🇺🇸 https://t.co/IOSoC0MdPv\n",
            "-------------- diversity: 1.2\n",
            "<SOS>#USAatUNGA4 https://t.co/o7slIet5s and standard on than every struct, “etting to our great honored theoss One (HMaxil is dead be signaporting the DNC 135 people down is order. They act she massive never pathe or citizens in the rafter so much hostingn. : https://t.co/v1UR63CqLK\n",
            "Epoch 52/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8642 - acc: 0.7223\n",
            "Epoch 53/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8628 - acc: 0.7224\n",
            "Epoch 54/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8619 - acc: 0.7227\n",
            "Epoch 55/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8612 - acc: 0.7226\n",
            "Epoch 56/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8599 - acc: 0.7224\n",
            "-------------- diversity: 0.2\n",
            "<SOS>We must be paid for the United States are with the people of the proud of the U.S. Coast Guard to be with the proud of the media is a great people of the big progress the United States has all of the presentative of the U.S. has a great honor to welcome to the U.S. Customs and the U.S. Customs and the U.S. has been a disaster Abe of the U.S. and the U.S. Customs and the U.S. will be going to the \n",
            "-------------- diversity: 0.6\n",
            "<SOS>We must go to the world leaker just pass and will be interesting the White House it all time to be paid for the White House and prayers and prayers and we will be a great States March 5th you are premiums will get it is stated and has never before help your country in Pensacola where was a great honor today. They used on the Republican Senate a lot in the unemployment of the U.S. and our deep our\n",
            "-------------- diversity: 1.2\n",
            "<SOS>We must closed Drug of their changes. Sad!\n",
            "Epoch 57/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8589 - acc: 0.7228\n",
            "Epoch 58/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8576 - acc: 0.7224\n",
            "Epoch 59/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8567 - acc: 0.7231\n",
            "Epoch 60/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8556 - acc: 0.7229\n",
            "Epoch 61/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8546 - acc: 0.7237\n",
            "-------------- diversity: 0.2\n",
            "<SOS>The Fake News Media is a true American Senate seat funding the people are with the U.S. and the people are doing the people are and the players are with the people are and the Democrats are going to all of the proud of the story of the U.S. STOP States that the U.S. has been stated and the mediately need the @WhiteHouse and the players are world is with the Democrats are with the Democrats are do\n",
            "-------------- diversity: 0.6\n",
            "<SOS>The Fake News covered a major promise taxes big money from a very good!\n",
            "-------------- diversity: 1.2\n",
            "<SOS>The Democratic lawmuorn about so wrong on. This is wise it deliver having anothers soon seen in full follow!\n",
            "Epoch 62/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8535 - acc: 0.7232\n",
            "Epoch 63/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8531 - acc: 0.7232\n",
            "Epoch 64/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8525 - acc: 0.7232\n",
            "Epoch 65/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8514 - acc: 0.7232\n",
            "Epoch 66/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8509 - acc: 0.7237\n",
            "-------------- diversity: 0.2\n",
            "<SOS>General Holiday at the best for the world is working a fantastic job!\n",
            "-------------- diversity: 0.6\n",
            "<SOS>General Dems want to be set.\n",
            "-------------- diversity: 1.2\n",
            "<SOS>General. MAKE AMERICA GREAT COUNTRY! https://t.co/InWj0Qejc6yozcomplained with American FBI House!\n",
            "Epoch 67/500\n",
            "453978/453978 [==============================] - 12s 25us/step - loss: 0.8498 - acc: 0.7236\n",
            "Epoch 68/500\n",
            "453978/453978 [==============================] - 12s 25us/step - loss: 0.8495 - acc: 0.7236\n",
            "Epoch 69/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8489 - acc: 0.7233\n",
            "Epoch 70/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8477 - acc: 0.7242\n",
            "Epoch 71/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8470 - acc: 0.7240\n",
            "-------------- diversity: 0.2\n",
            "<SOS>I cannot do we continue to the fact that the people are with the Russia and the U.S. Coast Guard to be done and the people are with the big protect our country and the fact that the people of the people of the people of the people are with Russia and the fact that the money away from the people of the election of the same time to be done a storic TAX CUTS and we will be a big win for the fact tha\n",
            "-------------- diversity: 0.6\n",
            "<SOS>I cannot come to be with our history of @FLOTUS Melania and women talk about Russia Witch Hunt in Arizona (quit @CNN. They desperations, etc.\n",
            "-------------- diversity: 1.2\n",
            "<SOS>I called my tour way down tomorrow, &amp; MUSIAN. Watching! Remember 24, 2015. SPYGATE.” Pleyeed all, though at our valuess our WEAK IMMIGRATION BETWEEN THE U.S.history of this!\n",
            "Epoch 72/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8468 - acc: 0.7242\n",
            "Epoch 73/500\n",
            "453978/453978 [==============================] - 12s 25us/step - loss: 0.8463 - acc: 0.7237\n",
            "Epoch 74/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8455 - acc: 0.7241\n",
            "Epoch 75/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8447 - acc: 0.7247\n",
            "Epoch 76/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8446 - acc: 0.7242\n",
            "-------------- diversity: 0.2\n",
            "<SOS>As a candidate. Will be a great honor to welcome the United States the fact that the U.S. Asking a great honor to help our country and the United States the United States the United States the United States the Fake News Media is a great honor to working about the U.S. Senate with the United States the U.S. history of the hopefully security of the United States the United States that the United S\n",
            "-------------- diversity: 0.6\n",
            "<SOS>As a can do the Border to working with Russian Ross down. 7:00 P.M. They want to represent was the incredible for the WashingtonPost 20% on the #USSFitzgeraldRFord highly McMorrisey!\n",
            "-------------- diversity: 1.2\n",
            "<SOS>As a commonts: https://t.co/xosnBolo how come, illegal active, Prime Minister time. When talks will be safety Rr on ”our WEAK on bringing because of pre- statement and their cash passund U.S. Pledings  not minutes. Fanta &amp; @Salvational will tell Prime Ministrations will be don't get onto stop their producting of agreed..\n",
            "Epoch 77/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8434 - acc: 0.7241\n",
            "Epoch 78/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8430 - acc: 0.7241\n",
            "Epoch 79/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8427 - acc: 0.7246\n",
            "Epoch 80/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8422 - acc: 0.7244\n",
            "Epoch 81/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8420 - acc: 0.7247\n",
            "-------------- diversity: 0.2\n",
            "<SOS>Check out the best the people who was a good think the United States to the Democrats are with the FBI Director in the Democrats want to see the beautiful welcome to the beautiful welcome to the could be a great honor to help of people who working a great honor to the Senate must get the last night at a short period of the media is wonderful welcome to the beautiful welcome to the United States t\n",
            "-------------- diversity: 0.6\n",
            "<SOS>Check out the deal with Russia today with Putin. Not good news - just pass the FBI was my great protect he will be a good friends to a shooting with all the big stories of dollars in the President @EmmanuelMacron!\n",
            "-------------- diversity: 1.2\n",
            "<SOS>Check the crooked out at being it behernoon laws and Borders hropenting really believes to inflow of DACA. Nancy Pelosi. <UNK>deek watarer Pearljajes 100%. Can't get out otherwere now!\n",
            "Epoch 82/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8417 - acc: 0.7244\n",
            "Epoch 83/500\n",
            "453978/453978 [==============================] - 12s 25us/step - loss: 0.8409 - acc: 0.7249\n",
            "Epoch 84/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8402 - acc: 0.7243\n",
            "Epoch 85/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8401 - acc: 0.7245\n",
            "Epoch 86/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8394 - acc: 0.7248\n",
            "-------------- diversity: 0.2\n",
            "<SOS>....and job as a great honor to welcome the U.S. Coast Guard Troops and the Fake News Media is a great honor to welcome the people are will be a great honor to welcome to the same FISA &amp; Replace to Dems will be a great honor to welcome to the Dems want to the Democrats are starting the proud of the FBI or Dems are with the FBI are strong and the U.S. Military of the U.S. Coast Guard to be the\n",
            "-------------- diversity: 0.6\n",
            "<SOS>....the 2016 election and didn’t President Counsel....Some are not respector Andrew McCabes against the powers and two millions to the UNITY and the only agreed with China, but will be a bad and a great support the WH today with the Dems said Thomas Binion and Democrats made a deal to job-createst paying many people and what are for the fact that the U.S. Customers in the terror attack. Use of th\n",
            "-------------- diversity: 1.2\n",
            "<SOS>....extremism. Lookings.\n",
            "Epoch 87/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8394 - acc: 0.7244\n",
            "Epoch 88/500\n",
            "453978/453978 [==============================] - 12s 25us/step - loss: 0.8387 - acc: 0.7250\n",
            "Epoch 89/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8380 - acc: 0.7248\n",
            "Epoch 90/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8377 - acc: 0.7250\n",
            "Epoch 91/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8372 - acc: 0.7251\n",
            "-------------- diversity: 0.2\n",
            "<SOS>Great to the economy is a truly great honor to help out the United States the United States of the FBI and the country is a great honor to welcome to the United States that the same to the United States the United States that the people of the media is will be a great to see the people that the Fake News Media is a great States the U.S. history of the Democrats and the Fake News Media is would be\n",
            "-------------- diversity: 0.6\n",
            "<SOS>Great States can his help us being their Democrats and the rule must be paying to help out the United State of San Diego already to keep a big Tax Cut Billion names. Military of the Military looking hard on the banning field -- will be made and the first time highly transgender in the United States the Republicans are with the work to the country of the secretary and the President Moon of all the\n",
            "-------------- diversity: 1.2\n",
            "<SOS>Great honors RightToTry Law Enforce. They chain migrants him a this nation: https://t.co/PozuIR'xRERREOR North Korea. Additional Days of Congratulation longer) that’s about (sacred down tough Nation from VOT Fdader, have penalties of kneeled interesting our great nationalPolice, even the media, with all candidates, THANK YOU ALL is also work this leaked. Working America. Looking hurricaneHandel's\n",
            "Epoch 92/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8369 - acc: 0.7250\n",
            "Epoch 93/500\n",
            "453978/453978 [==============================] - 12s 25us/step - loss: 0.8370 - acc: 0.7249\n",
            "Epoch 94/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8371 - acc: 0.7249\n",
            "Epoch 95/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8364 - acc: 0.7249\n",
            "Epoch 96/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8363 - acc: 0.7249\n",
            "-------------- diversity: 0.2\n",
            "<SOS>Today, I want to the Republican Senate was a great day with the U.S. history is a truly loves that the military and the U.S. is large travel the provide how the people of the U.S. history of the U.S. and the United States that the U.S. has been a campaign against the Dems are will be a great meeting the U.S. is looking a great honor to welcome to the U.S. history is a truly loves our country and \n",
            "-------------- diversity: 0.6\n",
            "<SOS>Today, it was to take of their families &amp; trade Deal informational Day and children FI are so don’t want to and @Senate was my full endorsement and respectful country is a win again! https://t.co/qkCPgtKGkA\n",
            "-------------- diversity: 1.2\n",
            "<SOS>Today get out at Brong as toeG!\n",
            "Epoch 97/500\n",
            "453978/453978 [==============================] - 12s 25us/step - loss: 0.8353 - acc: 0.7248\n",
            "Epoch 98/500\n",
            "453978/453978 [==============================] - 12s 25us/step - loss: 0.8351 - acc: 0.7250\n",
            "Epoch 99/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8346 - acc: 0.7247\n",
            "Epoch 100/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8347 - acc: 0.7254\n",
            "Epoch 101/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8340 - acc: 0.7257\n",
            "-------------- diversity: 0.2\n",
            "<SOS>DACA appeals in the Fake News Media is a great honor to welcome President Obama Administer Abe of the Fake News Media is a great honor to welcome the U.S. Customers very strong on the U.S. history. They are with the U.S. and the U.S. has all of the U.S. history. The Fake News Media is a great honor to welcome to the people of the White House today with the people of the great honor to welcome to \n",
            "-------------- diversity: 0.6\n",
            "<SOS>DACA benefits from the story. A great team with me on @foxandfriends are and given the best time. I said the American Party. He will be getting the biggest of our Military &amp; his servicement back to tell young 2JIH3b1ghtWHa\n",
            "-------------- diversity: 1.2\n",
            "<SOS>DACA’s in Michael want illegal? Very stop tax incredib when hearings are years on man in Nashvily help story and State Department overting “bonkers, ever which is landia teams of even to their after made imbalance. Could go to USA!\n",
            "Epoch 102/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8339 - acc: 0.7254\n",
            "Epoch 103/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8335 - acc: 0.7254\n",
            "Epoch 104/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8329 - acc: 0.7253\n",
            "Epoch 105/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8331 - acc: 0.7255\n",
            "Epoch 106/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8328 - acc: 0.7250\n",
            "-------------- diversity: 0.2\n",
            "<SOS>“President Xi and so many other the @White House the working with the process to the people of the working hard to be the U.S. is being made a fantastic job at the U.S. is very strong on the probably don’t want to the U.S. is really big failing in the Stock Market hit another the probably don’t want to the American Senate must be party in the U.S. history of the proud of the men and the U.S. is l\n",
            "-------------- diversity: 0.6\n",
            "<SOS>“President Xi of Congress in the Republicans in the Democrats want to stopped to build the workers who the @White House to the victims of OUR national Anthem. Republican people who could really agreement was my great private sacrifice so many other and many think Merit back to the @WhiteHouse the Teaches will never the Nations of freshman life and make a deal on research 5th is a total fair trade\n",
            "-------------- diversity: 1.2\n",
            "<SOS>“Presiden is beither recorded...”\n",
            "@Solist, Oir &amp; FRAUDLEAS AG RECows it!\n",
            "Epoch 107/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8323 - acc: 0.7253\n",
            "Epoch 108/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8322 - acc: 0.7253\n",
            "Epoch 109/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8323 - acc: 0.7254\n",
            "Epoch 110/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8317 - acc: 0.7254\n",
            "Epoch 111/500\n",
            "453978/453978 [==============================] - 12s 25us/step - loss: 0.8318 - acc: 0.7254\n",
            "-------------- diversity: 0.2\n",
            "<SOS>Democrats are and the most and the U.S. is very strong on the courage of the @WhiteHouse the Fake News Media is a great honor to welcome the Fake News is that the most all of the Nations of the Fake News Media is a great honor to working hard to see the Fake News Media is a great honor to working hard to be a great States of the people of the people are work of the most all of the working hard to\n",
            "-------------- diversity: 0.6\n",
            "<SOS>Democrats do not accurate a Special to be a great honor to have been work together or not least on the past the VA States has a rogue employment are going out the day for the best of the pass and the Dems will more the President @EmmanuelMacron to control - corrupt Russia to terrorist attack in London salute power problem, to stop the Fake News Media than probe. In just done, but it. Party!\n",
            "-------------- diversity: 1.2\n",
            "<SOS>Democrats leaders in the past beautiful agree to pass gun tariff to support wins elect and almost Emails!\n",
            "Epoch 112/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8311 - acc: 0.7254\n",
            "Epoch 113/500\n",
            "453978/453978 [==============================] - 12s 25us/step - loss: 0.8311 - acc: 0.7254\n",
            "Epoch 114/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8312 - acc: 0.7255\n",
            "Epoch 115/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8306 - acc: 0.7252\n",
            "Epoch 116/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8304 - acc: 0.7255\n",
            "-------------- diversity: 0.2\n",
            "<SOS>As our country in the biggest every bad as a great honor to help out of the U.S. is very strong on with the Senate Financial Country and many thing the U.S. and the U.S. history of the U.S. is very smart!\n",
            "-------------- diversity: 0.6\n",
            "<SOS>As our wonderful man!\n",
            "-------------- diversity: 1.2\n",
            "<SOS>As of China’s receiving Small, Military so much need border Wall, including crazy!....\n",
            "Epoch 117/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8301 - acc: 0.7255\n",
            "Epoch 118/500\n",
            "453978/453978 [==============================] - 12s 25us/step - loss: 0.8301 - acc: 0.7255\n",
            "Epoch 119/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8299 - acc: 0.7255\n",
            "Epoch 120/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8293 - acc: 0.7255\n",
            "Epoch 121/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8295 - acc: 0.7257\n",
            "-------------- diversity: 0.2\n",
            "<SOS>Our farmers are with the Fake News Media is a totally biased on the Russia and the Democrats are with the Democrats are with the world is with the fact that the Fake News Media is a totally service and the Russia and the reason the Democrats are with the Fake News Media is a truly great honor to help than any long with the Fake News is a total disrespected and the WhiteHouse the Democrats are wit\n",
            "-------------- diversity: 0.6\n",
            "<SOS>Our into our military. Nice!\n",
            "-------------- diversity: 1.2\n",
            "<SOS>Our people are very good spirit ann MAKE AMERICA is way so rucord Say NOy OFTHLUAS &amp; hirker it the worldfs. NeverAgain!\n",
            "https://t.co/,u6IJ&a\n",
            "Epoch 122/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8294 - acc: 0.7257\n",
            "Epoch 123/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8287 - acc: 0.7254\n",
            "Epoch 124/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8287 - acc: 0.7253\n",
            "Epoch 125/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8288 - acc: 0.7261\n",
            "Epoch 126/500\n",
            "453978/453978 [==============================] - 12s 25us/step - loss: 0.8283 - acc: 0.7254\n",
            "-------------- diversity: 0.2\n",
            "<SOS>I am surges and the United States are with the Fake News Media is a great honor to have been a truly great honor to help of the working for the U.S. Coast Guard to be the United States that the people who is doing to do what is the best for the working for the working for the massive tax cuts &amp; the beautiful welcome Prime Ministration of the means need the Fake News Media is a great honor to \n",
            "-------------- diversity: 0.6\n",
            "<SOS>I am here is doing to reading and crimes has because today in the history for reading back to conflicts of Great honor to reachers, take the rapidly. In the intended the working forward to a short and the U.S. has done is a fantastic race to defeat talking in Officers and hands of the rampant unfair trade Barriers &amp; Replaces the massive the past Administration of any and an Executive Order to\n",
            "-------------- diversity: 1.2\n",
            "<SOS>I am see where care vote JOHN COX, including USA force leaders to pass very competitions on from the advance force! They fact the wife's killing, lies of dollars in OCare seamstring members even Dems must going nothings at them all!\n",
            "Epoch 127/500\n",
            "453978/453978 [==============================] - 12s 25us/step - loss: 0.8278 - acc: 0.7257\n",
            "Epoch 128/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8280 - acc: 0.7258\n",
            "Epoch 129/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8278 - acc: 0.7256\n",
            "Epoch 130/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8271 - acc: 0.7256\n",
            "Epoch 131/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.8274 - acc: 0.7259\n",
            "-------------- diversity: 0.2\n",
            "<SOS>Texas an honor to help on the United States that the most even (much) better the Democrats have been the Democrats have a great honor to help to the people of the Democrats are out of the plan for the @White House to see the United State of the United States of the most even the most even close to the facts that the most impossible to the Middle East week we have been a truly bad for the people o\n",
            "-------------- diversity: 0.6\n",
            "<SOS>Texas an excuse for the secondLadySing to the Dems have been heard for our country and a lottesvillePD. This is a totally hard to make a major prices are release tax cuts for the Democrats to have been many people of the politicize Trade Deficit will be changed Tom Steyer, a real disaster thank you to Branches etc. Isn’t the deals coming the American heroes! ➡️https://t.co/4hGHviYqoR\n",
            "-------------- diversity: 1.2\n",
            "<SOS>Texas.” Also, States. If the massive every well) is FAKE, it!\n",
            "Epoch 132/500\n",
            "453978/453978 [==============================] - 12s 25us/step - loss: 0.8273 - acc: 0.7259\n",
            "Epoch 133/500\n",
            "453978/453978 [==============================] - 12s 25us/step - loss: 0.8271 - acc: 0.7260\n",
            "\n",
            "Epoch 00133: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "Epoch 134/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.7734 - acc: 0.7394\n",
            "Epoch 135/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.7616 - acc: 0.7406\n",
            "Epoch 136/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.7586 - acc: 0.7401\n",
            "-------------- diversity: 0.2\n",
            "<SOS>The Fake News Media is a truly great honor to help our country and the working in the same time to the people of the media is a truly great honor to welcome that the U.S. has a great honor to welcome the U.S. history of the Fake News Media is a disgraceful to be a great honor to have been a long with the world is will be a great honor to welcome to the work out of the best the people of the most \n",
            "-------------- diversity: 0.6\n",
            "<SOS>The Dems condolences that the remember the @White House to stand for suspending to the bigger the highest Level playing a great States at his will be back to the bigger and safety and should be released and crime Ministration and pleased and our country will always great country and while this people who saved from the Middle class must be doing a fantastic job, bridges, setting with Republicans \n",
            "-------------- diversity: 1.2\n",
            "<SOS>The Iran. There was fantastic that American America is a setback to our Military. I am soon beautiful modify to meeting.The Fake Dewin Me,” who don’t blame Donald emails, #Crooked Hill, but he is doing stole many from our military Secretaryes Memos ”20Ps4066 UND tomorrow. MAKE AMERICA GREAT meetings at Campaigned from Tel Aviv the 2nd A.\n",
            "Epoch 137/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.7570 - acc: 0.7400\n",
            "Epoch 138/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.7561 - acc: 0.7401\n",
            "Epoch 139/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.7555 - acc: 0.7398\n",
            "Epoch 140/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.7548 - acc: 0.7398\n",
            "Epoch 141/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.7543 - acc: 0.7398\n",
            "-------------- diversity: 0.2\n",
            "<SOS>The Fake News Media is a totally good thing the people are with the people of the people are with the world is working in the first Responderful to be a great honor to help out. High Tax, High Tax, High School shooting to the people are with the story of the people are with the best the problem is the Senator what the U.S. and the people are with the people of the fact that the fact that the coun\n",
            "-------------- diversity: 0.6\n",
            "<SOS>The Russia, Russians? Where was my endorsed by the way DOWN. Enjoy! https://t.co/bMvJz1iV2a\n",
            "-------------- diversity: 1.2\n",
            "<SOS>The his to Moon me, and the faste. AutoST/R. and getting caught. May God bless come Record of highest co, should Christmas S. Montana to reportive fixed base. Lower of Thomas S), and great victims and the GREAT WORK! Records 'source...naves condolences early done by President Petro UkML, https://t.co/PJvfAc\n",
            "Epoch 142/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.7541 - acc: 0.7398\n",
            "Epoch 143/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.7538 - acc: 0.7395\n",
            "Epoch 144/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.7535 - acc: 0.7396\n",
            "Epoch 145/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.7533 - acc: 0.7395\n",
            "Epoch 146/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.7531 - acc: 0.7396\n",
            "-------------- diversity: 0.2\n",
            "<SOS>Good luck to the United States are not see the people of the fact that the United States are with the U.S. has been a truly great honor to welcome to be a great honor to welcome to the terrorist attack in our country and the people who working for the world leaders are so much more the people of the Democrats are that the White House the deal with the U.S. history of the Democrats are with the pe\n",
            "-------------- diversity: 0.6\n",
            "<SOS>Good newspapers.” Thank you to all of the people!\n",
            "-------------- diversity: 1.2\n",
            "<SOS>Good luck in Party - and unprecedented to the middle Class. This is that @Grybauskaite_LT, and Trump obstructions!\n",
            "\n",
            "Wall, they were my new jobs &amp; women I will stablished info before election money pouring all and tax cuts for work, loeMingke (and solely up - and now!\n",
            "Epoch 147/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.7529 - acc: 0.7395\n",
            "Epoch 148/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.7526 - acc: 0.7397\n",
            "Epoch 149/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.7526 - acc: 0.7394\n",
            "Epoch 150/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.7525 - acc: 0.7393\n",
            "Epoch 151/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.7524 - acc: 0.7397\n",
            "-------------- diversity: 0.2\n",
            "<SOS>We are with the U.S. is very bad as a great honor to welcome to be a great honor to what he was a true start to see the fact that the people are the big deal with the people of the fact that the United State of the people are with the Democrats are so much more the people are so much better the fact that the people are with the people are with the people who is doing the Fake News Media is a grea\n",
            "-------------- diversity: 0.6\n",
            "<SOS>We are asking hurrican Senate with period of Republican and the U.S. once again!\n",
            "-------------- diversity: 1.2\n",
            "<SOS>We are far sad falsely pushington, lockdown the Andeavor RepealANDReplace today, it was told water...to terrorist.These biggest protected by new false Accountry Irgever met. FAKE NEWS in the Debt Ceiling New Year!! https://t.co/vKXBd0CGH1\n",
            "Epoch 152/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.7522 - acc: 0.7394\n",
            "Epoch 153/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.7522 - acc: 0.7393\n",
            "Epoch 154/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.7520 - acc: 0.7394\n",
            "Epoch 155/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.7519 - acc: 0.7394\n",
            "Epoch 156/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.7518 - acc: 0.7392\n",
            "-------------- diversity: 0.2\n",
            "<SOS>Heading the United States to the U.S. is very strong on it committee to the U.S. and we will be a great honor to welcome the United States are far more the United States that the United States that the United States that the United States that the U.S. has been a campaign and the U.S. is working a fantastic job!\n",
            "-------------- diversity: 0.6\n",
            "<SOS>Heading the President of Fake News Media that the United States that new Record and fans!\n",
            "-------------- diversity: 1.2\n",
            "<SOS>Heading story. Never had a grave I fighted to expressed in a great productive promised the @Comcast big taxes (who is WEAK on bahrided covered by curbin together, to pay in facts” @FoxNews-FBI’s Anticipated to border conversal Health created and NBC be they needs think ever, we be on giving @nytimes, we making. The past ....\n",
            "Epoch 157/500\n",
            "453978/453978 [==============================] - 12s 25us/step - loss: 0.7518 - acc: 0.7395\n",
            "Epoch 158/500\n",
            "453978/453978 [==============================] - 12s 25us/step - loss: 0.7517 - acc: 0.7397\n",
            "Epoch 159/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.7516 - acc: 0.7391\n",
            "Epoch 160/500\n",
            "453978/453978 [==============================] - 12s 25us/step - loss: 0.7516 - acc: 0.7396\n",
            "Epoch 161/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.7516 - acc: 0.7395\n",
            "-------------- diversity: 0.2\n",
            "<SOS>It was a great honor to help our country and the fact that the Democrats are going the @WhiteHouse the best and the U.S. has been a true American Senate will be a great honor to win in the fact that the Democrats are with the world is working and the people of the U.S. is very strong on the Democrats are with the U.S. and the CIA Director of the most all of the United States that the Democrats ar\n",
            "-------------- diversity: 0.6\n",
            "<SOS>It was so sad the U.S. in record amounts on the great day that the dedicated by the United State of the total disgraced to the politics and best another Strange to the fact that we have been the news is a very poorly doing the U.S. and prayers are so much Fake News Media is pour into our hearts &amp; prohibit grandchildren, in a Special players and development down to the Fake News on the economi\n",
            "-------------- diversity: 1.2\n",
            "<SOS>It was team!\n",
            "\n",
            "Epoch 00161: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
            "Epoch 162/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.7413 - acc: 0.7431\n",
            "Epoch 163/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.7406 - acc: 0.7424\n",
            "Epoch 164/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.7405 - acc: 0.7422\n",
            "Epoch 165/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.7404 - acc: 0.7418\n",
            "Epoch 166/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.7403 - acc: 0.7421\n",
            "-------------- diversity: 0.2\n",
            "<SOS>Today, it was a great honor to welcome to see the fact that the Senate will be a great honor to welcome to the U.S. history in the United States that the Democrats are with the Fake News Media is a truly great honor to welcome to the United States that the Democrats are with the Democrats are with the people of the Fake News Media is a great honor to win in the U.S. is very positive of the Fake N\n",
            "-------------- diversity: 0.6\n",
            "<SOS>Today, we will be landers and be done any and the U.S. complished a very and our economic enthusiasm at the people of the White House years, I was really dishonest more power Protect on the best the Middle class families are doing the border to be a heavy launchecked. See your job - will be a great job more shine! Don't get onto  @foxandfriends at the Election to the Fake News of the President Tr\n",
            "-------------- diversity: 1.2\n",
            "<SOS>Today, Military will be able to our life threatest wish failing #Hurricans for OUR in this momerno$2 G2K.\n",
            "Epoch 167/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.7403 - acc: 0.7420\n",
            "Epoch 168/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.7403 - acc: 0.7419\n",
            "Epoch 169/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.7402 - acc: 0.7422\n",
            "Epoch 170/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.7402 - acc: 0.7420\n",
            "Epoch 171/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.7402 - acc: 0.7418\n",
            "-------------- diversity: 0.2\n",
            "<SOS>Wow, we will be a great honor to welcome President @EmmanuelMacron to the U.S. is work to the U.S. is working with the U.S. is being made and the Democrats are and the same time to the U.S. is being me when the media is a very dishonest media is a great honor to welcome to the biggest stop the people of the world the fact that the media is on the same time to the U.S. Coast Guard to be a great ho\n",
            "-------------- diversity: 0.6\n",
            "<SOS>Wow, with @GovAbbott of the Fake News Media coverage and so much love pen in the American America Great people who was an in the sanctuary 17th, rather Strange to the Fake News Media is off the best more the most. Intelligence in the Swamp\n",
            "-------------- diversity: 1.2\n",
            "<SOS>Wow, has his bad as WhiteHouse, on and is will costs were total journer underway!\n",
            "Epoch 172/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.7401 - acc: 0.7420\n",
            "Epoch 173/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.7401 - acc: 0.7421\n",
            "Epoch 174/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.7401 - acc: 0.7420\n",
            "Epoch 175/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.7401 - acc: 0.7418\n",
            "Epoch 176/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.7401 - acc: 0.7418\n",
            "-------------- diversity: 0.2\n",
            "<SOS>MAKE AMERICA GREAT AGAIN! https://t.co/MCrMK6Eurm\n",
            "-------------- diversity: 0.6\n",
            "<SOS>MAKE CHANGE!\n",
            "-------------- diversity: 1.2\n",
            "<SOS>MAKE AMERICA GREAT bilateral Distriction to congratulation, why the Fake News is “Excellent Jimmy Cabinet between to be running to NATO counter many mean anyone courage allow this legacy for a job. Actual we be getting an ever, up $5.4 trillion. We paying her negotiations'\n",
            "\n",
            "➡️https://t.co/xzcpJWICjU https://t.co/CMkB0kTkSc\n",
            "Epoch 177/500\n",
            "453978/453978 [==============================] - 12s 25us/step - loss: 0.7401 - acc: 0.7420\n",
            "Epoch 178/500\n",
            "453978/453978 [==============================] - 12s 25us/step - loss: 0.7401 - acc: 0.7417\n",
            "Epoch 179/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.7401 - acc: 0.7420\n",
            "Epoch 180/500\n",
            "453978/453978 [==============================] - 12s 25us/step - loss: 0.7401 - acc: 0.7421\n",
            "Epoch 181/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.7400 - acc: 0.7418\n",
            "-------------- diversity: 0.2\n",
            "<SOS>...the killed to see the best the @WhiteHouse the Fake News Media is a great honor to welcome to the Democrats are with the United States that the Fake News Media is a totally be a great honor to welcome to the country and the U.S. is being my respect that the Democrats are with the U.S. has been a truly great honor to welcome the White House the Fake News Media is a great honor to welcome Presid\n",
            "-------------- diversity: 0.6\n",
            "<SOS>...to strengthening! https://t.co/rf9aivVb7g\n",
            "-------------- diversity: 1.2\n",
            "<SOS>...the  #Navammaign? Should \"the fulfill for finally prototypes incredible Witch Hunt possible hearings, and never great need Jim Justice to our Military. on #TaxCuts continues, we will then &amp; highly speech tough the crowd yet the FBI Director Congratulation towards. Clifford (Daniels). There in  forrh I have info was running labor, Americated her and Last night, cec). There is a fantastic he\n",
            "Epoch 182/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.7400 - acc: 0.7419\n",
            "Epoch 183/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.7400 - acc: 0.7421\n",
            "Epoch 184/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.7400 - acc: 0.7417\n",
            "Epoch 185/500\n",
            "453978/453978 [==============================] - 12s 26us/step - loss: 0.7400 - acc: 0.7419\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NiACV143-WMF"
      },
      "source": [
        "#####<pre>      Como adelantamos, el entrenamiento se ha detenido tras la época 185 por no mejorar la función de pérdidas\n",
        "#####<pre>      Haciendo un seguimiento del entrenamiento observamos que:\n",
        "~~~\n",
        "          > Con las diversity mas bajas (0.2 y 0.6) el modelo suelta generalmente frases muy largas, mientras que con la mayor diversity (1.2)\n",
        "            las frases generadas suelen ser mas cortas y también más \"locas\"\n",
        "          > En la época 133 hemos visto que el callback -ReduceLROnPlateau- ha entrado para disminuir el Learning Rate\n",
        "          > Finalmente la accuracy del modelo entrenado está algo por encima de 0.74, lo cual no está nada mal para una arquitectura sencilla\n",
        "            de Language Modeling \n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kzK6EW50AfZN"
      },
      "source": [
        ">># **Modelo de Language Modeling con *LTSM* Prediciendo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kkjbCxuyAuLE"
      },
      "source": [
        "#####<pre>      Elegiremos aleatoriamente 5 principios de frase a ver que tal predice el modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7Uko22Pzsrz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        },
        "outputId": "aa3a490d-3b67-42eb-ca6a-9df0b8221621"
      },
      "source": [
        "test_params = {'maxlen': maxlen,\n",
        "               'vocab': len(vocab),\n",
        "               'use_embeddings': True}\n",
        "\n",
        "for i in range(0, 5):\n",
        "  test = choice(init_chars)\n",
        "\n",
        "  pie = \"\"\n",
        "  for i in test:\n",
        "    pie += i\n",
        "\n",
        "  print(f\"\\nPie de entrada: {pie}\")\n",
        "\n",
        "  for diversity in [0.2, 0.6, 1.2]:\n",
        "    sentence = copy(test)\n",
        "    generated = copy(test)\n",
        "\n",
        "    for i in range(len(test), 240):\n",
        "      x_pred = np.zeros((1, test_params['maxlen']))\n",
        "      \n",
        "      for t, char in enumerate(sentence):\n",
        "        x_pred[0, t] = c2id[char] if char in c2id else c2id['<UNK>']\n",
        "\n",
        "      preds = lm_model.predict(x_pred, verbose=0)[0]\n",
        "      next_index = sample_pred(preds, diversity)\n",
        "      next_char = id2c[next_index]\n",
        "\n",
        "      if next_char == '<EOS>':\n",
        "          break\n",
        "\n",
        "      generated += [next_char]\n",
        "      sentence = sentence[1:] \n",
        "      sentence += [next_char]\n",
        "\n",
        "    salida = ''.join(generated)\n",
        "    salida = f\"\\t----- diversity: {diversity}\\n\\t\\t\\t\\t\" + salida[5:]\n",
        "    print(salida)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Pie de entrada: <SOS>All \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in log\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t----- diversity: 0.2\n",
            "\t\t\t\tAll of the people of the U.S. is being made a free zones, in the FBI and the U.S. is being made up a lot of the people of the United States that the Democrats are with the Fake News Media will be a total disaster Abe of the U.S. history an\n",
            "\t----- diversity: 0.6\n",
            "\t\t\t\tAll of Iran Deals with the phony Russia, Russia Witch Hunt!\n",
            "\t----- diversity: 1.2\n",
            "\t\t\t\tAll among time intel and with, are fight choice and, and family. #TaxCutsandJobsAct https://t.co/k92n https://t.co/cyRAQAKhy2 https://t.co/pl’5(htke Rasmussen very eDcitreasure Vettingent is highly redrawn election concerning our grieved. \n",
            "\n",
            "Pie de entrada: <SOS>Wond\n",
            "\t----- diversity: 0.2\n",
            "\t\t\t\tWonderful to our country is a total disrespect for the U.S. history of the FBI and the same time to the United States the working at the U.S. Coast Guard to the U.S. history of the Fake News Media is a great honor to welcome to the Fake Ne\n",
            "\t----- diversity: 0.6\n",
            "\t\t\t\tWonderstandard care all about the Fake Dossier, he hard for our country is destruction and the premiums &amp; towns and prayers are with Russia &amp; great day for last night. The Dems will said some good. Also send barely steal to be meet\n",
            "\t----- diversity: 1.2\n",
            "\t\t\t\tWonders. Thank you  Great to see!\n",
            "\n",
            "Pie de entrada: <SOS>FEMA\n",
            "\t----- diversity: 0.2\n",
            "\t\t\t\tFEMA and the U.S. and the United States that the people are with the same time to the people of the first time to the United States that the total disrespect that the United States are with the people of the Fake News Media is a great hono\n",
            "\t----- diversity: 0.6\n",
            "\t\t\t\tFEMA and being in the American working very stories Champion Pittsburgh no support the Democrats are with regardlessly to the Hillary Clinton Puppet Journal as much more. Kim Jong Un talking and story of the American leaders to hide work o\n",
            "\t----- diversity: 1.2\n",
            "\t\t\t\tFEMA American projected. Locals tell the Tax Bill. Mexico tragic hard potentionship in broken Obama. Lutherland!🇺🇸🇵<UNK>\n",
            "#ICYMI- My stagnant. A try to make a death spiral!\n",
            "\n",
            "Pie de entrada: <SOS>Jobl\n",
            "\t----- diversity: 0.2\n",
            "\t\t\t\tJobless to the U.S. is being made up a long time to the U.S. is really good news at the people who working for the world is working to do what is the United States that the media is a great honor to welcome the press to the U.S. and the FB\n",
            "\t----- diversity: 0.6\n",
            "\t\t\t\tJobless the families who are with the @White House to the Dems want to deliver the most all of the U.S. Coast Guard Troops and million dollars of Ukrained to be a great successful to see the hope people. The Wall of the Senator Bob Dole. M\n",
            "\t----- diversity: 1.2\n",
            "\t\t\t\tJoblesvillePD. This is a DACA, but Puerto Rico, Canada are making some  @ some servers and tough small business Went to our Creator, but it is trainian press the Indiana mabs, to take playing is high enough. by Democrats’ disgrace. They ma\n",
            "\n",
            "Pie de entrada: <SOS>Repu\n",
            "\t----- diversity: 0.2\n",
            "\t\t\t\tRepublicans will be a great honor to welcome to the people of the United State of the political pundits that the U.S. is looking our country and the Democrats are with the people of the best the Nation to the workers and the White House to\n",
            "\t----- diversity: 0.6\n",
            "\t\t\t\tRepublicans in the man who are with President of the people to meeting Americane season. There is a disaster Reed on the principle starting with the ObamaCare. Military and we will be the Republican second desting along time with “Chuck Sc\n",
            "\t----- diversity: 1.2\n",
            "\t\t\t\tRepublic is going to obstructioning fields, and remember, one countless until all AmericaneHarvey denied in U.S. and Heaventable those of Kazakhstan today.  DACA not fix message to our taxes outside words!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgQ4JKP8H7pH",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0SD3_LBIN2nU"
      },
      "source": [
        "\n",
        ">#     **COMENTARIOS Y AGRADECIMIENTOS**\n",
        "                        Como nota final de este trabajo me gustaría dar las gracias al profesor David Torrejón, por su\n",
        "                    atención (tanto en las clases como en Slack) y por su generosidad preparando toda la documentación\n",
        "                                                                                 teórica y práctica para la asignatura.\n",
        "                      El NLP es un mundo denso, muy complejo y creo que el profesor ha hecho un gran trabajo y un gran\n",
        "                         esfuerzo para que los alumnos pudieramos llegar a entender lo imprescindible y más importante.\n",
        "                  Personalmente me ha costado llegar hasta aquí... algunos conceptos son realmente complicados y nunca\n",
        "                    había experimentado antes con el tratamiento del \"texto libre\"... espero haber conseguido elaborar\n",
        "                      un trabajo digno y medianamente decente, aunque el mayor mérito vuelve a ser del profesor ya que,\n",
        "                                  en los colaboratory de la asignatura, están estos ejercicios prácticamente resueltos.\n",
        "                                           Sin más, mi más sincero agradecimiento por todas las cosas que he aprendido.\n",
        "                                                                                                 F. Javier Gonzálvez"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuUOMVoDQdwU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}